{
  
    
        "post0": {
            "title": "Convolution",
            "content": "This is the temperature for Tel Aviv, between 2 and 5 of January 2022. Data is in intervals of 10 minutes, and was downloaded from the Israel Meteorological Service. . . We see that the temperature curve has a rough profile. Can we find ways of getting smoother curves? . Convolution . Convolution is a fancy word for averaging a time series using a running window. We will use the terms convolution, running average, and rolling average interchangeably. See the animation below. We take all temperature values inside a window of width 500 minutes (51 points), and average them with equal weights. The weights profile is called kernel. . The pink curve is much smoother than the original! However, the running average cannot describe sharp temperature changes. If we decrease the window width to 200 minutes (21 points), we get the following result. . There is a tradeoff between the smoothness of a curve, and its ability to describe sharp temporal changes. . Kernels . We can modify our running average, so that values closer to the center of the window have higher weights, and those further away count less. This is achieved by changing the weight profile, or the shape of the kernel. We see below the result of a running average using a triangular window of base 500 minutes (51 points). . Things can get as fancy as we want. Instead of a triangular kernel, which has sharp edges, we can choose a smoother gaussian kernel, see the difference below. We used a gaussian kernel with 60-minute standard deviation (the window in the animation is 4 standard deviations wide). . Math . The definition of a convolution between signal $f(t)$ and kernel $k(t)$ is . (fâˆ—k)(t)=âˆ«f(Ï„)k(tâˆ’Ï„)dÏ„.(f * k)(t) = int f( tau)k(t- tau)d tau.(fâˆ—k)(t)=âˆ«f(Ï„)k(tâˆ’Ï„)dÏ„. . The expression $f*k$ denotes the convolution of these two functions. The argument of $k$ is $t- tau$, meaning that the kernel runs from left to right (as $t$ does), and at every point the two signals ($f$ and $k$) are multiplied together. It is the product of the signal with the weight function $k$ that gives us an average. Because of $- tau$, the kernel is flipped backwards, but this has no effect to symmetric kernels, like to ones in the examples above. Finally, the actual running average is not the convolution, but . (fâˆ—k)(t)âˆ«k(t)dt. frac{(f * k)(t)}{ displaystyle int k(t)dt}.âˆ«k(t)dt(fâˆ—k)(t)â€‹. . Whenever the integral of the kernel is 1, then the convolution will be identical with the running average. . Numerics . Running averages are very common tools in time-series analysis. The pandas package makes life quite simple. For example, in order to calculate the running average of temperature using a rectangular kernel, one writes . df[&#39;temperature&#39;].rolling(window=&#39;20&#39;, center=True).mean() . window=20 means that the width of the window is 20 points. Pandas lets us define a window width in time units, for example, window=&#39;120min&#39;. | center=True is needed in order to assign the result of averaging to the center of the window. Make it False and see what happens. | mean() is the actual calculation, the average of temperature over the window. The rolling part does not compute anything, it just creates a moving window, and we are free to calculate whatever we want. Try to calculate the standard deviation or the maximum, for example. | . It is implicit in the command above a â€œrectangularâ€ kernel. What if we want other shapes? . Gaussian . ( df[&#39;temperature&#39;].rolling(window=window_width, center=True, win_type=&quot;gaussian&quot;) .mean(std=std_gaussian) ) . where . window_width is an integer, number of points in your window | std_gaussian is the standard deviation of your gaussian, measured in sample points, not time! | . For instance, if we have measurements every 10 minutes, and our window width is 500 minutes, then window_width = 500/10 + 1 (first and last included). If we want a standard deviation of 60 minutes, then std_gaussian = 6. The gaussian kernel will look like this: . . You can take a look at various options for kernel shapes here, provided by the scipy package. The graph above was achieved by running: . g = scipy.signal.gaussian(window_width, std) plt.plot(g) . Triangular . Same idea as gaussian, but simpler, because we donâ€™t need to think about standard deviation. . ( df[&#39;temperature&#39;].rolling(window=window_width, center=True, win_type=&quot;triang&quot;) .mean() ) . ğŸ¤·â€â™‚ï¸ Which window shape and width to choose? . Sorry, there is not definite answer hereâ€¦ It really depends on your data and what you need to do with it. See below a comparison of all examples in the videos above. . .",
            "url": "https://yairmau.github.io/website/markdown/2022/09/08/convolution.html",
            "relUrl": "/markdown/2022/09/08/convolution.html",
            "date": " â€¢ Sep 8, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Book 1, Proposition 02",
            "content": ". Proposition 2, Problem . From a given point to draw a straight line equal to a given finite straight line. . . Draw the dashed line $( { color{#000} rule[-0.5em]{0.2cm}{2pt} } { color{#FFF} rule[-0.5em]{0.2cm}{2pt} } { color{#000} rule[-0.5em]{0.2cm}{2pt} } { color{#FFF} rule[-0.5em]{0.2cm}{2pt} } { color{#000} rule[-0.5em]{0.2cm}{2pt} } )$, (post. 1.) . . Describe the equilateral triangle (red-red-dashed) (prop. 1), produce red line $({ color{#D32F2F} rule[-0.5em]{1cm}{2pt} })$ on the left (post. 2.). . . Describe the blue circle, whose radius is the black line $({ color{#000} rule[-0.5em]{1cm}{2pt} })$ (post. 3.). . . Construct the yellow line $({ color{#FFd700} rule[-0.5em]{1cm}{2pt} })$, continuation of left red line $({ color{#D32F2F} rule[-0.5em]{1cm}{2pt} })$, until it reaches the blue circle. . . Construct the red circle (post. 3.), whose radius is the red-yellow line $( { color{#D32F2F} rule[-0.5em]{0.5cm}{2pt} } { color{#FFd700} rule[-0.5em]{0.5cm}{2pt} } )$ . . . Construct the blue line $({ color{#1565C0} rule[-0.5em]{1cm}{2pt} })$, (post. 2.), which is the continuation of the right red line $({ color{#D32F2F} rule[-0.5em]{1cm}{2pt} })$ , until it reaches the red circle. . . The blue line $({ color{#1565C0} rule[-0.5em]{1cm}{2pt} })$ is the required line. . For . =% red yellow { color{#D32F2F} rule[-0.5em]{0.5cm}{2pt} } { color{#FFd700} rule[-0.5em]{0.5cm}{2pt} } = % red blue { color{#D32F2F} rule[-0.5em]{0.5cm}{2pt} } { color{#1565C0} rule[-0.5em]{0.5cm}{2pt} }= . (def. 15.), and . =% red { color{#D32F2F} rule[-0.5em]{1cm}{2pt} } = { color{#D32F2F} rule[-0.5em]{1cm}{2pt} }= . (const.), . âˆ´= therefore { color{#FFd700} rule[-0.5em]{1cm}{2pt} } = { color{#1565C0} rule[-0.5em]{1cm}{2pt} }âˆ´= . (ax. 3.), but (def. 15.) . =={ color{#000} rule[-0.5em]{1cm}{2pt} } . { color{#FFd700} rule[-0.5em]{1cm}{2pt} } = { color{#1565C0} rule[-0.5em]{1cm}{2pt} }&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;==&lt;/span&gt;&lt;/span&gt; . therefore ${ color{#1565C0} rule[-0.5em]{1cm}{2pt} }$ drawn from the given point is equal to the given line ${ color{#000} rule[-0.5em]{1cm}{2pt} }$. . Q.E.D. . I made a Geogebra of this proposition: .",
            "url": "https://yairmau.github.io/website/markdown/2022/09/02/euclid-book1-prop02.html",
            "relUrl": "/markdown/2022/09/02/euclid-book1-prop02.html",
            "date": " â€¢ Sep 2, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Book 1, Proposition 01",
            "content": ". Proposition 1, Problem . On a given finite straight line $({ color{#000} rule[-0.5em]{1cm}{2pt} })$ to describe an equilateral triangle. . . Describe the blue circle, whose radius is the black line (post. 3.). . . Describe the red circle, whose radius is the black line (post. 3.). . . Draw lines ${ color{#FFd700} rule[-0.5em]{1cm}{2pt} }$ and ${ color{#D32F2F} rule[-0.5em]{1cm}{2pt} }$ (post. 1.). . . Then will the triangle $( { color{#000} rule[-0.5em]{1cm}{2pt} }, { color{#FFd700} rule[-0.5em]{1cm}{2pt} }, { color{#D32F2F} rule[-0.5em]{1cm}{2pt} } )$ be equilateral. . For . ={ color{#000} rule[-0.5em]{1cm}{2pt} } . { color{#FFd700} rule[-0.5em]{1cm}{2pt} }&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;=&lt;/span&gt;&lt;/span&gt; . (def. 15.); and . ={ color{#000} rule[-0.5em]{1cm}{2pt} } . { color{#D32F2F} rule[-0.5em]{1cm}{2pt} }&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;=&lt;/span&gt;&lt;/span&gt; . (def. 15.), . âˆ´= therefore { color{#D32F2F} rule[-0.5em]{1cm}{2pt} } = { color{#FFd700} rule[-0.5em]{1cm}{2pt} }âˆ´= . (axiom. 1.); . and therefore we constructed the required equilateral triangle. . Q.E.D. . I made a Geogebra of this proposition: .",
            "url": "https://yairmau.github.io/website/markdown/2022/09/01/euclid-book1-prop01.html",
            "relUrl": "/markdown/2022/09/01/euclid-book1-prop01.html",
            "date": " â€¢ Sep 1, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Markdown test",
            "content": "Inline math: $ax^2+bx+c=0$ . Another equation: . x1,2=âˆ’bÂ±b2âˆ’4ac2ax_{1,2} = frac{-b pm sqrt{b^2 - 4ac}}{2a}x1,2â€‹=2aâˆ’bÂ±b2âˆ’4ac . â€‹â€‹ . Aligned equations: . x=A(cosâ¡Î¸âˆ’vsinâ¡Î¸)y=B(sinâ¡Î¸+vcosâ¡Î¸)z=CÎµv. begin{align} x &amp;= A ( cos theta - v sin theta) y &amp;= B ( sin theta + v cos theta) z &amp;= C varepsilon v. end{align}xyzâ€‹=A(cosÎ¸âˆ’vsinÎ¸)=B(sinÎ¸+vcosÎ¸)=CÎµv.â€‹â€‹ . now some code: . import numpy as np for i in np.arange(10): print(np.sin(i)) .",
            "url": "https://yairmau.github.io/website/markdown/2022/03/04/test_md.html",
            "relUrl": "/markdown/2022/03/04/test_md.html",
            "date": " â€¢ Mar 4, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "jupyter test",
            "content": "Inline math: $ax^2+bx+c=0$ . Another equation: . $$ x_{1,2} = frac{-b pm sqrt{b^2 - 4ac}}{2a} $$Aligned equations: . $$ begin{align} x &amp;= A ( cos theta - v sin theta) y &amp;= B ( sin theta + v cos theta) z &amp;= C varepsilon v. end{align} $$now some code: . import numpy as np for i in np.arange(10): print(np.sin(i)) .",
            "url": "https://yairmau.github.io/website/jupyter/2022/03/04/test_jupyter.html",
            "relUrl": "/jupyter/2022/03/04/test_jupyter.html",
            "date": " â€¢ Mar 4, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "NÃ£o foi milagre",
            "content": ". O dia da independÃªncia de Israel nÃ£o Ã© nada mais que uma segunda comemoraÃ§Ã£o de Chanuka. Yom haAtzmaut (Dia da IndependÃªncia em hebraico) celebra o comeÃ§o de uma soberania judaica na Terra de Israel, pela primeira vez desde a soberania conquistada pelos macabeus, cerca de dois mil anos antes. . Ã‰ difÃ­cil saber onde uma festa comeÃ§a e a outra acaba. Enquanto que Ã© claro para todos que as tradiÃ§Ãµes relacionadas a Yom haAtzmaut sÃ£o bastante recentes, e obviamente nÃ£o podem anteceder o prÃ³prio sionismo, Ã© importante marcar que a festa de Chanuka como a conhecemos hoje Ã© extremamente moderna, e Ã© fruto da empreitada sionista. . Segundo o judaÃ­smo rabÃ­nico, os macabeus nÃ£o eram herÃ³is. Para eles, toda a ideia da festa de Chanuka Ã© que Deus â€œentregou herÃ³is na mÃ£o dos fracos, e [entregou] os que eram muitos nas mÃ£os dos poucos.â€ Ã‰ assim que se lÃª na prece â€œSobre os Milagreâ€ (Al haNisim) de Chanuka, os herÃ³is sÃ£o os gregos, e os fracos dos judeus venceram apenas com a ajuda de Deus. . Nada disso!, disseram os primeiros sionistas, que criaram a organizaÃ§Ã£o Chibat Tzion, no final do sÃ©culo 19. Os macabeus foram resgatados como os verdadeiros herÃ³is, aqueles cuja bravura os permitiu criar um Estado autÃ´nomo, contrariando todas as expectativas. Nada de milagres, nada de Ã³leo que durou oito dias. Foi com os Chovevei Tzion (membros da Chibat Tzion) que Chanuka se tornou uma das festas mais importantes do calendÃ¡rio judaico. HÃ¡ muitos outros exemplos de como a histÃ³ria dos macabeus inspirou o sionismo. . O pai do sionismo polÃ­tico, Theodor Herzl, termina seu livro â€œO Estado Judeuâ€ com a seguinte frase: . Portanto, eu acredito que uma maravilhosa geraÃ§Ã£o de judeus se formarÃ¡. Os macabeus se levantarÃ£o novamente. Deixe-me repetir minhas primeiras palavras: Os judeus que desejarem um paÃ­s o terÃ£o. Finalmente viveremos como pessoas livres em sua prÃ³pria terra, e morreremos em paz em nossas prÃ³prias casas. O mundo serÃ¡ libertado por nossa liberdade, enriquecido com nossa riqueza, e engrandecido com nossa grandeza. E o que quer que tentemos realizar para o nosso prÃ³prio bem estar, terÃ¡ efeito poderosamente e beneficialmente para todas as pessoas. . Segundo Herzl, a criaÃ§Ã£o de um Estado Judeu Ã© uma segunda versÃ£o da histÃ³ria dos macabeus. Max Nordau, braÃ§o direito de Herzl no movimento sionista, criou o conceito de judaÃ­smo dos mÃºsculos. â€œA histÃ³ria do nosso povo nos conta que um dia fomos fortes fisicamente, mas hoje este nÃ£o Ã© o caso,â€ disse Nordau em um discurso no segundo Congresso Sionista Mundial, em 1898 na Basileia. â€œNinguÃ©m pode nos negar a atividade fÃ­sica necessÃ¡ria para fazer os nossos corpos novamente saudÃ¡veis. NÃ³s renovaremos nossa juventude [â€¦], desenvolveremos peitos largos, braÃ§os e pernas fortes, um olhar corajoso. Seremos guerreiros [â€¦] Vida longa ao esporte! Que os clubes de esporte hebraicos avancem e floresÃ§am!â€ . Seguindo a visÃ£o de Nordau, foram criados diversos clubes judaicos e movimentos juvenis, muitos dos quais se uniram sob uma sÃ³ organizaÃ§Ã£o chamada Macabi. Ã‰ bem verdade que essa cultura de valor Ã  forÃ§a do corpo tem muito mais a ver com a cultura grega contra a qual os Hasmoneus lutaram em sua revolta, mas nÃ³s nÃ£o deixaremos os fatos nos confundir. Seguimos. . O educador David Judelovitch, que fundou em 1888 a primeira escola hebraica do mundo (segundo ele), em Rishon leTzion, organizava bailes em Chanuka, e tambÃ©m introduziu um desfile com tochas pelas ruas da cidade durante a festa das luzes. Judelovitch era membro da organizaÃ§Ã£o de pioneiros sionistas Bilu, que em um de seus posters de 1882 diziam: â€œOnde estÃ£o os macabeus? JudÃ¡ [o macabeu] hÃ¡ de se levantar!â€ . O desfile de tochas de Chanuka se espalhou pelos assentamentos da regiÃ£o, e bastante mais tarde a tradiÃ§Ã£o foi adaptada a uma corrida de revezamento de tocha. A tradiÃ§Ã£o olÃ­mpica do revezamento de tochas comeÃ§ou nas OlimpÃ­adas de Berlim, em 1936. Como reaÃ§Ã£o, o movimento juvenil Macabi haTzair (O Jovem Macabeu) introduziu em Chanuka de 1944 a tradiÃ§Ã£o de fazer um revezamento de tochas desde o tÃºmulo dos macabeus, ao lado de Modiin, com direÃ§Ã£o a diversas cidades. Em Chanuka de 1949, o BatalhÃ£o Jovem (Gadna = ×’×“× ×´×¢) introduziu a corrida de oito tochas, que comeÃ§avam em partes diferentes do paÃ­s, e todas chegavam ao Monte Herzl, para serem recebidas pelo primeiro-ministro. . Abaixo vemos uma corredora levando a tocha de Chanuka, acesa em Modiin, a JerusalÃ©m para o acendimento da MenorÃ¡ de Chanuka. Dezembro de 1948, Central Zionist Archives. . . Em um artigo anterior, eu notei a tensÃ£o existente dentro da festa de Chanuka, entre os motivos religiosos e os nacionais. Mostrei entÃ£o diversas canÃ§Ãµes dos dois lados, mas guardei para hoje uma das mais importantes. Trata-se de â€œNÃ³s carregamos tochasâ€ (Anu nosâ€™im lapidim = ×× ×• × ×•×©××™× ×œ×¤×™×“×™×), de Aharon Zeev. . NÃ³s carregamos tochas // Em noites escuras // As trilhas brilham sob nossos pÃ©s // E quem tiver coraÃ§Ã£o // Quem tiver sede de luz // LevantarÃ¡ seus olhos e a nÃ³s // E virÃ¡ Ã  luz! . NÃ£o nos aconteceu um milagre // Uma latinha de Ã³leo nÃ£o encontramos // Ao vale andamos, subimos ao montes // As fontes de luz // Escondidas, encontramos. . NÃ£o nos aconteceu um milagre // Uma latinha de Ã³leo nÃ£o encontramos // Carvamos a pedra atÃ© sair sangue // E que haja luz! . Escutem aqui a canÃ§Ã£o. . EstÃ¡ clarÃ­ssimo que a canÃ§Ã£o, escrita em no comeÃ§o dos anos 1930, fala ao mesmo tempo da guerrilha dos macabeus e dos movimentos armados do Yishuv (populaÃ§Ã£o judaica da Palestina prÃ©-1948). O autor nÃ£o sÃ³ tira o milagre do Ã³leo para fora da festa, ele ainda toma para si a criaÃ§Ã£o da luz do GÃªnesis. . O novo judeu da Terra de Israel nÃ£o foi moldado segundo o judeu diaspÃ³rico, muito pelo contrÃ¡rio. Ele Ã© forte e sabe o que quer, dono de seu destino, assim como os macabeus o eram. Vejamos este selo do Keren Kayemet leIsrael (Fundo Nacional Judaico), da dÃ©cada de 1950. Vemos um agricultor com uma arma, e a sombra de um macabeu por trÃ¡s dele. Sabemos que Ã© um macabeu porque em cima estÃ¡ escrito Dinheiro de Chanuka (dmei chanuka), e abaixo o valor de 25 centavos, o valor do selo. . . A tradiÃ§Ã£o laica da festa de Chanuka que se formou durante as dÃ©cadas que precederam a independÃªncia de Israel foi simplesmente adaptada, e transposta para outro mÃªs do calendÃ¡rio. O auge da cerimÃ´nia oficial de Yom haAtzmaut, que dÃ¡ inÃ­cio Ã s comemoraÃ§Ãµes, Ã© o acendimento de 12 tochas, representando as tribos de Israel e a uniÃ£o do povo. . Ã€ esquerda um cartaz de 1951 sobre o Dia da IndependÃªncia, Ã  direita um cartaz de 1950 sobre a corrida de tochas de Chanuka. Mera coincidÃªncia? . . Hoje, a canÃ§Ã£o â€œNÃ³s Carregamos Tochasâ€ pode ser escutada tanto no inverno, durante a festa de Chanuka, quanto na primavera, quando cai o Dia da IndependÃªncia. Abaixo, podemos ver o presidente da Knesset, o deputado Yuli Edelstein, na cerimÃ´nia oficial de Yom haAtzmaut de 2015 no Monte Herzl, em JerusalÃ©m. Enquanto ele discursa antes do acendimento da primeira tocha, escutamos ao fundo a canÃ§Ã£o â€œnÃ³s carregamos tochasâ€. . Durante seus primeiros sÃ©culos, a festa de Chanuka ainda nÃ£o tinha os elementos religiosos como conhecemos, principalmente o milagre do Ã³leo. A explicaÃ§Ã£o de porque a festa dura oito dias tem a ver com a festa de Sukot, uma das trÃªs peregrinaÃ§Ãµes anuais a JerusalÃ©m. Com a inauguraÃ§Ã£o do templo de JerusalÃ©m apÃ³s sua reconquista em 164 AEC, os judeus puderam comemorar a festa de Sukot tardiamente, o que dÃ¡ sete dias e mais um de Shmini Atzeret. . Por um lado, Chanuka tem dentro de si a comemoraÃ§Ã£o tardia de Sukot. Por outro lado, durante as dÃ©cadas que precederam a independÃªncia de Israel, Chanuka era uma comemoraÃ§Ã£o antecipada da esperada autonomia judaica. Foi assim que neste Ãºltimo sÃ©culo estas duas festas evoluiram juntas, e sÃ£o duas faces de uma mesma comemoraÃ§Ã£o nacional de soberania judaica. . Feliz Chanuka! Feliz Yom haAtzmaut! . Fontes: Zeev Galili, United with Israel, My Jewish Learning, Zemereshet, Jpress, Jpress, Shalom Hartman Institute, Palestine Poster Project, Wikipedia, Rashut haShidur, Museum of Family History, Gutenberg.org, WikiSource. .",
            "url": "https://yairmau.github.io/website/markdown/2022/02/01/nao-foi-milagre.html",
            "relUrl": "/markdown/2022/02/01/nao-foi-milagre.html",
            "date": " â€¢ Feb 1, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Maoz Tzur: massacre e vinganÃ§a",
            "content": ". Maoz Tzur, a canÃ§Ã£o mais conhecida de Chanuka, fala de massacre e vinganÃ§a, e ainda sim Ã© uma das favoritas das crianÃ§as. Em Israel, faz parte da sequÃªncia â€œoficialâ€ do acendimento das velas: acendem-se enquanto duas benÃ§Ã£os sÃ£o feitas, e logo apÃ³s, sem perder um milionÃ©simo de segundo, todos comeÃ§am a cantar Maoz Tzur. . A melodia Ã© linda, exaltante, e apropriada para ser cantada em coro, com toda a famÃ­lia. Contudo, a letra nÃ£o Ã© muito clara para a maior parte dos israelenses. Talvez isto explique o seu sucesso, nÃ£o sei bemâ€¦ Neste momento em que escrevo estas palavras, minha filha estÃ¡ assistindo um programa israelense no youtube, onde Rinat, a rainha dos baixinhos, canta Maoz Tzur para um grupo de crianÃ§as. Ã‰ incrÃ­vel a cara que ela faz enquanto diz as palavras â€œNo momento em que [Deus] preparar o massacre dos inimigos que latemâ€. . Mordechai . Mordechai, autor do poema, era um cara modesto, e colocou as 5 letras de seu nome (××¨×“×›×™) abrindo as cinco primeiras estrofes (sÃ£o 6 no total, a parte que todos cantam Ã© a primeira estrofe, e â€œninguÃ©mâ€ conhece o resto). Quem Ã© Mordechai? NinguÃ©m sabe. . O que sim sabemos Ã© que o poema data dos anos 1200, no que hoje chamamos de Alemanha. Essa era uma Ã©poca difÃ­cil aos judeus europeus, com a perseguiÃ§Ã£o â€œnormalâ€ exacerbada pelas cruzadas. . Pode ser que se trate do rabino Mordechai Ben Hillel, de Nuremberg. Ele Ã© muito conhecido por seu livro de decisÃµes acerca da halachÃ¡ (lei judaica), e tambÃ©m escreveu um livro de poemas e lamentaÃ§Ãµes, onde assinava suas obras como â€œMordechaiâ€. Nasceu em 1250, e morreu (juntamente com sua esposa e filhos) em primeiro de agosto de 1298, no pogrom de Rintfleisch. . TambÃ©m pode ser que o autor seja o poeta Mordechai ben Yitzhak HaLevi, nascido na ItÃ¡lia, e que mais tarde se mudou para Mainz. Seu sogro teria sido morto durante os massacres de Rhineland, em 1096, durante a primeira cruzada. . Seja quem for que escreveu Maoz Tzur, fica claro que a perseguiÃ§Ã£o aos judeus Ã© um fator essencial para entendermos a temÃ¡tica do poema. . Pessach, BabilÃ´nia, Purim e Chanuka . Maoz tzur conta a histÃ³ria do povo judeu, e como Deus salvou seu povo escolhido em quatro momentos diferentes. . A segunda estrofe fala do Ãªxodo do Egito. . â€œMinha alma se saciou de tragÃ©dias, com tristeza minha forÃ§a se apagou.Minha vida eles amarguraram com dificuldades, na escravidÃ£o no reino do Egito.E com sua grande mÃ£o [Deus] tirou dali seu povo especial.Enquanto o exÃ©rcito do faraÃ³ desceu Ã s profundezas como uma pedra.â€ . A terceira estrofe trata do final do exÃ­lio da BabilÃ´nia. . â€œEle me trouxe ao seu lugar mais santo, e mesmo ali nÃ£o descansei.O opressor veio e me exilou, pois eu tinha deuses estranhos,e bebia vinho venenoso. Mas eu nem havia chegado,A BabilÃ´nia caiu e Zorobabel [veio], em setenta anos fui salvo.â€ . A quarta estrofe reconta o milagre de Purim. . â€œO Agagita filho de Hamedata [Haman] queria cortar o cipreste [Mordechai].Mas acabou sendo-lhe um embuste e obstÃ¡culo, seu orgulho se silenciou.VocÃª levantou a cabeÃ§a do benjamita [Mordechai], e riscou o nome do inimigo.Os seus vÃ¡rios filhos vocÃª enforcou numa Ã¡rvore.â€ . Finalmente, na quinta estrofe chegamos a Chanuka: . â€œOs gregos se juntaram contra mim nos tempos dos Hasmoneus.E derrubaram os muros de minha torre, e deixaram todo o Ã³leo impuro.E do Ãºltimo dos frascos, foi feito um milagre Ã s rosas [ao povo judeu].[EntÃ£o] os sÃ¡bios determinaram os oito dias de canÃ§Ãµes de alegria.â€ . O comeÃ§o e o final . Maoz Tzur nÃ£o foi escrita como canÃ§Ã£o de Chanuka, acabamos de ver isto acima. Como foi entÃ£o que esta poesia tornou-se um sÃ­mbolo de Chanuka e nÃ£o de Pessach ou de Purim? . A primeira e Ãºltima estrofes sÃ£o as Ãºnicas no tempo presente, e dizem o seguinte: . â€œMinha fortaleza, rocha de minha salvaÃ§Ã£o, Ã© agradÃ¡vel Te louvar.EstabeleÃ§a a minha casa de preces [o templo de JerusalÃ©m], e ali Lhe faremos sacrifÃ­cios de agradecimento.Quando vocÃª massacrar os inimigos que latem,EntÃ£o terminarei cantando um hino Ã  inauguraÃ§Ã£o [chanuka] do altar.â€ . Ãšltima estrofe: . â€œRevele o Seu santo braÃ§o e aproxime a redenÃ§Ã£o.Traga vinganÃ§a ao povo mau, em nome de Seus servos.Pois [a salvaÃ§Ã£o] jÃ¡ demorou muito para chegar, e os dias de maldade nÃ£o tÃªm fim.Empurre o vermelho [EsaÃº, representando o cristianismo] Ã s sombras, e nos estabeleÃ§a sete pastores [que libertarÃ£o Israel da opressÃ£o, MiquÃ©ias 5:4].â€ . Antes de mais nada: nÃ£o Ã© uma temÃ¡tica para mÃºsica infantil. Ficou clarÃ­ssimo. Mas a Marselhesa tambÃ©m nÃ£o Ã© (Ã€s armas, cidadÃ£os, // Formai vossos batalhÃµes! // marchemos, marchemos! // Que um sangue impuro // Banhe o nosso solo!), e milhÃµes de criancinhas tambÃ©m a cantamâ€¦ . Provavelmente porque a palavra Chanuka aparece na primeira estrofe (literalmente no sentido de inauguraÃ§Ã£o e consagraÃ§Ã£o!), a associaÃ§Ã£o com esta festa Ã© inevitÃ¡vel, e no final das contas ninguÃ©m lembra de Pessach e Purim quando se fala de Maoz Tzur. . A Ãºltima estrofe sÃ³ foi aparecer pela primeira vez hÃ¡ cerca de 200 anos, em AmsterdÃ£. NÃ£o se sabe se ela Ã© mais recente, ou se faz parte do poema original, e foi censurada por conta do tema da vinganÃ§a, para nÃ£o gerar mais perseguiÃ§Ãµes. Alguns dizem que faz sentido que a estrofe tenha sido escrita junto com o resto, porque as primeiras letras de suas trÃªs primeiras palavras formam Chazak (×—×–×§), ou seja, â€œforteâ€. Novamente, Ã© tudo especulaÃ§Ã£o. . A melodia . A melodia Ã© uma adaptaÃ§Ã£o de uma canÃ§Ã£o folclÃ³rica alemÃ£, â€œSo weiss ich eins, dass mich erfreut, das pluemlein auff preiter heydeâ€. A mesma canÃ§Ã£o parece ter sido adaptada por Martinho Lutero para seus corais, e podemos avaliar as semelhanÃ§as no vÃ­deo abaixo (coral â€œNun freut euch, lieben Christen gâ€™meinâ€) com harmonia de Bach. . Antes mesmo da fundaÃ§Ã£o de Israel, as crianÃ§as aprendiam Maoz Tzur no jardim de infÃ¢ncia, e depois iam para casa e ensinavam a seus pais. Possivelmente, isto explica a sua hegemonia absoluta. Ã‰ interessante notar que Maoz Tzur nÃ£o aparecia em nenhum livro de poesias ou cÃ¢nticos dos judeus sefaraditas. Com sua imigraÃ§Ã£o massiva nos anos 1950 e 1960, os sefaraditas tambÃ©m adotaram Maoz Tzur dos ashkenazitas que jÃ¡ estavam em Israel. . . . Fontes: YNET, YNET, Simania, ZeevGalili, Wikipedia, Wikipedia. . Imagem de destaque: Mosaico de menorÃ¡, sÃ©culo 6 EC, de uma sinagoga na TunÃ­sia. Fonte: ancienthistory. .",
            "url": "https://yairmau.github.io/website/markdown/2022/02/01/maoz-tzur.html",
            "relUrl": "/markdown/2022/02/01/maoz-tzur.html",
            "date": " â€¢ Feb 1, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "O israelense otÃ¡rio",
            "content": ". O maior medo do israelense, maior do que uma guerra nuclear com o IrÃ£, maior que o medo da morte, Ã© o medo de ser Frayer. Em uma palavra, frayer Ã© o otÃ¡rio, mas nÃ³s jÃ¡ vamos entender melhor o que isso significa. O pavor de ser frayer acaba modelando a vida do indivÃ­duo e da sociedade como um todo, e conhecer isto Ã© indispensÃ¡vel para se poder entender o que Ã© ser israelense. . Etimologia . Frayer (×¤×¨××™×™×¨) vem do alemÃ£o Freier, que significa â€œpretendenteâ€, um homem solteiro que procura uma mulher (Frau). De pretendente, freier ganhou o novo significado de â€œaquele que costuma ir a prostitutasâ€, e tambÃ©m â€œaquele que Ã© fÃ¡cil engana-lo, rouba-lo, a vÃ­tima de uma trambicagemâ€. A palavra migrou ao Leste, e entrou no polonÃªs e no russo. Em polonÃªs freier significa ladrÃ£o. JÃ¡ no inÃ­cio do sÃ©culo XX, freier ganhou na linguagem popular russa o significado de â€œaquele que se deixa levar, pessoa ingÃªnua, boba e sem experiÃªncia, no melhor dos casos apenas um cara que nÃ£o Ã© nem ladrÃ£o nem criminosoâ€. A palavra chegou ao hebraico pelo idish, lÃ­ngua dos judeus da Europa oriental, e a gÃ­ria â€œfrayerâ€ claramente tem um significado bastante parecido ao do russo. Em hebraico se usa frayer com o verbo sair: ser otÃ¡rio se diz sair frayer (latzet frayer = ×œ×¦××ª ×¤×¨××™×™×¨). . Exemplos . Antes de discutir sobre o que significa ser frayer, citarei dois exemplos para entrarmos no clima. Todos sÃ£o verdadeiros, e vivenciados por mim (aproximadamente). A pergunta que o israelense sempre faz, e que guia o seu comportamento, Ã©: Que-que Ã©? Eu tenho cara de otÃ¡rio?? (Ma, ani frayer? = ××”, ×× ×™ ×¤×¨××™×™×¨ ). . 1 â€“ Domingo Ã© o dia em que os soldados de todo o paÃ­s voltam para suas bases, apÃ³s o fim de semana. Jovens de 18 a 21 anos se amontoam nas rodoviÃ¡rias tentando pegar o primeiro Ã´nibus que puderem, e a cena nÃ£o Ã© nada bonita. NÃ£o existe fila para subir no Ã´nibus, nÃ£o existe respeito pelo espaÃ§o do outro. Quem tiver cotovelos mais ousados vai ganhar um lugar. Os civis que tambÃ©m querem subir no Ã´nibus nÃ£o tem tratamento diferenciado, eles tambÃ©m empurram e sÃ£o empurrados. Ã‰ cada um por si, e Deus por todos. A velhinha pede passagem, e o soldado com espinhas na cara nÃ£o dÃ¡, senÃ£o nÃ£o vai ter mais lugar na janela. â€œQue que Ã©? Eu tenho cara de otÃ¡rio?â€ . 2 â€“ SaguÃ£o de embarque de um aeroporto em uma capital europÃ©ia. Destino: Tel Aviv. Aos poucos os israelenses que estÃ£o voltando para casa se reÃºnem em volta do portÃ£o de embarque e amigavelmente conversam com seus compatriotas sobre as agradÃ¡veis histÃ³rias das fÃ©rias. Todos embarcam sem demais complicaÃ§Ãµes, e o aviÃ£o decola. Ao longo do curto voo, uma inquietaÃ§Ã£o crescente pode ser sentida. TÃ£o logo o aviÃ£o aterrisa, os amigaveis compatriotas tornam-se feras, a cordialidade desaparece e cada um sÃ³ pensa em ser o primeiro a sair do aviÃ£o, o primeiro a passar pela imigraÃ§Ã£o, o primeiro a pegar as malas. Dois colegas de voo, que sentaram juntos na fileira 17, agora se ignoram, e fazem a conhecida â€˜marcha atlÃ©ticaâ€™ em direÃ§Ã£o ao primeiro tÃ¡xi livre. â€œQue que Ã©? Eu tenho cara de otÃ¡rio?â€ . O que significa ser frayer? . Gadi, o personagem principal do sitcom israelense A Vida NÃ£o Ã‰ Tudo (hachaim ze lo hakol = ×”×—×™×™× ×–×” ×œ× ×”×›×œ), explicou desta forma: . â€œPara ser um frayer Ã© preciso de duas pessoas: O frayer e aquele que faz do frayer um frayerâ€. . . Uma pessoa nÃ£o pode ser frayer apenas porque se deu mal em certa situaÃ§Ã£o. O frayer surge quando alguÃ©m tem medo de ser transformado em frayer, e como estratÃ©gia preventiva, faz de outro um frayer. Esse raciocÃ­nio Ã© conhecido como â€œa armadilha hobbesianaâ€. Em â€œO LeviatÃ£â€, Hobbes escreve: â€œPois a natureza dos homens Ã© tal que, embora sejam capazes de reconhecer em muitos outros maior inteligÃªncia, maior eloqÃ¼Ãªncia ou maior saber, dificilmente acreditam que haja muitos tÃ£o sÃ¡bios como eles prÃ³priosâ€. Acertou em cheio, taÃ­ uma boa caracterizaÃ§Ã£o do israelense mÃ©dio. Hobbes continua: â€œE contra esta desconfianÃ§a de uns em relaÃ§Ã£o aos outros, nenhuma maneira de se garantir Ã© tÃ£o razoÃ¡vel como a antecipaÃ§Ã£o; isto Ã©, pela forÃ§a ou pela astÃºcia, subjugar as pessoas de todos os homens que puder, durante o tempo necessÃ¡rio para chegar ao momento em que nÃ£o veja qualquer outro poder suficientemente grande para ameaÃ§Ã¡-lo.â€ Nota 10 para o Thomas. . O mecanismo descrito acima funciona quando o LeviatÃ£ (o Estado) nÃ£o estÃ¡ presente, e esta Ã© a grande diferenÃ§a entre o nÃ£o-frayer israelense e o jeitinho brasileiro. A grande maioria dos casos de â€œfrayerismoâ€ (frayeriut = ×¤×¨××™×™×¨×™×•×ª) sÃ£o os de furar fila, empurrar, grosseria verbal para ganhar alguma vantagem, etc. Muitas sÃ£o as vezes que um amigo israelense me conta uma histÃ³ria e eu a imagino acontecendo no Brasil. Uns colegas foram acampar ilegalmente numa reserva natural no deserto do Negev, e foram surpreendidos por um fiscal. Eles pagaram a multa (cara) sem dizer nada e foram levados embora. NinguÃ©m pensou em molhar a mÃ£o de ninguÃ©m. Neste caso eles nÃ£o foram frayerim, pois ninguÃ©m os fez de frayer, mas esta Ã© exatamente a perfeita situaÃ§Ã£o para o jeitinho brasileiro entrar em aÃ§Ã£o. . Os israelenses estÃ£o aprendendo a fazer fila, embora num ritmo pouco satisfatÃ³rio para o meu gosto. Apenas hÃ¡ alguns anos atrÃ¡s senhas com nÃºmeros foram introduzidas em farmÃ¡cias e bancos para dar ordem na bagunÃ§a. Na agÃªncia de correio perto de casa nÃ£o hÃ¡ senhas, e quem chega pergunta â€œquem Ã© o Ãºltimo?â€, essa Ã© a regra. As pessoas nÃ£o ficam de pÃ© em fila, ficam cada um em seu canto, lendo jornal ou jogando candy crush no celular. Nos 10 ou 15 minutos que normalmente levo para ser atendido, eu tenho que ficar esperto pra ver se ninguÃ©m vai furar a fila, Ã© uma tensÃ£o constante. Na Ãºltima vez que fui Ã  farmÃ¡cia um cara sem senha queria ser atendido quando o meu nÃºmero foi chamado. â€œEu nÃ£o sabia que tinha que pegar uma senhaâ€. Acabamos discutindo e fazendo um mini-barraco, atÃ© que eu fui atendido. Eu estava segurando a minha filha de um ano e meio no colo, e ele estava acompanhado do filho de 10 anos. Ele nÃ£o teve vergonha nenhuma de mostrar ao filho como Ã© que se faz para nÃ£o dar uma de frayer, mas eu sim fiquei com vergonha de ter caÃ­do na â€œarmadilha hobbesianaâ€. . A intervenÃ§Ã£o do Estado nÃ£o Ã© a Ãºnica maneira de controlar a â€œsÃ­ndrome do frayerâ€. Normas sociais sÃ£o tÃ£o ou mais importantes. Os pesquisadores Luis Roniger e Michael Feige publicaram em 1992 o artigo â€œA cultura do frayer e a identidade israelenseâ€ no periÃ³dico Alpaim, onde explicam o fenÃ´meno do frayer como uma mudanÃ§a de identidade de geraÃ§Ãµes na sociedade israelense. A primeira geraÃ§Ã£o, a das primeiras aliot (primeiras ondas de imigraÃ§Ã£o judaica Ã  Palestina), se auto identificava como pioneira, a segunda geraÃ§Ã£o como sabras (israelenses natos), e a terceira geraÃ§Ã£o Ã© caracterizada pela cultura do nÃ£o-frayer, que surgiu como crÃ­tica interna Ã  cultura das geraÃ§Ãµes que a antecederam. A primeira geraÃ§Ã£o de pioneiros queria realizar um ideal, e por isso o sacrifÃ­cio pessoal para o bem do coletivo era bem visto e esperado. Sempre houve a opÃ§Ã£o de nÃ£o contribuir com o coletivo, mas nÃ£o havia a mesma legitimaÃ§Ã£o como hÃ¡ hoje. A cultura do nÃ£o frayer Ã© uma expressÃ£o profundamente anti-ideolÃ³gica, seus herÃ³is nÃ£o se sacrificam pelos outros, muito pelo contrÃ¡rio, lutam pelos seus interesses pessoais. A cultura do nÃ£o-frayer nos mostra a mudanÃ§a na percepÃ§Ã£o do que Ã© forÃ§a: para os pioneiros, a forÃ§a do indivÃ­duo derivava de sua integraÃ§Ã£o na narrativa de renascimento nacional e sua abdicaÃ§Ã£o pessoal para o bem de todos. O frayer simboliza a queda da fonte de poder do coletivo ao indivÃ­duo. Se o indivÃ­duo nÃ£o estÃ¡ disposto a se sacrificar, a forÃ§a do coletivo sofre, e a longo prazo os prÃ³prios interesses do indivÃ­duo podem ser atingidos. Esse Ã© o paradoxo que Roniger e Feige indentificam na cultura do frayer. . O â€œProtesto dos OtÃ¡riosâ€ (mechaat hafrayerim = ××—××ª ×”×¤×¨××™×™×¨×™×) vem lutar exatamente contra uma crescente percepÃ§Ã£o de que aquele que faz exÃ©rcito e se sacrifica pelo paÃ­s Ã© frayer. Assim como as â€œvadiasâ€ da marcha das Vadias e os palmeirenses que se chamam orgulhosamente de â€œporcoâ€, esses â€œotÃ¡riosâ€ surgiram apÃ³s os grandes protestos sociais de 2011 para exigir que a lei de serviÃ§o militar obrigatÃ³rio fosse cumprida por todos, sem exceÃ§Ãµes. Se o jovem judeu ortodoxo Ã© liberado de trÃªs anos de um duro serviÃ§o militar e pode ficar estudando Talmud no conforto de sua yeshiva, por que um jovem laico teria que carregar o fardo adicional em seus ombros, e dar uma de otÃ¡rio? O protesto dos otÃ¡rios veio tentar acabar com a crescente espiral que levaria o serviÃ§o militar a entrar na lista de coisas onde Ã© aceitÃ¡vel e esperado de cada um lutar por si sÃ³, como na fila de correios e bancos, trÃ¢nsito e tantos outros. . Faixa do â€œProtesto dos OtÃ¡riosâ€ que diz â€œtodos devem fazer serviÃ§o militarâ€ . â€œOtÃ¡riaâ€ . â€œEu tambÃ©m sou otÃ¡rio/a, se eu fosse aluno de uma yeshiva, eu estudaria grÃ¡tis.â€ . Toda a discussÃ£o acima trata do significado mais restrito do que significa ser frayer: uma pessoa que se dÃ¡ mal porque fulano (normalmente seu par, peer em inglÃªs) o ferrou antes que ele mesmo tivesse se dado mal. Existe tambÃ©m o sensu lato, ou seja, uma pessoa que apenas se deu mal, sem um agente externo que propositadamente pÃ´s uma pedra em seu caminho. Exemplo clÃ¡ssico: se eu compro um produto e depois, conversando com um amigo, fico sabendo que ele pagou muito menos, eu claramente dei uma de frayer. Se eu pago altos preÃ§os por certos produtos que sÃ£o sujeitos a um monopÃ³lio, ou se pago altos impostos, tambÃ©m posso me sentir um frayer. O famoso grupo de hip hop HaDag Nachash escreveu a mÃºsica â€œNÃ£o Somos OtÃ¡riosâ€ (Lo Frayerim = ×œ× ×¤×¨××™×™×¨×™×), onde o refrÃ£o diz ironicamente â€œFaremos serviÃ§o de reserva do exÃ©rcito, pagaremos os impostos, Ficaremos no engarrafamento, ninguÃ©m nos fode, NÃ³s com certeza, certeza, certeza nÃ£o, NÃ³s com certeza nÃ£o somos otÃ¡riosâ€. . Para terminar, nÃ£o gostaria de passar a impressÃ£o de que Israel Ã© uma selva, e que por aqui â€œhomo homini lupusâ€ em toda circunstÃ¢ncia. SÃ£o relativamente poucos os que se preocupam muito em nÃ£o dar uma de frayer o tempo todo, mas eles certamente fazem muito barulho e estrago. Visitar Israel e voltar dizendo que â€œtem muita gente mal educadaâ€ nÃ£o ajuda nada, e espero ter contribuido para explicar um pouco o choque que alguns novatos (e veteranos tambÃ©m) sentem quando sÃ£o postos Ã  prova por aqui. A sociedade israelense nÃ£o Ã© unicamente caracterizada pela sÃ­ndrome do nÃ£o-frayer, e hÃ¡ muitos outros fatores que a caracterizam, pelo bem e pelo mal. Em futuros textos abordarei outras caracterÃ­sticas da sociedade israelense, como o â€œespaÃ§o pessoalâ€, a â€œinter-conectividadeâ€ e outros. . . . Fontes: . Steven Pinker, The Better Angels of Our Nature: Why Violence Has Declined | Thomas Hobbes, O LeviatÃ£ | Luis Roniger e Michael Feige, Tarbut haFrayer vehaZehut haIsraelit. Para ler o artigo em hebraico, entre neste site e faÃ§a download do terceiro artigo. | CanÃ§Ã£o â€œLo Frayerimâ€, da banda HaDag Nachash. Letra em hebraico, portuguÃªs e transliteraÃ§Ã£o no site Shirim em PortuguÃªs. | .",
            "url": "https://yairmau.github.io/website/markdown/2022/02/01/israelense-otario.html",
            "relUrl": "/markdown/2022/02/01/israelense-otario.html",
            "date": " â€¢ Feb 1, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Hino dos partisans judeus",
            "content": ". Em 1943, as notÃ­cias do levante do Gueto de VarsÃ³via se espalharam rapidamente, chegando aos ouvidos de Hirsch Glick, jovem judeu lituano de 21 anos, que morava no Gueto de Vilna. Os mais famosos partisans judeus, liderados por Mordechaj Anielewicz, inspiraram Glick a escrever um poema em yiddish, cuja primeira estrofe de esperanÃ§a dizia: . NÃ£o diga: â€œeste Ã© meu Ãºltimo caminhoO cÃ©u tenebroso escondeu a luz do diaâ€Este Ã© o dia que esperÃ¡vamos que chegasseE a nossa marcha ainda trovejarÃ¡ â€œestamos aquiâ€! . A jovem Rachel Margolis, integrante da â€œOrganizaÃ§Ã£o Partisan Unidaâ€, o grupo da resistÃªncia judaica do gueto, ouviu Hirschke (como ela o chamava carinhosamente) declamar seu poema com emoÃ§Ã£o e, imediatamente, lhe veio Ã  cabeÃ§a uma melodia soviÃ©tica para acompanhar a poesia. Trata-se de uma mar cha dos irmÃ£os Dmitri e Daniel Pokrass, judeus da UniÃ£o SoviÃ©tica, que a compuseram para uma poesia de Alexey Surkov, para fazer parte de um filme de 1938. . Os irmÃ£os Pokrass contaram mais tarde que sua marcha foi baseada na melodia da famosÃ­ssima canÃ§Ã£o yiddish â€œOyfn Pripetshikâ€, aquela que Spielberg, bastante mais tarde, usaria na inesquecÃ­vel cena da menina do casaco vermelho na Lista de Schindler. Isto de certa forma fecha um ciclo, acho eu. . A canÃ§Ã£o â€œZog Nit Keynmolâ€ (sÃ£o as primeiras palavras em yidish) se espalhou rapidamente pelo gueto e muito alÃ©m dele, nos outros guetos, nos campos de concentraÃ§Ã£o, e pelas florestas onde os partisans se escondiam. Claro que a canÃ§Ã£o tambÃ©m chegou ao Yishuv na Palestina, onde o poeta Avraham Shlonsky a traduziu ao hebraico, publicando-a em fevereiro de 1945 no diÃ¡rio HaMishmar. Eu nÃ£o falo yiddish, entÃ£o ofereÃ§o-lhes no final deste artigo a traduÃ§Ã£o ao portuguÃªs da letra em hebraico. . AtÃ© aÃ­ era a histÃ³ria que eu queria contar, de como surgiu o Hino dos Partisans Judeus. Fazendo a pesquisa para este texto, acabei conhecendo um pouco quem foi Rachel Margolis, e a ela eu dedico este artigo. . Rachel Margolis . Rachel Margolis tinha 20 anos quando a LituÃ¢nia foi invadida pelos nazistas. Elas encontrou refÃºgio em uma famÃ­lia de cristÃ£os, mas acabou optando em ir ao gueto de Vilna, para se juntar Ã  resistÃªncia judaica. . â€œTodos estavam ansiosos por lutar. Nossa missÃ£o era adquirir armas, completar preparaÃ§Ãµes militares, tudo isto com o objetivo de fazer um levante no gueto. Se morrÃªssemos, seria com honra, tendo provado Ã  humanidade que nÃ£o somos ovelhas indo silenciosamente ao matadouro.â€ . A comparaÃ§Ã£o com ovelhas Ã© familiar, e nÃ£o por acaso. Esta Ã© uma frase famosa de Abba Kovner, comandante da OrganizaÃ§Ã£o Partisan Unida da qual Rachel fazia parte. . O gueto de Vilna foi liquidado em junho de 1943, e apenas algumas centenas de judeus conseguiram escapar. Rachel foi Ã s florestas lutar com a resistÃªncia, explodindo pontes e linhas de trem, essenciais para o abastecimento dos alemÃ£es. . Rachel foi a Ãºnica pessoa de sua famÃ­lia a sobreviver o holocausto; seu pai, sua mÃ£e, e seu irmÃ£o foram mortos somente alguns dias antes da liberaÃ§Ã£o da LituÃ¢nia em julho de 1944. Depois da guerra, Rachel continuou morando na LituÃ¢nia. Ela recebeu um PhD em biologia, e lecionou atÃ© os anos 1980. Ela tambÃ©m fundou um museu do holocausto, chamado de Casa Verde de Vilna. . Enquanto era estudante de biologia no final dos anos 1940, Rachel foi voluntÃ¡ria na Universidade de Vilna em um museu judaico. Foi entÃ£o que dois poetas yiddish, temporariamente responsÃ¡veis pelo museu, contaram-lhe sobre Kazimierz Sakovicz. Sakovicz era um jornalista polonÃªs catÃ³lico, que testemunhou o massacre de PonÃ¡r, e o relatou em um diÃ¡rio. Quando ele entendeu que talvez nÃ£o sobrevivesse a guerra (ele morreu durante o levante polonÃªs conhecido como OperaÃ§Ã£o Tempestade), enterrou em seu jardim pedaÃ§os de seu diÃ¡rio, dentro de jarros. Depois da guerra, seus vizinhos desenterraram os jarros e os levaram ao museu judaico. Contudo, Margolis teve que esperar atÃ© o colapso da UniÃ£o SoviÃ©tica em 1991 para finalmente ter acesso a estes documentos. Ela transcreveu e reconstituiu meticulosamente o diÃ¡rio de Kazimierz Sakowicz. Com a ediÃ§Ã£o do historiador israelense Yitzhak Arad (ele tambÃ©m participou da resistÃªncia judaica e da soviÃ©tica), o livro foi publicado sob o tÃ­tulo â€œDiÃ¡rio de Ponary, 1941-1943: O relato de uma testemunha de uma assassinato em massaâ€. Em PonÃ¡r foram mortos cerca de 100 mil pessoas pelas mÃ£os da SS e de colaboradores lituÃ¢nos. Dentre os cerca de 70 mil judeus mortos, estÃ¡ a famÃ­lia de Margolis. . Rachel escreveu suas memÃ³rias da guerra e da resistÃªncia no livro â€œUma Partisan de Vilnaâ€. Um dos relatos do livro foi tirado de contexto por antissemitas lituanos, que acusaram a resistÃªncia judaica de cometer uma â€œatrocidade comunistaâ€ na batalha Kanyuki. Em um artigo de Gordon Brown, ex-premiÃª britÃ¢nico, ele conta que diferentemente da Alemanha, a sociedade lituana nunca passou por um perÃ­odo de reconciliaÃ§Ã£o e arrependimento por seu passado nazista, e que hÃ¡ um debate ideolÃ³gico feroz de como descrever a colaboraÃ§Ã£o de lituanos comuns com as forÃ§as de ocupaÃ§Ã£o alemÃ£s. . O novo nacionalismo dos paÃ­ses bÃ¡lticos estÃ¡ reescrevendo a histÃ³ria, colocando a ocupaÃ§Ã£o nazista em pÃ© de igualdade com o regime soviÃ©tico, misturando o que nÃ£o pode ser misturado. Uma comissÃ£o lituana se propÃµe a investigar os crimes dos â€œregimes de ocupaÃ§Ã£oâ€ (nazista e comunista), porÃ©m deixam de fora os crimes de genocÃ­dio cometidos por forÃ§as locais. Sobreviventes do holocausto, como Yitzhak Arad e outros, chegaram a ser investigados por crimes de guerra, sob a alegaÃ§Ã£o de terem se juntado aos soviÃ©ticos na luta contra os nazistas! Rachel Margolis nÃ£o foi deixada de lado e foi chamada para testemunhar a respeito das atividades de outra partisan, Fanya Brantsovsky. . Vale a pena repetir: judeus que participaram da resistÃªncia contra os nazistas estÃ£o sendo acusados de crimes de guerra. Ã‰ a histÃ³ria sendo reescrita sob os nossos olhos, em pleno ano de 2017. . Com medo da intimidaÃ§Ã£o imposta, Rachel Margolis, que jÃ¡ morava em Israel, nÃ£o pÃ´de mais voltar a visitar a LituÃ¢nia durante o verÃ£o, como costumava fazer. Ela faleceu em julho de 2015. Em memÃ³ria a Rachel, aos partisans, e a todos os que foram mortos no holocausto, Ã© mais importante do que nunca seguir dizendo: estamos aqui! . Hino dos partisans judeus . VersÃ£o em hebraico. A versÃ£o em yiddish estÃ¡ no final do texto. . PortuguÃªs TransliteraÃ§Ã£o Hebraico . NÃ£o diga â€œeste Ã© meu Ãºltimo caminhoO cÃ©u tenebroso escondeu a luz do diaâ€Este Ã© o dia que esperÃ¡vamos que chegasseE a nossa marcha ainda trovejarÃ¡ â€œestamos aquiâ€!Da terra da tÃ¢mara atÃ© as remotas terras congeladasEstamos aqui em dores e tormentosE se uma gota de nosso sangue ali se derramarCertamente crescerÃ¡ nossa bravuraA primeira luz da manhÃ£ iluminarÃ¡ nosso diaCom o inimigo nosso â€˜ontemâ€™ passara como uma sombraMas se Deus-nos-livre a luz tardar em virA canÃ§Ã£o serÃ¡ como um hino de geraÃ§Ã£o em geraÃ§Ã£oCom sangue e chumbo ela foi escritaEla nÃ£o Ã© a canÃ§Ã£o dos livres e desimpedidosPois entre paredes que caem todo o povo a cantouCantaram juntos com revÃ³lveres nas mÃ£osPortanto nÃ£o diga â€œmeu Ãºltimo caminhoO cÃ©u tenebroso escondeu a luz do diaâ€Este Ã© o dia que esperÃ¡vamos que chegasseE a nossa marcha ainda trovejarÃ¡ â€œestamos aquiâ€! | Al na tomar â€œhine darki haachronaâ€Et or hayom histiru shmei haananaZe yom nichsafnu lo od yaal veyavoUmitzâ€™adenu od yarâ€™im â€œanachnu poâ€!Meeretz hatamar ad yarketei kforimAnachnu po bemachâ€™ovot veisurimUvaasher tipat damenu sham nigraHalo yanuv od oz ruchenu bigvuraAmud hashachar al yomenu or yahelIm hatzorer yachlof tmolenu kemo tzelAch im chalila yeacher lavo haorKemo sisma yehe hashir midor ledorBichtav hadam vehaoferet hu nichtavHu lo shirat tzipor hadror vehamerchavKi bein kirot noflim sharuhu kol haamYachdav sharuhu venaganim beyadamAl ken al na tomar â€œdarki haarchronaâ€Et or hayom histiru shmei haananaZe yom nichsafnu lo od yaal veyavoUmitzâ€™adenu od yarâ€™im â€œanachnu poâ€! | ×Ö·×œ × Ö¸× ×ªÖ¹Ö¼××Ö·×¨: ×”Ö´× ÖµÖ¼×” ×“Ö·Ö¼×¨Ö°×›Ö´Ö¼×™ ×”Ö¸×Ö·×—Ö²×¨×•Ö¹× Ö¸×”,×Ö¶×ª ××•Ö¹×¨ ×”Ö·×™Ö¼×•Ö¹× ×”Ö´×¡Ö°×ªÖ´Ö¼×™×¨×•Ö¼ ×©Ö°××Öµ×™ ×”Ö¸×¢Ö²× Ö¸× Ö¸×”.×–Ö¶×” ×™×•Ö¹× × Ö´×›Ö°×¡Ö·×¤Ö°× ×•Ö¼ ×œ×•Ö¹ ×¢×•Ö¹×“ ×™Ö·×¢Ö·×œ ×•Ö°×™Ö¸×‘×•Ö¹×,×•Ö¼×Ö´×¦Ö°×¢Ö¸×“Öµ× ×•Ö¼ ×¢×•Ö¹×“ ×™Ö·×¨Ö°×¢Ö´×™×: ×Ö²× Ö·×—Ö°× ×•Ö¼ ×¤Ö¹Ö¼×”!×Öµ×Ö¶×¨Ö¶×¥ ×”Ö·×ªÖ¸Ö¼×Ö¸×¨ ×¢Ö·×“ ×™Ö·×¨Ö°×›Ö°Ö¼×ªÖµ×™ ×›Ö°Ö¼×¤×•Ö¹×¨Ö´×™××Ö²× Ö·×—Ö°× ×•Ö¼ ×¤Ö¹Ö¼×” ×‘Ö°Ö¼×Ö·×›Ö°××•Ö¹×‘×•Ö¹×ª ×•Ö°×™Ö´×¡Ö¼×•Ö¼×¨Ö´×™××•Ö¼×‘Ö·×Ö²×©Ö¶××¨ ×˜Ö´×¤Ö·Ö¼×ª ×“Ö¸Ö¼×Öµ× ×•Ö¼ ×©Ö¸×× × Ö´×’Ö°Ö¼×¨Ö¸×”×”Ö²×œÖ¹× ×™Ö¸× ×•Ö¼×‘ ×¢×•Ö¹×“ ×¢Ö¹×– ×¨×•Ö¼×—Öµ× ×•Ö¼ ×‘Ö´Ö¼×’Ö°×‘×•Ö¼×¨Ö¸×”.×¢Ö·×Ö¼×•Ö¼×“ ×”Ö·×©Ö·Ö¼××—Ö·×¨ ×¢Ö·×œ ×™×•Ö¹×Öµ× ×•Ö¼ ××•Ö¹×¨ ×™Ö¸×”Öµ×œ.×¢Ö´× ×”Ö·×¦Ö¼×•Ö¹×¨Öµ×¨ ×™Ö·×—Ö²×œÖ¹×£ ×ªÖ°Ö¼××•Ö¹×œÖµ× ×•Ö¼ ×›Ö°Ö¼××•Ö¹ ×¦Öµ×œ.×Ö·×šÖ° ×Ö´× ×—Ö¸×œÖ´×™×œÖ¸×” ×™Ö°×Ö·×—Öµ×¨ ×œÖ¸×‘×•Ö¹× ×”Ö¸××•Ö¹×¨×›Ö°Ö¼××•Ö¹ ×¡Ö´×™×¡Ö°×Ö¸×” ×™Ö°×”Öµ× ×”Ö·×©Ö´Ö¼××™×¨ ×Ö´×“Ö¼×•Ö¹×¨ ×œÖ°×“×•Ö¹×¨.×‘Ö´Ö¼×›Ö°×ªÖ·×‘ ×”Ö·×“Ö¸Ö¼× ×•Ö°×”Ö¸×¢×•Ö¹×¤Ö¶×¨Ö¶×ª ×”×•Ö¼× × Ö´×›Ö°×ªÖ·Ö¼×‘;×”×•Ö¼× ×œÖ¹× ×©Ö´××™×¨Ö·×ª ×¦Ö´×¤Ö¼×•Ö¹×¨ ×”Ö·×“Ö°Ö¼×¨×•Ö¹×¨ ×•Ö°×”Ö·×Ö¶Ö¼×¨Ö°×—Ö¸×‘,×›Ö´Ö¼×™ ×‘ÖµÖ¼×™×Ÿ ×§Ö´×™×¨×•Ö¹×ª × ×•Ö¹×¤Ö°×œÖ´×™× ×©Ö¸××¨×•Ö¼×”×•Ö¼ ×›Ö¸Ö¼×œ ×”Ö¸×¢Ö¸×,×™Ö·×—Ö°×“Ö¸Ö¼×™×• ×©Ö¸××¨×•Ö¼×”×•Ö¼ ×•Ö°× ×Ö·×’×Ö·× Ö´×™× ×‘Ö°Ö¼×™Ö¸×“Ö¸×.×¢Ö·×œ ×›ÖµÖ¼×Ÿ ×Ö·×œ × Ö¸× ×ªÖ¹Ö¼××Ö·×¨: ×“Ö·Ö¼×¨Ö°×›Ö´Ö¼×™ ×”Ö¸×Ö·×—Ö²×¨×•Ö¹× Ö¸×”×Ö¶×ª ××•Ö¹×¨ ×”Ö·×™Ö¼×•Ö¹× ×”Ö´×¡Ö°×ªÖ´Ö¼×™×¨×•Ö¼ ×©Ö°××Öµ×™ ×”Ö¸×¢Ö²× Ö¸× Ö¸×”.×–Ö¶×” ×™×•Ö¹× × Ö´×›Ö°×¡Ö·×¤Ö°× ×•Ö¼ ×œ×•Ö¹ ×¢×•Ö¹×“ ×™Ö·×¢Ö·×œ ×•Ö°×™Ö¸×‘×•Ö¹×,×•Ö¼×Ö´×¦Ö°×¢Ö¸×“Öµ× ×•Ö¼ ×¢×•Ö¹×“ ×™Ö·×¨Ö°×¢Ö´×™×: ×Ö²× Ö·×—Ö°× ×•Ö¼ ×¤Ö¹Ö¼×”! | . VersÃ£o em yiddish, cantada por Chava Alberstein. . . Imagem de destaque: Pilar da Bravura, no museu do holocausto Yad vaShem, em JerusalÃ©m. CrÃ©dito da imagem, Flickr de Benjamin, segundo a seguinte licenÃ§a Creative Commons. . Fontes: Zemereshet, Oneg Shabbat, YouTube, keene.edu, defendinghistory.com, Amazon, Academic Studies Press, Independent, Wikipedia 1 e 2. .",
            "url": "https://yairmau.github.io/website/markdown/2022/02/01/hino-dos-partisans-judeus.html",
            "relUrl": "/markdown/2022/02/01/hino-dos-partisans-judeus.html",
            "date": " â€¢ Feb 1, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Eu vos declaro sogro e genro",
            "content": ". O sogro Ã© quem casa com o noivo, nÃ£o a noiva. Pelo menos Ã© assim que acontecia na Ã©poca bÃ­blica. Os exemplos sÃ£o fartos e nÃ£o deixam dÃºvida alguma. . E SalomÃ£o casou-se com FaraÃ³, rei do Egito; e tomou a filha de FaraÃ³â€¦ ×•Ö·×™Ö´Ö¼×ªÖ°×—Ö·×ªÖµÖ¼×Ÿ ×©Ö°××œÖ¹×Ö¹×”, ×Ö¶×ª-×¤Ö·Ö¼×¨Ö°×¢Ö¹×” ×Ö¶×œÖ¶×šÖ° ×Ö´×¦Ö°×¨Ö¸×™Ö´×; ×•Ö·×™Ö´Ö¼×§Ö·Ö¼×— ×Ö¶×ª-×‘Ö·Ö¼×ª-×¤Ö·Ö¼×¨Ö°×¢Ö¹×” 1 Reis 3:1 . A mulher Ã© tomada, levada, e os dois homens, genro e sogro, sÃ£o aqueles que se casam. Outro exemplo. . Nem te casarÃ¡s com eles; nÃ£o darÃ¡s tuas filhas a seus filhos, e nÃ£o tomarÃ¡s suas filhas para teus filhos. ×•Ö°×œÖ¹× ×ªÖ´×ªÖ°×—Ö·×ªÖµÖ¼×Ÿ, ×‘Ö¸Ö¼×: ×‘Ö´Ö¼×ªÖ°Ö¼×šÖ¸ ×œÖ¹×-×ªÖ´×ªÖµÖ¼×Ÿ ×œÖ´×‘Ö°× ×•Ö¹, ×•Ö¼×‘Ö´×ªÖ¼×•Ö¹ ×œÖ¹×-×ªÖ´×§Ö·Ö¼×— ×œÖ´×‘Ö°× Ö¶×šÖ¸ DeuteronÃ´mio 7:3 . Ã‰ assim que Deus proÃ­be o povo de se misturar com os povos que jÃ¡ viviam em CanaÃ£ antes da chegada dos hebreus. As traduÃ§Ãµes acima sÃ£o adaptaÃ§Ãµes minhas, hoje em dia vocÃªs encontrarÃ£o o verbo â€œaparentar-seâ€ no lugar de casar-se. Em hebraico, a forma reflexiva hitCHaTeN (×”Ö´×ªÖ°×—Ö·×ªÖµÖ¼×Ÿ) Ã© usada nos dois exemplos, e indica que o casamento era a forma como dois homens (ou duas famÃ­lias) firmavam acordos e parcerias. Os termos para sogro e genro tambÃ©m tem a mesma raiz (CH-T-N). Sogro Ã© CHoTeN (×—×•Ö¹×ªÖµ×Ÿ), e genro Ã© CHaTaN (×—Ö¸×ªÖ¸×Ÿ). . Com o passar do tempo, Chatan ganhou o significado de noivo, e Ã© o par de KalÃ¡, a noiva. Assim diz uma das 7 benÃ§Ã£os proferidas no casamento judaico: â€œSe escutarÃ¡ nas cidades de JudÃ¡ e nas ruas de JerusalÃ©m, vozes de alegria e vozes de felicidade, voz do noivo e voz da noivaâ€ ( ×™Ö´×©Ö¸Ö¼××Ö·×¢ ×‘Ö°Ö¼×¢Ö¸×¨Öµ×™ ×™Ö°×”×•Ö¼×“Ö¸×” ×•Ö¼×‘Ö°×—×•Ö¼×¦×•Ö¹×ª ×™Ö°×¨×•Ö¼×©Ö¸××œÖ·×™Ö´×, ×§×•Ö¹×œ ×©Ö¸×‚×©×‚×•Ö¹×Ÿ ×•Ö°×§×•Ö¹×œ ×©Ö´×‚×Ö°×—Ö¸×”, ×§×•Ö¹×œ ×—Ö¸×ªÖ¸×Ÿ ×•Ö°×§×•Ö¹×œ ×›Ö·Ö¼×œÖ¸Ö¼×”, adaptaÃ§Ã£o de Jeremias 33:10-11). Contudo, a mesma frase entendida segundo o hebraico bÃ­blico diz â€œvoz do genro e voz da noraâ€. . E o que significa KalÃ¡ (×›Ö·Ö¼×œÖ¸Ö¼×”), a noiva ou nora? Vem da raiz KLL (×›×œ×œ), que significa â€œcompletoâ€ ou perfeito, embora muitas sogras insistam em discordar desta etimologia. Outras lÃ­nguas semÃ­ticas tem a mesma palavra, como na lÃ­ngua acÃ¡dia (kallatu) e arameu (kalta). . Antes de seguirmos adiante, uma curta observaÃ§Ã£o. Choten Ã© o sogro do noivo, mas o sogro da noiva tem outro nome: Cham (×—Ö¸×). Assim, os sogros do noivo sÃ£o Choten e Chotenet, e os sogros da noiva sÃ£o Cham e Chamot. Cham Ã© derivado do verbo â€˜Amam (×¢××), com o significado de congregar, juntar. Do mesmo verbo tambÃ©m sÃ£o derivadas as palavras â€˜am (×¢Ö·× = povo), â€˜im (×¢Ö´× = com) e Gam (×’Ö·Ö¼× = tambÃ©m). . O hebraico, lÃ­ngua machista . Pois bem, vimos que na bÃ­blia hebraica (Tanach), a mulher nÃ£o Ã© o sujeito no casamento entre genro e sogro, e sim o â€œobjetoâ€. SerÃ¡ que nÃ£o dÃ¡ pra encontrar evidÃªncias que aliviem essa visÃ£o machista? Dependeâ€¦ . Como se diz esposo e esposa? Simples. Esposo Ã© Baâ€™al (×‘Ö·Ö¼×¢Ö·×œ), que significa literalmente â€œdonoâ€, ou â€œsenhorâ€, e esposa Ã© Isha (×Ö´×©Ö¸Ö¼××”), que Ã© literalmente â€œmulherâ€. A coisa nÃ£o estÃ¡ bonita para o hebraico. Felizmente, quando alguÃ©m fala baâ€™al em hebraico no contexto de um casamento, sempre se entende â€œmaridoâ€, ninguÃ©m nunca pensa em â€œdonoâ€. Da mesma forma que numa indÃºstria textil do Brasil, â€œmangaâ€ Ã© quase certamente a da roupa, e nÃ£o a fruta. . Quem vem salvar o dia Ã© o Tanach (pelo menos no que diz respeito a baâ€™al). Ao longo de todo o Tanach, â€œishiâ€ (×Ö´×™×©Ö´××™ = meu homem) Ã© usado 83 vezes com o sentido de marido. Assim, em GÃªnesis 16:3, lemos que . â€¦ tomou Sarai, mulher de AbrÃ£o, a Agar egÃ­pcia, sua serva, e deu-a por mulher a AbrÃ£o seu marido, ao fim de dez anos que AbrÃ£o habitara na terra de CanaÃ£. (×•Ö·×ªÖ´Ö¼×§Ö·Ö¼×— ×©Ö¸×‚×¨Ö·×™ ×Öµ×©Ö¶××ª-×Ö·×‘Ö°×¨Ö¸×, ×Ö¶×ª-×”Ö¸×’Ö¸×¨ ×”Ö·×Ö´Ö¼×¦Ö°×¨Ö´×™×ª ×©Ö´××¤Ö°×—Ö¸×ªÖ¸×”Ö¼, ×Ö´×§ÖµÖ¼×¥ ×¢Ö¶×©Ö¶×‚×¨ ×©Ö¸×× Ö´×™×, ×œÖ°×©Ö¶××‘Ö¶×ª ×Ö·×‘Ö°×¨Ö¸× ×‘Ö°Ö¼×Ö¶×¨Ö¶×¥ ×›Ö°Ö¼× Ö¸×¢Ö·×Ÿ; ×•Ö·×ªÖ´Ö¼×ªÖµÖ¼×Ÿ ×Ö¹×ªÖ¸×”Ö¼ ×œÖ°×Ö·×‘Ö°×¨Ö¸× ×Ö´×™×©Ö¸××”Ö¼, ×œ×•Ö¹ ×œÖ°×Ö´×©Ö¸Ö¼××”) . Em contra-partida, baâ€™al Ã© usado apenas 10 vezes no Tanach com o sentido de marido, e todas as vezes relacionado com o coito, que em hebraico se diz beâ€™ilÃ¡ (×‘Ö°Ö¼×¢Ö´×™×œÖ¸×”). Apenas mais tarde, na Ã©poca do Talmud, a palavra baâ€™al tornou-se mais comum para designar o marido. . E assim ficou atÃ© os dias de hoje. De vez em quando se lÃª por aÃ­ que um grupo de pessoas (sempre mulheres) querem mudar o termo marido de volta para â€œishiâ€ (meu homem). Uma histÃ³ria interessante Ã© a da parlamentar Ada Maimon. Conhecida por sua atuaÃ§Ã£o pela igualdade dos sexos em Israel, Maimon pediu ao entÃ£o primeiro-ministro David Ben-Gurion para que tratasse do tema. O primeiro-ministro, por sua vez, escreveu uma carta oficial ao ministro das finanÃ§as (datada de 5 de maio de 1953, veja imagem abaixo), pedindo que se trocasse a palavra â€œbaaliâ€ por â€œishiâ€ em um dos formulÃ¡rios do governo. Segundo Ben-Gurion, â€œa palavra baâ€™al tem significado de autoridade e paganismo [Baâ€™al tambÃ©m Ã© o nome de um deus de vÃ¡rios povos da regiÃ£o], e nÃ£o se adequa ao princÃ­pio de respeito Ã s mulheres, que tem direitos iguais aos dos homens.â€ Ben-Gurion, que amava o Tanach, ainda citou um versÃ­culo de OsÃ©ias 2:18 . â€¦ tu me chamarÃ¡s meu homem; e nÃ£o mais me chamarÃ¡s meu senhor. ×ªÖ´Ö¼×§Ö°×¨Ö°×Ö´×™ ×Ö´×™×©Ö´××™; ×•Ö°×œÖ¹×-×ªÖ´×§Ö°×¨Ö°×Ö´×™-×œÖ´×™ ×¢×•Ö¹×“, ×‘Ö·Ö¼×¢Ö°×œÖ´×™. . Carta de Ben-Gurion de 1953 . O noivo, o premiado, e o circuncidado . Ã‰ interessante notar que Chatan, alÃ©m de noivo/genro, tambÃ©m Ã© usado para designar alguÃ©m que ganhou um prÃªmio importante. Por exemplo, Shimon Peres Ã© â€œchatan prÃ¡s nobelâ€ (×—×ª×Ÿ ×¤×¨×¡ × ×•×‘×œ), ou seja, laureado com o prÃªmio Nobel. Em portuguÃªs a honra vem do louro, e em hebraico? O que tem a ver o prÃªmio com o genro/noivo? . Aqui o Ã¡rabe, lÃ­ngua-irmÃ£ do hebraico, poderÃ¡ nos ajudar. O â€œHebrew and English Lexicon of the Old Testamentâ€ [Gesenius, 1850] diz que originalmente a raiz CH-T-N significava apenas â€œcircuncidarâ€. Em Ã¡rabe menino circuncidado se diz chatin (Ø®ÙØªÙÙŠÙ†), e circuncisÃ£o Ã© chitan (Ø®ØªØ§Ù†). A partir da grande festa que se faz na ocasiÃ£o da circuncisÃ£o, a raiz CH-T-N ganhou o significado de â€œgrande festaâ€ e outras associaÃ§Ãµes com os personagens principais de uma festa. DaÃ­ pode-se entender como em hebraico a raiz evoluiu para significar casamento (chatunÃ¡ = ×—Ö²×ª×•Ö¼× Ö¸Ö¼×”) e seus envolvidos, o genro (chatan = ×—Ö¸×ªÖ¸×Ÿ) e o sogro (choten = ×—×•Ö¹×ªÖµ×Ÿ). . A raiz CH-T-N tambÃ©m Ã© usada em hebraico para outras grandes festas alÃ©m do casamento. Ã‰ assim que chegamos ao â€œchatan prÃ¡s nobelâ€, ou â€œaquele para quem se estÃ¡ fazendo uma grande festa pela ocasiÃ£o do recebimento do prÃªmio Nobelâ€. HÃ¡ outros usos de chatan, por exemplo na festa judaica de Simchat TorÃ¡. Ã‰ nesta data em que se termina o pentateuco, com o Chatan TorÃ¡ lendo a porÃ§Ã£o semanal â€œVeZot HaBrachaâ€, e logo depois o Chatan Bereshit comeÃ§ando novamente o pentateuco com a porÃ§Ã£o semanal â€œBereshitâ€. Mais um exemplo de uma grande festa, com duas pessoas honradas chamadas de chatan. Quando a homenageada Ã© uma mulher, usa-se â€œkalÃ¡â€ (×›Ö·Ö¼×œÖ¸Ö¼×” = nora, noiva). . JÃ¡ em Ã¡rabe, a palavra â€œcasamentoâ€ tem outra raiz, se diz zawaj (Ø²ÙÙˆÙØ§Ø¬), e estÃ¡ ligada a zug (×–×•×’) em hebraico, que significa â€œpar, duplaâ€. [Zawaj.com Ã© o JDate muÃ§ulmano, para quem estiver interessadoâ€¦ enquanto que judeus querem sÃ³ um â€œdateâ€, os muÃ§ulmanos querem zawaj mesmo.] Entretanto, em Ã¡rabe a palavra chatan (Ø®ÙØªÙÙ†) Ã© usada para â€œrelacionamentos por meio de casamentoâ€, como o inglÃªs â€œin-lawâ€. No hebraico de hoje, pode-se dizer que os pais e mÃ£es do noivo e da noiva sÃ£o â€œmechutanimâ€ (××—×•×ª× ×™×), ou ligados por um casamento, mas este tratamento nÃ£o passa a cunhados e demais familiares, vale apenas para os pais. . Na bÃ­blica hebraica, o significado de circuncisÃ£o jÃ¡ havia se desconectado da raiz CH-T-N, com uma excessÃ£o interessantÃ­ssima. A histÃ³ria Ã© a de MoisÃ©s, que estÃ¡ voltando ao Egito sob ordens divinas para libertar os hebreus do faraÃ³. No meio do caminho, por algum motivo nÃ£o explicado, Deus resolve matar MoisÃ©s. Vem entÃ£o ZÃ­pora, sua esposa, e faz o seguinte: . EntÃ£o ZÃ­pora tomou uma pedra aguda, e circuncidou o prepÃºcio de seu filho, e lanÃ§ou-o a seus pÃ©s, e disse: Certamente me Ã©s um noivo de sangue. E desviou-se dele. EntÃ£o ela disse: Noivo de sangue, por causa da circuncisÃ£o. ×•Ö·×ªÖ´Ö¼×§Ö·Ö¼×— ×¦Ö´×¤Ö¹Ö¼×¨Ö¸×” ×¦Ö¹×¨, ×•Ö·×ªÖ´Ö¼×›Ö°×¨Ö¹×ª ×Ö¶×ª-×¢Ö¸×¨Ö°×œÖ·×ª ×‘Ö°Ö¼× Ö¸×”Ö¼, ×•Ö·×ªÖ·Ö¼×’Ö·Ö¼×¢, ×œÖ°×¨Ö·×’Ö°×œÖ¸×™×•; ×•Ö·×ªÖ¹Ö¼××Ö¶×¨, ×›Ö´Ö¼×™ ×—Ö²×ªÖ·×Ÿ-×“Ö¸Ö¼×Ö´×™× ×Ö·×ªÖ¸Ö¼×” ×œÖ´×™. ×›×• ×•Ö·×™Ö´Ö¼×¨Ö¶×£, ×Ö´×Ö¶Ö¼× Ö¼×•Ö¼; ×Ö¸×–, ×Ö¸×Ö°×¨Ö¸×”, ×—Ö²×ªÖ·×Ÿ ×“Ö¸Ö¼×Ö´×™×, ×œÖ·×Ö¼×•Ö¼×œÖ¹×ª. ÃŠxodo 4:25-26 . Esta passagem faz tanto sentido em hebraico como em portuguÃªs: nenhum sentido. Mas, como jÃ¡ somos mais inteligentes, e sabemos que noivo (chatan) pode ser aquele em que se fez a circuncisÃ£o, tudo fica mais claro. A passagem acima foi e Ã© ainda amplamente debatida pelos estudiosos judeus, com mil e uma explicaÃ§Ãµes do porque Deus queria matar MoisÃ©s e como a circuncisÃ£o de seu filho o salvou. Destaco uma lenda (midrash) que diz que o anjo Uriel virou uma serpente e engoliu MoisÃ©s da cabeÃ§a ao pÃªnis, deixando a ponta de fora. ZÃ­pora entendeu o recado e entÃ£o cortou o prepÃºcio de seu filho para salvar o marido. O texto jÃ¡ estava longo, mas eu nÃ£o pude deixar de fora este lindo midrash. . O que aprendemos? . 1 â€“ O hebraico Ã© uma lÃ­ngua machista, pois as pessoas hÃ¡ muito tempo atrÃ¡s eram machistas. Muitas ainda o sÃ£o atÃ© hoje. 2 â€“ A raiz de trÃªs letras CH-T-N tem uma histÃ³ria fascinante. 3 â€“ Os sÃ¡bios que explicaram a TorÃ¡ sÃ£o muito criativos. . Nos prÃ³ximos textos continuaremos a aprender o que a lÃ­ngua hebraica tem a nos ensinar sobre outros parentescos, como pais, mÃ£es, filhos, netos e primos. . Fontes: Safa-ivrit.org, Ynet, IsraelPost, Hebrew and English Lexicon of the Old Testament [Gesenius, 1850], midrash da histÃ³ria de Moises. .",
            "url": "https://yairmau.github.io/website/markdown/2022/02/01/eu-vos-declaro.html",
            "relUrl": "/markdown/2022/02/01/eu-vos-declaro.html",
            "date": " â€¢ Feb 1, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Ayin: o Ãºltimo som antes de vomitar",
            "content": ". O hebraico tem um som fascinante, representado pela letra Ayin. A melhor explicaÃ§Ã£o que jÃ¡ escutei de como pronunciar Ã© â€œfazer o som logo antes de vomitarâ€. Como nÃ£o amar esta letra?! . Ayin Ã© olho . Literalmente, Ayin significa â€œolhoâ€. Os fenÃ­cios, pioneiros na criaÃ§Ã£o do alfabeto, tomaram o hieroglifo egÃ­pcio â€œolhoâ€, e fizeram dele uma letra redonda, tambÃ©m indicando olho, ou melhor, o primeiro som desta palavra. Do alfabeto fenÃ­cio, a letra Ayin ganhou outras formas nas demais linguas semÃ­ticas, vejam: . . O grego e o latim criaram seus alfabetos baseados no alfabeto fenÃ­cio, mas algumas das letras ali indicavam sons que PÃ©ricles e RÃ´mulo nÃ£o reconheceriam. Foi assim que o Ayin, de formato redondo, ganhou um novo significado, o da vogal O. . Ayin Ã© passÃ© . No hebraico que se fala hoje em dia em Israel, a letra Ayin nÃ£o tem mais o seu som tradicional. Os imigrantes judeus Ã  Terra de Israel durante a primeira metade do sÃ©culo 20 eram predominantemente da Europa, e portanto nÃ£o sabiam pronunciar esta letra devidamente. Assim, o hebraico mainstream ficou sem este som, apesar da letra seguir sendo usada. A situaÃ§Ã£o lembra o uso da letra H no portuguÃªs, que nÃ£o tem som algum, e sÃ³ Ã© usada para manter-se a grafia histÃ³rica das palavras. (Olha sÃ³ o H em â€œhistÃ³ricaâ€, que ninguÃ©m pronunciou! Ã‰ mais ou menos assim no hebraico de hoje) . Os judeus do Norte da Ãfrica e do Oriente MÃ©dio (chamados de â€œorientaisâ€ em Israel, mizrachim), que sabiam pronunciar corretamente o Ayin, quando chegaram em Israel na segunda metade do sÃ©culo 20, acabaram por adotar muitos dos costumes jÃ¡ instaurados, inclusive o abandono da pronÃºncia do Ayin. Hoje em Israel, apenas os mais velhos destas imigraÃ§Ãµes mantÃªm o Ayin, e nenhuma crianÃ§a aprende a falar hebraico assim, mesmo que seus pais e avÃ³s o falem. Como eu sei disso? Minha esposa Ã© professora, e deu aula em duas escolas no sul de Israel, onde boa parte da populaÃ§Ã£o Ã© descendente dos â€œorientaisâ€. Nenhuma das mais de 600 crianÃ§as com as quais esteve em contato pronunciava a letra Ayin. . Apesar das novas geraÃ§Ãµes nÃ£o usarem o Ayin, muitas pessoas sabem pronunciar esta consoante, e o uso Ã© feito em diversas circunstÃ¢ncias. a. Para diferenciar duas palavras parecidas. Felicidade (osher = ××•×©×¨) e riqueza (osher = ×¢×•×©×¨) comeÃ§am com Alef e Ayin, respectivamente. Pode-se dizer â€œosher com Ayinâ€, ou entÃ£o pronunciar logo o coitado do som que ficou para trÃ¡s. E como faz pra nÃ£o confundir as duas palavras quando fala-se â€œnormalâ€, sem o Ayin? Do mesmo jeito que se diferencia a â€œmangaâ€ da salada de frutas com a â€œmangaâ€ da camisa. Contexto. b. Em humor, o uso do Ayin Ã© um jeito fÃ¡cil e imediato de caracterizar o personagem como â€œorientalâ€, e muitas vezes alguÃ©m de classe sÃ³cio-econÃ´mica mais baixa. c. Na mÃºsica â€œorientalâ€. Hoje em dia a maior parte dos cantores de mÃºsica mizrachit jÃ¡ nasceram sem falar o Ayin, apesar de escutarem o som em casa. Para dar autenticidade, o Ayin Ã© fundamental quando eles cantam suas canÃ§Ãµes, mas Ã© sÃ³ a mÃºsica acabar que seu uso Ã© deixado de lado. Tem outra letra bem tÃ­pica, o Chet, esse sim Ã© outra hisÃ³ria, provavelmente para outro artigo. Aqui nÃ£o. . Ã‰ sÃ³ dar uma vomitadinha . A simpÃ¡tica Maha Yacoub Ã© uma israelense palestina de Kfar Yassim, perto da cidade de Akko, em Israel. Hoje, ela mora na ItÃ¡lia, onde ensina Ã¡rabe. Em seu canal do youtube â€œLearn Arabic with Mahaâ€, que tem hoje quase 250 mil assinantes (um nÃºmero MUITO alto), podemos encontrar vÃ­deos que ensinam expressÃµes, costumes, gramÃ¡tica, e claro, a fonÃ©tica Ã¡rabe. . Eu sempre achei que o som do Ayin se parece com o Ãºltimo som antes de se vomitar, mas a querida Maha acha que parece mesmo com estrangulamento. Seja como for, o que hÃ¡ de comum entre as duas explicaÃ§Ãµes Ã© que o som tem que vir bem do fundo da garganta, como resultado da compressÃ£o da faringe. Chega de falar, vejam a prÃ³pria Maha explicando como Ã© o Ayin em Ã¡rabe. O vÃ­deo tem 10 minutos, sugiro pular logo para o minuto 3:30, Ã© bem divertido. . Ayin Ã© tempo . A palavra mais comum para â€œtempoâ€ em hebraico moderno Ã© Zman, que vem na verdade do persa Zaman, cuja raiz tem o significado de andar. Em portuguÃªs o tempo voa, mas em persa ele vai a pÃ© mesmo. . A palavra hebraica para tempo Ã© ET, que se soletra Ayin Tav (×¢Öµ×ª). Esta Ã© uma palavra feminina, conforme indicado pelo sufixo Tav. Tirando-se o sufixo, temos a mais pura essÃªncia do tempo: apenas a letra Ayin. HÃ¡ vÃ¡rios anos minha professora do ulpan (escola de hebraico) me revelou este fato, e desde entÃ£o estou coletando palavras hebraicas relacionadas com o tempo, todas com Ayin. Procurem a seguinte letra: ×¢. . Et â€“ ×¢Öµ×ª Significado: tempo, estaÃ§Ã£o; era, Ã©poca, idade Frase: ×œÖ·×›Ö¹Ö¼×œ, ×–Ö°×Ö¸×Ÿ; ×•Ö°×¢Öµ×ª ×œÖ°×›Ö¸×œ-×—Öµ×¤Ö¶×¥, ×ªÖ·Ö¼×—Ö·×ª ×”Ö·×©Ö¸Ö¼××Ö¸×™Ö´×. ×¢Öµ×ª ×œÖ¸×œÖ¶×“Ö¶×ª, ×•Ö°×¢Öµ×ª ×œÖ¸××•Ö¼×ª; ×¢Öµ×ª ×œÖ¸×˜Ö·×¢Ö·×ª, ×•Ö°×¢Öµ×ª ×œÖ·×¢Ö²×§×•Ö¹×¨ × Ö¸×˜×•Ö¼×¢Ö· . OnÃ¡ â€“ ×¢×•Ö¹× Ö¸×” Significado: estaÃ§Ã£o (do ano); perÃ­odo Frase: ×”××‘×™×‘ ×”×•× ×¢×•× ×” ×©×œ ×œ×‘×œ×•×‘ ×•×¤×¨×™×—×” . Achshav â€“ ×¢Ö·×›Ö°×©Ö¸××™×• Significado: agora Frase: ××” ×”×©×¢×” ×¢×›×©×™×• . Ad â€“ ×¢Ö·×“ Significado: atÃ©; eternidade Frases: ×”×•× ×™×™×©××¨ ×›××Ÿ ×¢×“ ××—×¨; ×”××“× ××™× ×• ×—×™ ×œ×¢×“ . Adain â€“ ×¢Ö²×“Ö·×™Ö´×Ÿ &amp; Od â€“ ×¢×•Ö¹×“ Significado: ainda Frases: ×”×•× ×¢×“×™×™×Ÿ ×œ× ×”×’×™×¢, ×¢×•×“ ×œ× ××‘×“×” ×ª×§×•×•×ª× ×• . Moed â€“ ××•Ö¹×¢Öµ×“ Significado: um tempo especÃ­fico; uma festa Frase: ×”×•×—×œ×˜ ×œ×”×§×“×™× ×›×›×œ ×”××¤×©×¨ ××ª ××•×¢×“ ×”×•×•×¢×™×“×” . Shaâ€™a â€“ ×©Ö¸××¢Ö¸×” Significado: hora; Ã©poca, tempo Frases: ××” ×”×©×¢×” ×¢×›×©×™×•; ×‘×©×¢×” ×§×©×” ×–×• ×¢×œ×™× ×• ×œ×”×™×•×ª ×××•×—×“×™× . Olam â€“ ×¢×•Ö¹×œÖ¸× Significado: eternidade Frases: ××”×‘×ª× ×• ×”×™× ××”×‘×ª ×¢×•×œ×, ×œÖ°×¢×•Ö¹×œÖ°×Öµ×™ ×¢×•Ö¹×œÖ¸×Ö´×™× . LeOlam, MeOlam â€“ ×œÖ°×¢×•Ö¹×œÖ¸×, ×Öµ×¢×•Ö¹×œÖ¸× Significado: sempre (futuro e passado) Frases: ×××– ×•××¢×•×œ×; ×œ×¢×•×œ× ××–×›×•×¨ ××ª ×™××™ ×™×œ×“×•×ª×™ ×”×™×¤×™× . Rega â€“ ×¨Ö¶×’Ö·×¢ Significado: momento, instante Frase: ×œ×¤× ×™ ×¨×’×¢ ×¨××™×ª×™ ××•×ª×• . NÃ£o fale assim . Amigos leitores, treinem dizer o som da letra Ayin, mas por favor, nÃ£o falem hebraico usando-a. Ã‰ legal usar este som para aprender a ortografia correta das palavras, ou para fazer graÃ§a dizendo gaâ€™aguâ€™a (×’×¢×’×•×¢). Esta Ãºltima palavra, aliÃ¡s, representa um conceito que nÃ£o Ã© exclusivo do portuguÃªs, â€œsaudadeâ€, o que parece que os israelenses nÃ£o tem mais pela letra Ayin. .",
            "url": "https://yairmau.github.io/website/markdown/2022/02/01/ayin.html",
            "relUrl": "/markdown/2022/02/01/ayin.html",
            "date": " â€¢ Feb 1, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "As moedas do Estado Judeu: ontem e hoje",
            "content": ". A ideia Ã© genial. Uma naÃ§Ã£o milenar decide mostrar a sua ligaÃ§Ã£o Ã  terra atravÃ©s de suas moedas. Assim, Israel resgatou a menorÃ¡ do Ãºltimo rei Hasmoneu, a harpa do perÃ­odo de Bar Kochva, e o melhor de todos, a tamareira. Esta havia sido usada pelos romanos para representar o povo judeu derrotado, e agora Ã© representada em toda a sua glÃ³ria na mais alta das moedas, â€œpela redenÃ§Ã£o de SiÃ£oâ€. Chupa Titus Flavius! . CurtÃ­ssimos esclarecimentos . A moeda de Israel Ã© o Shekel Novo, que em 1985 substituiu o Shekel, com taxa de conversÃ£o de 1 Shekel Novo para 1000 Shekels (medida tomada contra a hiperinflaÃ§Ã£o dos anos 1980). Para evitar dizer Shekel Novo o tempo todo, direi simplesmente Shekel, e quando a intenÃ§Ã£o for o de antes de 1985, direi â€œShekel Velhoâ€. â€œShekel Antigoâ€ fica para a moeda usada hÃ¡ milÃªnios atrÃ¡s. AEC e EC significam Antes da Era Comum e Era Comum, respectivamente. . 1 agorÃ¡ . A agorÃ¡ Ã© a menor divisÃ£o do Shekel, valendo um centÃ©simo ou, em outras palavras, centavo. A moeda tem um valor tÃ£o baixo que o Banco de Israel decidiu interromper sua produÃ§Ã£o em 1991. . . O nome â€œagorÃ¡â€ foi proposto pela Academia da LÃ­ngua Hebraica, jÃ¡ em 1960, quando a Lira Israelense deixou de ser dividida em 1000 prutot (prutÃ¡ no singular), e precisava-se de um nome para â€œcentavoâ€. Encontraram â€œagorÃ¡â€ no versÃ­culo 1 Samuel 2:36, onde â€œagorat kesefâ€ (×Ö²×’×•Ö¹×¨Ö·×ª ×›Ö¶Ö¼×¡Ö¶×£) Ã© entendido como â€œum pedaÃ§o de prataâ€. . O desenho do navio foi herdado da moeda de 10 Shkalim Velhos (shkalim Ã© o plural de shekel), uma vez que a taxa de conversÃ£o era 1 para mil. Veja a moeda de 10 Shkalim Velhos: . . Esta, por sua vez, herdou seu desenho de uma moeda de Herodes Arquelau, governador da Judeia (4 AEC â€“ 6 EC), e sucessor de Herodes. Trata-se de uma galÃ©, ou navio impulsionado por remos. Certamente Ã© uma escolha interessante para o desenho da moeda moderna, dada a relaÃ§Ã£o conturbada entre Herodes, seus filhos, e o povo judeu. . . 5 agorot . A moeda de 5 centavos tambÃ©m jÃ¡ nÃ£o Ã© mais produzida, desde 2008. . . Esta tambÃ©m Ã© uma â€œherdeira naturalâ€ da moeda de 50 â€œShkalim Velhosâ€. . . O desenho Ã© de uma moeda do quarto ano da revolta judaica contra Roma (anos 69-70 EC), onde vemos as quatro espÃ©cies da festa de Sukot: no meio estÃ£o juntos o lulav (folha de palmeira), hadas (myrtus) e aravÃ¡ (salgueiro), e de ambos os lados dois etrogim (cidras amarelas = Citrus medica). . . Claramente pode-se ler â€œShnat Arbaâ€ (×©× ×ª ××¨×‘×¢) em hebraico, que significa â€œano quatroâ€. A inscriÃ§Ã£o Ã© feita com a antiga escrita hebraica, aquele que o Rei David conhecia. Sim, leitor, as familiares letras do hebraico que conhecemos hoje sÃ£o na verdade letras aramaicas, incorporadas ao hebraico durante o ExÃ­lio da BabilÃ´nia, apÃ³s a destruiÃ§Ã£o do Primeiro Templo Sagrado em 586 AEC. As duas escritas, hebraica e aramaica conviveram atÃ© o sÃ©culo 2 AEC, e a partir de entÃ£o apenas as letras aramaicas foram usadas. Deu-se uma adaptada pequena a estas letras, para que ganhassem a familiar forma quadrada que conhecemos hoje. . De toda forma, o uso deste alfabeto antigo nas moedas da Grande Revolta provavelmente significa uma â€œvolta Ã s raizesâ€, e uma nostalgia aos bons e velhos tempos. Vejam abaixo uma tabela comparativa entre o alfabeto hebraico antigo (linha de cima) e o alfabeto hebraico moderno (embaixo). Ambos tem 22 letras, porÃ©m 4 letras do alfabeto moderno sÃ£o diferentes quando usadas na Ãºltima letra da palavra, portanto a aparente disparidade. . . 10 agorot . EstÃ¡ Ã© a moeda de menor valor ainda em circulaÃ§Ã£o em Israel. . . Ela Ã© a Ãºltima das moedas que desta lista que herdou seu desenho de uma moeda do Shekel Velho, neste caso, a de 100 Shkalim Velhos, vejam. . . Em 1990, Yasser Arafat, lÃ­der da OrganizaÃ§Ã£o para LibertaÃ§Ã£o da Palestina, disse em uma coletiva de imprensa que havia descoberto â€œa verdadeira cara de Israel e seus planos de expansÃ£oâ€. Segundo ele, a silueta que serve de fundo para a menorÃ¡ Ã© um mapa da regiÃ£o, incluindo Israel, JordÃ¢nia, SÃ­ria, Iraque e o norte da ArÃ¡bia Saudita. NinguÃ©m o levou a sÃ©rio. . Na verdade, o desenho Ã© de uma moeda antiga danificada, e seu contorno original foi mantido no desenho da nova moeda. Trata-se de uma moeda de prutÃ¡ do ano 37 AEC, emitida durante o reinado de Antigonus II Mattathias, Ãºltimo rei Hasmoneu de JudÃ¡, morto por ordem de Herodes. No fim de seu reinado, quando jÃ¡ entendia que nÃ£o resistiria Ã  invasÃ£o romana, foram emitidas vÃ¡rias moedas com motivos judaicos, para dizer que ele era o rei pertencente Ã  cultura judaica (diferentemente de Herodes, de cultura edomita-judaica-romana). A tentativa era de ganhar um pouco mais de apoio popular e atrair combatentes para ajudÃ¡-lo. NÃ£o deu certoâ€¦ . A dinastia dos Hasmoneus chegou ao poder com a revolta dos Macabeus, e reinou por cerca de 100 anos. Este foi o Ãºltimo perÃ­odo (atÃ© o sÃ©culo 20) em que os judeus tiveram soberania prolongada de parte da Terra de Israel. . A moeda era feita de bronze, e foram encontradas apenas cerca de 40 delas. Veja abaixo. . . Como resposta a Arafat, o entÃ£o embaixador de Israel na ONU, Benjamin Netanyahu, disse que o sÃ­mbolo da OLP tinha o mapa do Mandato BritÃ¢nico embaixo da bandeira palestina, o que significaria que a intenÃ§Ã£o deles seria criar o estado palestino no lugar de Israel. TouchÃ©. . . Com a troca de moeda em 1985, o Banco de Israel adotou para si o seguinte logo â€œexpansionistaâ€. . . 50 agorot . JÃ¡ comecei mentindo. NÃ£o existe moeda de 50 agorot, mas sim uma moeda de meio shekel. Qual a diferenÃ§a? Veja a imagem abaixo. . . Talvez na psique do israelense, a Ãºltima (e hoje, Ãºnica) moeda de â€œcentavosâ€ seja a de 10 agorot, jÃ¡ que â€œmeio shekelâ€ claramente pertence Ã  famÃ­lia do â€œshekelâ€. A loucura nÃ£o comeÃ§ou agora. Veja a moeda de meio shekel do ano 2 da grande revolta de JudÃ¡ contra os romanos (anos 67-68). . . Novamente as letras antigas. Do lado esquerdo o desenho de um cÃ¡lice, com as letras â€œ×©×‘â€, representando Shnat B, ou seja, o segundo ano da revolta. Ao redor lemos â€œ×—×¦×™ ×”×©×§×œâ€, ou seja, meio shekel. Do lado direito trÃªs romÃ£s (uma das sete espÃ©cies da Terra de Israel), e em volta se lÃª â€œ×™×¨×•×©×œ×™× ×”×§×“×•×©×”â€, JerusalÃ©m Sagrada. (Podem verificar usando a tabela comparativa entre as letras hebraicas antigas e novas acima, Ã© bom pra cabeÃ§a). . Meio shekel era o valor do imposto que todos os israelitas pagavam para a manutenÃ§Ã£o do antigo templo de JerusalÃ©m. â€œCada um daqueles que forem recenseados pagarÃ¡ a metade de um shekel como contribuiÃ§Ã£o devida ao Senhor. (ÃŠxodo 30:13)â€ Metade de um shekel (×Ö·×—Ö²×¦Ö´×™×ª ×”Ö·×©Ö¶Ö¼××§Ö¶×œ), e nÃ£o meio shekel (×—×¦×™ ×©×§×œ), pois naquela Ã©poca shekel era uma unidade de medida de prata, e nÃ£o uma moeda. . AtenÃ§Ã£o, mostrei a moeda acima, do ano 2 da revolta, para mostrar um outro exemplo de meio shekel, mas ainda nÃ£o sabemos de onde veio a harpa da moeda de hoje em dia. . A harpa Ã© uma clara referÃªncia ao rei David, que em 1 Samuel 16:23 conta-se que tocava o instrumento para espantar o espÃ­rito ruim que baixava em Saul. A palavra usada Ã© kinor (×›Ö´Ö¼× Ö¼×•Ö¹×¨), que no hebraico de hoje significa violino, e nÃ£o se sabe ao certo qual instrumento era na Ã©poca, e a interpretaÃ§Ã£o usual Ã© que era uma harpa mesmo. Outras moedas do passado jÃ¡ tiveram o desenho de uma harpa, notadamente a moeda do primeiro ano da revolta de Bar Kochva (132-133 EC). Do lado esquerdo vemos uma folha de palmeira (lulav), e do outro lado da moeda uma harpa de sete cordas, com a inscriÃ§Ã£o (novamente no script hebraico antigo) â€œAno um da redenÃ§Ã£o de Israelâ€. NÃ£o deu certoâ€¦ a redenÃ§Ã£o durou menos de 5 anos. . . Como pode-se notar, a harpa da moeda de meio shekel que se usa hoje Ã© assimÃ©trica, e nÃ£o se parece nem um pouco com a harpa de Bar Kochva. Enrolei muito para contar que o desenho de hoje nÃ£o veio de uma moeda antiga (pasmem!), mas sim de um â€œselo de impressÃ£oâ€. (Ganha um sorvete quem souber por que as imagens abaixo estÃ£o espelhadas.) . . Trata-se de um objeto usado para carimbar a cera ainda mole e assim lacrar uma carta ou documento antigo. O selo foi comprado em uma feira de antiguidades no ano 1978 por Reuben Hecht. A inscriÃ§Ã£o diz â€œPara Maadana, filha do reiâ€ (×œ××¢×“× ×” ×‘×ª ×”××œ×š). Agora sim o uso das letras do hebraico antigo Ã© justificado: estima-se que o selo seja do sÃ©culo 7 AEC do reino de JudÃ¡, quando ainda existia o primeiro templo de JerusalÃ©m. Tentou-se descobrir quem Ã© Maadana, e qual rei seria seu pai, mas infelizmente Ã© apenas isto que sabemosâ€¦ Segundo o arqueÃ³logo israelense Nahman Avigad, este desenho Ã© a melhor estimativa que temos de como se pareceria o â€œkinorâ€ bÃ­blico. A harpa de 12 cordas Ã© ornada por uma roseta e tem uma moldura de uma corrente de pÃ©rolas. . 1 shekel . Enquanto todas as moedas de agora (centavo!) sÃ£o amarelas (meio shekel inclusive), jÃ¡ as de shekel sÃ£o prateadas. . . O desenho Ã© de uma flor de lis, ou em hebraico Shoshan Tzachor (×©×•×©×Ÿ ×¦×—×•×¨, lÃ­rio branco). O lÃ­rio branco indica pureza e segundo o profeta OsÃ©ias (14:6) tornou-se a flor-sÃ­mbolo do povo de Israel (Eu serei para Israel como o orvalho. Ele florescerÃ¡ como o lÃ­rio e lanÃ§arÃ¡ as suas raÃ­zes como o LÃ­bano). A moeda antiga que serviu de inspiraÃ§Ã£o pode ser vista abaixo. . . A inscriÃ§Ã£o na moeda antiga (do lado do pÃ¡ssaro) foi copiada na moeda moderna do lado da flor de lis, e diz Yehud (×™×”×“), usando o antigo alfabeto hebraico. Yehud Medinta (×™Ö°×”Ö»×“ ×Ö°×“Ö´×™× Ö°×ªÖ¸Ö¼×) Ã© como se chamava em aramaico a provÃ­ncia autÃ´noma judaica (yehud) na Ã©poca do domÃ­nio persa. Medinta tem a mesma raiz da palavra hebraica MedinÃ¡ (hoje significa paÃ­s, originalmente uma entidade polÃ­tica de qualquer tamanho, regida por uma mesma lei, Din=×“×™×Ÿ em hebraico). Um outro nome para a provÃ­ncia autÃ´noma Ã© Pachavat Yehuda (×¤×—×•×•×ª ×™×”×•×“×”), onde Pachava significa provÃ­ncia em persa antigo, e Ã© controlada por um Pecha (daÃ­ vem a palavra PaxÃ¡ em portuguÃªs). Esdras e Neemias foram paxÃ¡s de Yehuda. . A provÃ­ncia judaica existiu de 538 AEC, com Ciro II permitindo aos judeus voltar da BabilÃ´nia para SiÃ£o, atÃ© o ano de 332 AEC, com a conquista de Alexandre, o Grande. Nesta Ã©poca existia plena liberdade religiosa e de culto (Dario I permitiu a reconstruÃ§Ã£o do Segundo Templo), mas sem autonomia polÃ­tica, embora eles pudessem emitir moedas de baixo valor, como esta da flor de lis, de 1 Ã³bolo de prata (1/20 do shekel, cerca de 0.6 gramas). A moeda foi emitida aproximadamente em 350 AEC em JerusalÃ©m, e foi encontrada milÃªnios depois em JericÃ³ (uma das 6 grandes cidades da provÃ­ncia). . 2 shkalim . A moeda de 2 shkalim Ã© apelidada de Shnekel, um portmanteau carinhoso, que junta as palavras â€œShnei Shekelâ€ (×©× ×™ ×©×§×œ), ou seja, dois shekel. Ela Ã© a mais nova de todas as moedas, tendo sido lanÃ§ada no final de 2007, e nÃ£o em 1985, como todas as outras. . . O desenho Ã© de dois chifres que transbordam de comida, ou cornos da abundÃ¢ncia (cornucÃ³pias). Vemos o trigo e uvas, e algo mais que nÃ£o reconheÃ§o. No meio hÃ¡ uma romÃ£, outro sÃ­mbolo de abundÃ¢ncia. O desenho Ã© inspirado em uma moeda de JoÃ£o Hircano I (Yohanan Hyrcanus), filho de SimÃ£o (Shimon) e neto de Matatias (Matitiahu), da dinastia dos Hasmoneus (Chashmonaim). Ele reinou entre 134 e 104 AEC. . . Os dois chifres juntados pela base sÃ£o a adaptaÃ§Ã£o judaica deste sÃ­mbolo helenista tÃ£o comum. Anos antes do reinado de JoÃ£o Hircano I, o rei selÃªucida Demetrius I emitiu uma moeda sua, nos anos 151-150 AEC, onde a deusa da fortuna chamada Tique segura uma cornucÃ³pia. . . Um pouco mais tarde, nos anos 126-125 AEC, a rainha selÃªucida CleÃ³patra TÃ©ia emitiu uma moeda com duas cornucÃ³pias lado a lado (e seu prÃ³prio rosto do outro lado). Certamente JoÃ£o Hircano I tomou para si um sÃ­mbolo amplamente conhecido pelas pessoas da regiÃ£o (o ImpÃ©rio SelÃªucida era vizinho do reino de JudÃ¡). . . A festa de Chanuka celebra a vitÃ³ria dos macabeus sobre os gregos (na verdade, selÃªucidas), e um de seus valores centrais Ã© a rejeiÃ§Ã£o da cultura helenÃ­stica pela lideranÃ§a judaica. A menorÃ¡ que vimos na moeda de 10 agorot foi emitida pelo Ãºltimo dos reis hasmoneus, para tentar retomar o pouco de cultura judaica que havia sobrado. Ao longo das geraÃ§Ãµes de hasmoneus, fica claro que a cultura helenÃ­stica lhes era irresistÃ­vel. A cornucÃ³pia Ã© apenas uma das evidÃªncias. Mais claro ainda Ã© perceber seus nomes, todos uma mistura de hebraico com grego: Yohanan (JoÃ£o) Hircano era pai de Yehuda Aristobulus, Matitiahu Antigonos e Alexander Yanai. . 5 shkalim . O desenho da moeda de 5 shkalim mostra um capitel de uma coluna proto-jÃ´nica, tÃ­pica da Ã©poca do primeiro templo. NÃ£o se trata de uma apropriaÃ§Ã£o da famosa coluna jÃ´nica da cultura grega, que sÃ³ viria a ser criada no sÃ©culo 6 AEC. Este estilo de colunas com volutas (espirais) era comum por todo o Levante, e podemos ter orgulho desta criaÃ§Ã£o local da Terra de Israel. . . As volutas representam a Ã¡rvore da vida. Vejamos como. Abaixo Ã  esquerda encontramos um desenho assÃ­rio em relevo de uma tamareira. Para os assÃ­rios, esta Ã¡rvore representava a abundÃ¢ncia agricultural. A tamareira Ã© capaz de se reproduzir assexuadamente, seus clones tambÃ©m sendo capazes de produzir frutos. Da base do tronco nascem ramificaÃ§Ãµes que se afastam da tamareira-mÃ£e, e separam-se uns dos outros por um broto triangular, que Ã© o resto do fronde (folha ramificada) da estaÃ§Ã£o anterior. A Ã¡rvore cresce um pouco a cada estaÃ§Ã£o, e com a quebra dos brotos triangulares, o tronco ganha a sua tÃ­pica textura romboidal (com forma de losangos). . . Quando a Ã¡rvore nÃ£o Ã© cultivada, nascem mais e mais troncos em volta do tronco inicial, todos geneticamente idÃªnticos. Como sempre vemos imagens de tamareiras sozinhas, isto significa que sÃ£o plantas cultivadas. A facilidade de se clonar uma Ã¡rvore que tem propriedades desejÃ¡veis (muitos frutos de sabor doce, por exemplo) fez com que a tamareira ganhasse a simbologia de Ã¡rvore da vida. Portanto, a voluta no topo das colunas representa o ciclo de nascimento e renascimento que ocorre na base do tronco, e nÃ£o a copa, como seria natural de se supor. . Colunas proto-jÃ´nicas foram encontradas em escavaÃ§Ãµes em Megido, Hatzor, Ramat Rachel e JerusalÃ©m, entre outras. Nos anos 1960, a arqueÃ³loga britÃ¢nica Kathleen Kenyon encontrou o capitel abaixo em uma escavaÃ§Ã£o na Cidade de David. O capitel estava quebrado em dois, no meio de outras ruÃ­nas que haviam rolado a encosta leste da Cidade de David. Ele pode ser visto no Museu de Israel, em JerusalÃ©m. . . 10 shkalim . Esta Ã© a moeda de Shekel de mais alto valor, e a que tem o significado mais interessante. . . Acabamos de discutir o significado de â€œÃ¡rvore da vidaâ€ da tamareira, e ela naturalmente serviu como sÃ­mbolo em vÃ¡rias moedas ao longo da histÃ³ria. No quarto ano da revolta judaica contra Roma (anos 69-70 EC), foi emitida em JerusalÃ©m esta moeda de meio shekel. De um lado vemos a tamareira com duas cestas de tÃ¢maras aos seus pÃ©s, e do outro lado uma variaÃ§Ã£o do desenho das quatro espÃ©cies de Sukot que jÃ¡ vimos na moeda de 5 agorot. . . A inscriÃ§Ã£o do lado da tamareira estÃ¡ um pouco apagada, mas pode ser melhor identificada nesta moeda abaixo. â€œPela redenÃ§Ã£o de SiÃ£oâ€ (legeulat tzion = ×œ×’××•×œ×ª ×¦×™×•×Ÿ) Ã© o que a lideranÃ§a judaica desejava, jÃ¡ sabendo que nÃ£o poderiam resistir Ã  conquista romana. . . O general romano Vespasiano tornou-se imperador de Roma em 69 EC, partindo da JudÃ©ia para o Egito, e deixando o cerco de JerusalÃ©m a cargo de seu filho Tito. JerusalÃ©m finalmente caiu no verÃ£o de 70 EC, e o segundo templo foi destruÃ­do. Em comemoraÃ§Ã£o Ã  conquista da JudÃ©ia, a seguinte moeda romana foi emitida no ano 71. . . De um lado temos o imperador Vespasiano, e do outro lado trÃªs figuras: Ã  esquerda vemos o imperador com armadura, Ã  direita a JudÃ©ia Ã© representada por uma mulher chorando, e no meio a tamareira. A incriÃ§Ã£o diz â€œJudÃ©ia Capturadaâ€ (ivdaea capta). NÃ£o bastava conquistar JerusalÃ©m e destruir o templo, era tambÃ©m preciso jogar sal na ferida! . Eis que, 1915 anos depois, o Estado de Israel decide â€œdevolver na mesma moedaâ€, literalmente. A tamareira volta em toda a sua glÃ³ria, e a inscriÃ§Ã£o diz em hebraico moderno e antigo (para todos entenderem), Pela RedenÃ§Ã£o de SiÃ£o! . Chupa essa tÃ¢mara, Titus Flavius Vespasianus! . . . . Fontes: Imagens de todas as moedas de hoje no site do Banco de Israel, clique em â€œ××¢×•×ªâ€ (moedas). 1 agorÃ¡ Imagem da moeda de Herodes Arquelau: Wikipedia. 5 agorot Imagens das moedas do quarto ano da revolta: Otzar.org, antiquities.org.il, coinsmendy.com. Tabela comparativa das escritas judaicas antiga e moderna: safa-ivrit.org. 10 agorot HistÃ³ria sobre Arafat: Ynet. Imagem da moeda antiga: danielventura.wikia.com 50 agorot Imagem da moeda do ano 2 da revolta: Wikipedia. Moeda da harpa: winners-auctions.com. Imagem do selo de Maadana: sheqel.info, echad.info (pÃ¡gina 15). InformaÃ§Ãµes sobre o selo de Maadana: books.google.com, ancientlyre.com, Wikipedia. 1 shekel InformaÃ§Ãµes sobre a flor de lis: books.google.com. Imagem da moeda antiga: tsel.org. InformaÃ§Ãµes sobre Yehud Medinta: Wikipedia. 2 shkalim Imagem da moeda de JoÃ£o Hircano I: Wikipedia. Imagem da moeda da deusa Tique: coinworld.com. Imagem da moeda de CleÃ³patra TÃ©ia: Wikipedia. 5 shkalim InformaÃ§Ãµes e imagens sobre a tamareira: academia.edu (artigo interessante!). Imagem do capitel de pedra: cityofdavid.org.il. 10 shkalim Imagens da moeda antiga: press.khm.at, artportal.co.il. Imagens da moeda da JudÃ©ia Capturada: press.khm.at. .",
            "url": "https://yairmau.github.io/website/markdown/2022/02/01/as-moedas-do-estado-judeu.html",
            "relUrl": "/markdown/2022/02/01/as-moedas-do-estado-judeu.html",
            "date": " â€¢ Feb 1, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "Doubling Time",
            "content": "Suppose you have a process that can be described by exponential growth. It could be anything: interests on an investment, the early phases of infection in a pandemic, whatever. . It is often convenient to have an idea how fast is the growth by answering the question: . How long will it take for $x$ to double in size, given a growth of $n$% per year? . The rule of thumb I learned a while back is the following: . Doubling time = $ displaystyle frac{70}{n}$ (in years) . Of course, the time unit could be anything you like, Iâ€™ll deal here with years for simplicityâ€™s sake. Specifically, letâ€™s answer the question: . Israel has currently (2021) a population of 9.2 million, and a growth rate of 1.8% per year. How long will it take for the population to double, assuming a fixed growth rate? . The answer is about 39 years (70 divided by 1.8), but why?! . Letâ€™s call $x_0$ the population size now, and the growth rate $n$%. After one year, the population will be . x1=x0âˆ—(1+n100)(1)x_1 = x_0 * left( 1 + frac{n}{100} right) tag{1}x1â€‹=x0â€‹âˆ—(1+100nâ€‹)(1) . Assume that after $k$ years the population will be double, i.e.: . xk=x0âˆ—(1+n100)k=2x0.(2)x_k = x_0 * left( 1 + frac{n}{100} right)^k = 2x_0. tag{2}xkâ€‹=x0â€‹âˆ—(1+100nâ€‹)k=2x0â€‹.(2) . Cancelling $x_0$ we get . (1+n100)k=2.(3) left( 1 + frac{n}{100} right)^k = 2. tag{3}(1+100nâ€‹)k=2.(3) . We now take the natural logarith of both sides: . klnâ¡(1+n100)=lnâ¡(2).(4)k ln left( 1 + frac{n}{100} right) = ln(2). tag{4}kln(1+100nâ€‹)=ln(2).(4) . Note that we took $k$ out of the exponent and it now multiplies the logarithm on the left-hand side. Multiplying both sides by 100 yields . 100klnâ¡(1+n100)=100lnâ¡(2)â‰ƒ69.3.(5)100k ln left( 1 + frac{n}{100} right) = 100 ln(2) simeq 69.3. tag{5}100kln(1+100nâ€‹)=100ln(2)â‰ƒ69.3.(5) . That surely explains the number 70 in the rule of thumb! Because of the properties of logarithms, we put the number 100 as the exponent of the parenthesis: . klnâ¡(1+n100)100=100lnâ¡(2).(6)k ln left( 1 + frac{n}{100} right)^{100} = 100 ln(2). tag{6}kln(1+100nâ€‹)100=100ln(2).(6) . We are very close to the end! We now remind ourselves that we learned in Calculus the definition of the exponential function: . expâ¡(x)=limâ¡mâ†’âˆ(1+xm)m.(7) exp(x) = lim_{m rightarrow infty} left( 1 + frac{x}{m} right)^{m}. tag{7}exp(x)=mâ†’âˆlimâ€‹(1+mxâ€‹)m.(7) . Because the number 100 is â€œquite bigâ€, we will approximate the parenthesis inside the logarithm with the exponential function, thus . klnâ¡expâ¡(n)=100lnâ¡(2).(8)k ln exp(n) = 100 ln(2). tag{8}klnexp(n)=100ln(2).(8) . The logarithm is the inverse function of the exponential, therefore . kn=100lnâ¡(2).(9)kn = 100 ln(2). tag{9}kn=100ln(2).(9) . Finally, solving for $k$, we have . k=100lnâ¡(2)nâ‰ƒ70n.(10)k = frac{100 ln(2)}{n} simeq frac{70}{n}. tag{10}k=n100ln(2)â€‹â‰ƒn70â€‹.(10) . We have thus shown that the number of years $k$ it will take for Israel to double itâ€™s population is about $70/n = 70/1.8 = 38.88$ years!! . The exact number, without any approximations, would be . k=lnâ¡(2)lnâ¡(1+n/100)â‰ƒ38.85.(11)k = frac{ ln(2)}{ ln(1+n/100)} simeq 38.85. tag{11}k=ln(1+n/100)ln(2)â€‹â‰ƒ38.85.(11) . Conclusion: ğŸ‘ Very impressive rule of thumb ğŸ‘ .",
            "url": "https://yairmau.github.io/website/markdown/2021/11/16/doubling-time.html",
            "relUrl": "/markdown/2021/11/16/doubling-time.html",
            "date": " â€¢ Nov 16, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Parabolic Hyperboloid",
            "content": ". Just like the hyperboloid of one sheet, the Parabolic Hyperboloid is a ruled surface. This means that it can be imagined as the surface one gets by swiping a straight line through space. In the image below, we see that each of the skewers is exactly straight, but together thay make this beautiful curved shape. . . This project is quite easy to make with skewers, see its instructions here. . The equation that defines the surface of the parabolic hyperboloid is . z=Ax2âˆ’By2,z = Ax^2 - By^2,z=Ax2âˆ’By2, . where both $A$ and $B$ are positive numbers. This website allows us to play with the parameters and see how the surface responds. . In my opinion, a static image canâ€™t convey the beauty of this shape, so I made this gif: . In 2020, Dillon Berger noted on Twitter that the shape of Pringles is a parabolic hyperboloid: . The reason Pringles fit so nicely in a cylindrical tube is because they&#39;re hyperbolic paraboloids plotted over a circular domain pic.twitter.com/BUzjPw7e17 . &mdash; ã€ˆ Berger | Dillon ã€‰ (@InertialObservr) February 18, 2020 The company took notice of this (mildly) viral tweet, and sent Dillon a box full of parabolic hyperboloids ğŸ˜. . Thanks @Pringles, for sending me some of these delicious Hyperbolic Paraboloids! pic.twitter.com/L3WMgqObPM . &mdash; ã€ˆ Berger | Dillon ã€‰ (@InertialObservr) March 2, 2020 The lesson here is that itâ€™s good to know your math ğŸ˜œ. . .",
            "url": "https://yairmau.github.io/website/markdown/2021/11/13/parabolic-hyperboloid.html",
            "relUrl": "/markdown/2021/11/13/parabolic-hyperboloid.html",
            "date": " â€¢ Nov 13, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Skewer Hyperboloid",
            "content": ". George Hart is again the hero of this project. Go check out his webpage to see how to build this beautiful shape. All you need to make the skewer hyperboloid is a bunch of skewers and thin elastic bands. . This is an easy project, and the end result is so fun to play with! It is surprising how smoothly this shape morphes when you pull and push the skewers to change the angle, itâ€™s hard to explain, so you should definitely try it yourself ğŸ˜. See Annie Perkinsâ€™s video to get a better idea: . The one thing I can add to George Hartâ€™s page is that after many weeks and months the elastic bands will dry out and break! I gave a skewer hyperboloid to a friend, and one day he arrived at his office and found out that the whole thing had disintegrated overnight, and the skewers were all over the place. Iâ€™m still looking for a suitable substitute to the elastic bands. It must be something flexible that enables us to play with the construction, and at the same time something durableâ€¦ I have not found yet the perfect material, if you have a suggestion please write to me. . I wanted my hyperboloid to last for a long time, so after the construction was done, I used super glue to fix the skewers in a particular angle. Now I canâ€™t play with it, but at least it will not disintegrate overnight in a few months time (hopefully!). . . The mathematical name of this shape is â€œhyperboloid of one sheetâ€, and its equation is . x2A2+y2B2âˆ’z2C2=0. frac{x^2}{A^2} + frac{y^2}{B^2} - frac{z^2}{C^2} = 0.A2x2â€‹+B2y2â€‹âˆ’C2z2â€‹=0. . When $A=B$ the horizontal cross-sections are circles, just like in our construction. The elastic bands effectively allow us to play with the parameter $C$, which controls how fast the hyperboloid grows sideways. There is a wonderful widget in the Interactive Gallery of Quadric Surfaces, its quite fun to play with. . To see how this curved surface can be made entirely out of straight lines, one can define the surface as the set of all parametric curves of the kind . x=A(cosâ¡Î¸âˆ’vsinâ¡Î¸)y=B(sinâ¡Î¸+vcosâ¡Î¸)z=CÎµv. begin{align} x &amp;= A ( cos theta - v sin theta) y &amp;= B ( sin theta + v cos theta) z &amp;= C varepsilon v. end{align}xyzâ€‹=A(cosÎ¸âˆ’vsinÎ¸)=B(sinÎ¸+vcosÎ¸)=CÎµv.â€‹â€‹ . Substitute the parametric equations above into the equation of the paraboloid and see that they satisfy it. . There are actually two families of curves, for $ epsilon= pm 1$. I made a Mathematica plot of the two families of lines, in red ($ varepsilon=1$) and in blue ($ varepsilon=-1$), for ten $ theta$ values between 0 and $2 pi$. Itâ€™s easy to see that each family of lines swirls in a different direction. . . The Mathematica code I wrote is . a = 1; b = 1; c = 1; h = 4; n = 10; p1 = ContourPlot3D[ (*hyperboloid*) x^2/a^2 + y^2/b^2 - z^2/c^2 == 1, {x, -h, h}, {y, -h, h}, {z, -h, h}, Mesh -&gt; None, ContourStyle -&gt; Opacity[0.4], AxesLabel -&gt; {x, y, z}, LabelStyle -&gt; Large ]; p2 = ParametricPlot3D[ (*red lines*) Table[{ a (Cos[theta] - v Sin[theta]), b (Sin[theta] + v Cos[theta]), c v }, {theta, 0, 2 Pi, 2 Pi/n}], {v, -20, 20}, PlotStyle -&gt; {Red} ]; p3 = ParametricPlot3D[ (*blue lines*) Table[{ a (Cos[theta] - v Sin[theta]), b (Sin[theta] + v Cos[theta]), - c v }, {theta, 0, 2 Pi, 2 Pi/n}], {v, -20, 20}, PlotStyle -&gt; {Blue} ]; Show[{p1, p2, p3}] . .",
            "url": "https://yairmau.github.io/website/markdown/2021/11/12/skewer-hyperboloid.html",
            "relUrl": "/markdown/2021/11/12/skewer-hyperboloid.html",
            "date": " â€¢ Nov 12, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "Tomoko Fuse Unit Origami",
            "content": ". This structure has the general shape of a dodecahedron, with 12 beautiful pentagonal stars embeded in it. I found a while back a scan of the book Tomoko Fuse Unit Origami Fantasy. The book is all written in japanese, but I could make sense of the diagrams. . Tomoko Fuse is a master of modular origami, just google her name and youâ€™ll find tons of fun projects. . I used craft paper to give the structure a muted tones and a rustic feel. I think it turned out pretty good ğŸ˜Š. . .",
            "url": "https://yairmau.github.io/website/markdown/2021/11/08/tomoko-fuse.html",
            "relUrl": "/markdown/2021/11/08/tomoko-fuse.html",
            "date": " â€¢ Nov 8, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "George Hart's Card Constructions",
            "content": ". George Hart is a sculptor and geometer, and his webpage has the best projects for the artistically-minded nerd! . The image above shows the 30-cards construction, but it is not identical to what George Hart shows on his card constructions page. I actually used Francesco De ComitÃ©â€™s version, also credited by Hart. The idea is that you strategically cut the cards in very specific places, so that they can be slided together. . Figuring out how to cut the cards was quite hard to me, I had to experiment a lot! George Hart provides printable templates, but their dimensions are different from the cards I used. Finally besides cutting the cards, the task of putting everything together was a nightmare. Itâ€™s been a while since I built this, but I think that I used clothes pegs to hold the cards together before the whole thing is ready. Once you finish building the structure, it should be much more stable (but not enough to throw it in the air!!). . Because there is a 5-fold symmetry, naturally the golden ratio . Ï†=1+52 varphi = frac{1+ sqrt{5}}{2}Ï†=21+5 . â€‹â€‹ . figures quite prominently in there. Francesco De ComitÃ© provides in his Flickr page a plan for how to cut the cards: . . I put the plan above for the future me to understand what it means, because I myself didnâ€™t use thisâ€¦ I found in one of my â€œprojectsâ€ folders my own calculations of how to cut the cards. It shows only the cuts on the top side. One should rotate the card 180 degrees and do the same cuts on the other side. For a card of height H and length L, my cutting plan is the following. . . The cuts are the thick blue lines, and the red arrows show the length $L/ varphi$ of one of the cuts and of the uncut (dashed) part. From this I hope you can imagine that when the cards are slided together, the two cuts amount to the length of the whole â€œdiagonalâ€, such that the two card ends are flush. I wish I could explain why the plan is what it is, but some time has passed and I have no idea any more ğŸ¤·â€â™‚ï¸â€¦ . .",
            "url": "https://yairmau.github.io/website/markdown/2021/11/08/george-hart-card-constructions.html",
            "relUrl": "/markdown/2021/11/08/george-hart-card-constructions.html",
            "date": " â€¢ Nov 8, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "The Sonobe Module",
            "content": "All these modular origami constructions are made of a single basic module, the Sonobe unit. . The basic unit has flaps and pockets. . The actual folding of the Sonobe module is quite easy, see the instructions below, taken from Michael Naughtonâ€™s excellent diagram. . . This is an incredibly flexible unit, allowing us to combine it in many ways. This pdf shows how to combine different numbers of Sonobe modules to produce the shapes above, and many others. If all meeting points of the basic unit contain 3 modules, one gets a cube. If all meeting points contain 4 modules, then we produce a octahedron (with pyramids on each of its faces). . . The results can be very impressive! Above is a Truncated Icosahedron, made of pentagons surrounded by hexagons (like a soccer ball). This one is made of 90 Sonobe units, see detailed instructions. .",
            "url": "https://yairmau.github.io/website/markdown/2021/11/03/sonobe.html",
            "relUrl": "/markdown/2021/11/03/sonobe.html",
            "date": " â€¢ Nov 3, 2021"
        }
        
    
  
    
        ,"post18": {
            "title": "Pythagorean theorem",
            "content": "Iâ€™ve been interested lately in the Pythagorean theorem, and the myriad of ways one can prove it. . . An excellent resource is the website Cut The Knot. . Another great source is John C. Sparksâ€™ The Pythagorean Theorem: Crown Jewel of Mathematics. . I used my tablet to write down my own version of some of the nicest proofs, see below. .",
            "url": "https://yairmau.github.io/website/markdown/2021/10/28/pythagoras.html",
            "relUrl": "/markdown/2021/10/28/pythagoras.html",
            "date": " â€¢ Oct 28, 2021"
        }
        
    
  
    
        ,"post19": {
            "title": "Hexastix",
            "content": "Iâ€™m very proud of this construction I made about 4 years ago. . . It sits prominently on my office shelf, it has never failed to impress the occasional visitor :) Although it looks complicated, this 72-pencil construction is not very hard to build! I followed Matt Parkerâ€™s youtube tutorial, it took me about 1.5 hours to make it, and another hour or so to glue it. . Materials: . 72 pencils | a few elastic bands | super glue | . Here in Israel, I couldnâ€™t find non-sharpened pencils. There is some risk of (small) injury if you donâ€™t take care ğŸ˜¬. . Without the elastic bands, this construction would not be stable. The problem is that with time the elastic bands get dry and disintegrate, so if you want to have this construction standing for a long time, you have to glue it. I put tiny drops of super glue in most of the touching points between the pencils, and it worked great! . Enjoy making your own! . . PS: Iâ€™m in my office at the university writing these words, and exactly now a student passed in the corridor and asked me about the Hexastix ğŸ˜„ .",
            "url": "https://yairmau.github.io/website/markdown/2021/10/28/hexastix.html",
            "relUrl": "/markdown/2021/10/28/hexastix.html",
            "date": " â€¢ Oct 28, 2021"
        }
        
    
  
    
        ,"post20": {
            "title": "The can problem",
            "content": "watch this space .",
            "url": "https://yairmau.github.io/website/markdown/2021/10/27/can-problem.html",
            "relUrl": "/markdown/2021/10/27/can-problem.html",
            "date": " â€¢ Oct 27, 2021"
        }
        
    
  
    
        ,"post21": {
            "title": "Seasonal Decomposition",
            "content": "import numpy as np import matplotlib.pyplot as plt import pandas as pd from pandas.plotting import register_matplotlib_converters register_matplotlib_converters() # datetime converter for a matplotlib import seaborn as sns sns.set(style=&quot;ticks&quot;, font_scale=1.5) from statsmodels.tsa.seasonal import seasonal_decompose import matplotlib.dates as mdates from matplotlib.dates import DateFormatter . Trends in Atmospheric Carbon Dioxide . Mauna Loa CO2 concentration. data from NOAA . url = &quot;https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_weekly_mlo.csv&quot; df = pd.read_csv(url, header=47, na_values=[-999.99]) # you can first download, and then read the csv # filename = &quot;co2_weekly_mlo.csv&quot; # df = pd.read_csv(filename, header=47, na_values=[-999.99]) df . year month day decimal average ndays 1 year ago 10 years ago increase since 1800 . 0 1974 | 5 | 19 | 1974.3795 | 333.37 | 5 | NaN | NaN | 50.40 | . 1 1974 | 5 | 26 | 1974.3986 | 332.95 | 6 | NaN | NaN | 50.06 | . 2 1974 | 6 | 2 | 1974.4178 | 332.35 | 5 | NaN | NaN | 49.60 | . 3 1974 | 6 | 9 | 1974.4370 | 332.20 | 7 | NaN | NaN | 49.65 | . 4 1974 | 6 | 16 | 1974.4562 | 332.37 | 7 | NaN | NaN | 50.06 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2510 2022 | 6 | 26 | 2022.4836 | 420.31 | 7 | 418.14 | 395.36 | 138.71 | . 2511 2022 | 7 | 3 | 2022.5027 | 419.73 | 6 | 417.49 | 395.15 | 138.64 | . 2512 2022 | 7 | 10 | 2022.5219 | 419.08 | 6 | 417.25 | 394.59 | 138.52 | . 2513 2022 | 7 | 17 | 2022.5411 | 418.43 | 6 | 417.14 | 394.64 | 138.41 | . 2514 2022 | 7 | 24 | 2022.5603 | 417.84 | 6 | 415.68 | 394.11 | 138.36 | . 2515 rows Ã— 9 columns . df[&#39;date&#39;] = pd.to_datetime(df[[&#39;year&#39;, &#39;month&#39;, &#39;day&#39;]]) df = df.set_index(&#39;date&#39;) df . year month day decimal average ndays 1 year ago 10 years ago increase since 1800 . date . 1974-05-19 1974 | 5 | 19 | 1974.3795 | 333.37 | 5 | NaN | NaN | 50.40 | . 1974-05-26 1974 | 5 | 26 | 1974.3986 | 332.95 | 6 | NaN | NaN | 50.06 | . 1974-06-02 1974 | 6 | 2 | 1974.4178 | 332.35 | 5 | NaN | NaN | 49.60 | . 1974-06-09 1974 | 6 | 9 | 1974.4370 | 332.20 | 7 | NaN | NaN | 49.65 | . 1974-06-16 1974 | 6 | 16 | 1974.4562 | 332.37 | 7 | NaN | NaN | 50.06 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2022-06-26 2022 | 6 | 26 | 2022.4836 | 420.31 | 7 | 418.14 | 395.36 | 138.71 | . 2022-07-03 2022 | 7 | 3 | 2022.5027 | 419.73 | 6 | 417.49 | 395.15 | 138.64 | . 2022-07-10 2022 | 7 | 10 | 2022.5219 | 419.08 | 6 | 417.25 | 394.59 | 138.52 | . 2022-07-17 2022 | 7 | 17 | 2022.5411 | 418.43 | 6 | 417.14 | 394.64 | 138.41 | . 2022-07-24 2022 | 7 | 24 | 2022.5603 | 417.84 | 6 | 415.68 | 394.11 | 138.36 | . 2515 rows Ã— 9 columns . fig, ax = plt.subplots(1, figsize=(8,6)) ax.plot(df[&#39;average&#39;]) ax.set(xlabel=&quot;date&quot;, ylabel=&quot;CO2 concentration (ppm)&quot;, # ylim=[0, 430], title=&quot;Mauna Loa CO2 concentration&quot;); . fill missing data. interpolate method: &#39;time&#39; interpolation methods visualized . df[&#39;co2&#39;] = (df[&#39;average&#39;].resample(&quot;D&quot;) #resample daily .interpolate(method=&#39;time&#39;) #interpolate by time ) df . year month day decimal average ndays 1 year ago 10 years ago increase since 1800 co2 . date . 1974-05-19 1974 | 5 | 19 | 1974.3795 | 333.37 | 5 | NaN | NaN | 50.40 | 333.37 | . 1974-05-26 1974 | 5 | 26 | 1974.3986 | 332.95 | 6 | NaN | NaN | 50.06 | 332.95 | . 1974-06-02 1974 | 6 | 2 | 1974.4178 | 332.35 | 5 | NaN | NaN | 49.60 | 332.35 | . 1974-06-09 1974 | 6 | 9 | 1974.4370 | 332.20 | 7 | NaN | NaN | 49.65 | 332.20 | . 1974-06-16 1974 | 6 | 16 | 1974.4562 | 332.37 | 7 | NaN | NaN | 50.06 | 332.37 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2022-06-26 2022 | 6 | 26 | 2022.4836 | 420.31 | 7 | 418.14 | 395.36 | 138.71 | 420.31 | . 2022-07-03 2022 | 7 | 3 | 2022.5027 | 419.73 | 6 | 417.49 | 395.15 | 138.64 | 419.73 | . 2022-07-10 2022 | 7 | 10 | 2022.5219 | 419.08 | 6 | 417.25 | 394.59 | 138.52 | 419.08 | . 2022-07-17 2022 | 7 | 17 | 2022.5411 | 418.43 | 6 | 417.14 | 394.64 | 138.41 | 418.43 | . 2022-07-24 2022 | 7 | 24 | 2022.5603 | 417.84 | 6 | 415.68 | 394.11 | 138.36 | 417.84 | . 2515 rows Ã— 10 columns . decompose data . seasonal_decompose returns an object with four components: . observed: $Y(t)$ | trend: $T(t)$ | seasonal: $S(t)$ | resid: $e(t)$ | . Additive model: $$ Y(t) = T(t) + S(t) + e(t) $$ . Multiplicative model: $$ Y(t) = T(t) times S(t) times e(t) $$ . Interlude . learn how to use zip in a loop . letters = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;] numbers = [1, 2, 3, 4, 5] # zip let&#39;s us iterate over to lists at the same time for l, n in zip(letters, numbers): print(f&quot;{l} = {n}&quot;) . a = 1 b = 2 c = 3 d = 4 e = 5 . Plot each component separately. . fig, ax = plt.subplots(4, 1, figsize=(8,6), sharex=True) decomposed_m = seasonal_decompose(df[&#39;co2&#39;], model=&#39;multiplicative&#39;) decomposed_a = seasonal_decompose(df[&#39;co2&#39;], model=&#39;additive&#39;) decomposed = decomposed_m pos = (0.5, 0.9) components =[&quot;observed&quot;, &quot;trend&quot;, &quot;seasonal&quot;, &quot;resid&quot;] colors = [&quot;tab:blue&quot;, &quot;tab:orange&quot;, &quot;tab:green&quot;, &quot;tab:red&quot;] for axx, component, color in zip(ax, components, colors): data = getattr(decomposed, component) axx.plot(data, color=color) axx.text(*pos, component, bbox=dict(facecolor=&#39;white&#39;, alpha=0.8), transform=axx.transAxes, ha=&#39;center&#39;, va=&#39;top&#39;) . decomposed = decomposed_m fig, ax = plt.subplots(1, 2, figsize=(10,6)) ax[0].plot(df[&#39;co2&#39;], color=&quot;tab:blue&quot;, label=&quot;observed&quot;) ax[0].plot(decomposed.trend * decomposed.resid, color=&quot;tab:orange&quot;, label=&quot;trend*resid&quot;) ax[0].plot(decomposed.trend * decomposed.seasonal, color=&quot;tab:red&quot;, label=&quot;trend*seasonal&quot;) ax[0].plot(decomposed.trend, color=&quot;black&quot;, label=&quot;trend&quot;) ax[0].set(ylabel=&quot;CO$_2$ concentration (ppm)&quot;, title=&quot;Mauna Loa CO$_2$ concentration&quot;) ax[0].legend(frameon=False) start = &quot;2000-01-01&quot; end = &quot;2003-01-01&quot; zoom = slice(start, end) ax[1].plot(df.loc[zoom, &#39;co2&#39;], color=&quot;tab:blue&quot;, label=&quot;observed&quot;) ax[1].plot((decomposed.trend * decomposed.resid)[zoom], color=&quot;tab:orange&quot;, label=&quot;trend*resid&quot;) ax[1].plot((decomposed.trend * decomposed.seasonal)[zoom], color=&quot;tab:red&quot;, label=&quot;trend*seasonal&quot;) ax[1].plot(decomposed.trend[zoom], color=&quot;black&quot;, label=&quot;trend&quot;) date_form = DateFormatter(&quot;%Y&quot;) ax[1].xaxis.set_major_formatter(date_form) ax[1].xaxis.set_major_locator(mdates.YearLocator(1)) ax[1].set_title(&quot;Components, 2000--2003&quot;); .",
            "url": "https://yairmau.github.io/website/jupyter/2020/03/01/seasonal-decomposition.html",
            "relUrl": "/jupyter/2020/03/01/seasonal-decomposition.html",
            "date": " â€¢ Mar 1, 2020"
        }
        
    
  
    
        ,"post22": {
            "title": "Python Tutorial --- basic time series analysis",
            "content": "Import packages. If you don&#39;t have a certain package, e.g. &#39;newpackage&#39;, just type pip install newpackage . import urllib import matplotlib.pyplot as plt import numpy as np import pandas as pd import os.path import matplotlib.dates as mdates import datetime as dt import matplotlib as mpl from pandas.tseries.frequencies import to_offset from scipy.signal import savgol_filter . This is how you download data from Thingspeak . filename1 = &quot;test_elad.csv&quot; # if file is not there, go fetch it from thingspeak if not os.path.isfile(filename1): # define what to download channels = &quot;1690490&quot; fields = &quot;1,2,3,4,6,7&quot; minutes = &quot;30&quot; # https://www.mathworks.com/help/thingspeak/readdata.html # format YYYY-MM-DD%20HH:NN:SS start = &quot;2022-05-01%2000:00:00&quot; end = &quot;2022-05-08%2000:00:00&quot; # download using Thingspeak&#39;s API # url = f&quot;https://api.thingspeak.com/channels/{channels}/fields/{fields}.csv?minutes={minutes}&quot; url = f&quot;https://api.thingspeak.com/channels/{channels}/fields/{fields}.csv?start={start}&amp;end={end}&quot; data = urllib.request.urlopen(url) d = data.read() # save data to csv file = open(filename1, &quot;w&quot;) file.write(d.decode(&#39;UTF-8&#39;)) file.close() . You can load the data using Pandas. Here we create a &quot;dataframe&quot;, which is a fancy name for a table. . df = pd.read_csv(filename1) # rename columns df = df.rename(columns={&quot;created_at&quot;: &quot;timestamp&quot;, &quot;field1&quot;: &quot;T1&quot;, &quot;field2&quot;: &quot;RH&quot;, &quot;field3&quot;: &quot;T2&quot;, &quot;field4&quot;: &quot;motion_sensor&quot;, &quot;field6&quot;: &quot;VWC&quot;, &quot;field7&quot;: &quot;VPD&quot;,}) # set timestamp as index df[&#39;timestamp&#39;] = pd.to_datetime(df[&#39;timestamp&#39;]) df = df.set_index(&#39;timestamp&#39;) . First graph . fig, ax = plt.subplots(1, figsize=(8,6)) ax.plot(df[&#39;VPD&#39;]) # add labels and title ax.set(xlabel = &quot;time&quot;, ylabel = &quot;VPD (kPa)&quot;, title = &quot;my first graph&quot;) # makes slanted dates plt.gcf().autofmt_xdate() . Two columns in the same graph . fig, ax = plt.subplots(1, figsize=(8,6)) ax.plot(df[&#39;T1&#39;], color=&quot;tab:blue&quot;, label=&quot;SHT Temperature&quot;) ax.plot(df[&#39;T2&#39;], color=&quot;tab:orange&quot;, label=&quot;DS18B20 Temperature&quot;) # add labels and title ax.set(xlabel = &quot;Time&quot;, ylabel = &quot;Temperature (deg C)&quot;, title = &quot;two sensors&quot;, ylim=[20,35], ) # makes slanted dates plt.gcf().autofmt_xdate() ax.legend(loc=&quot;upper right&quot;) . &lt;matplotlib.legend.Legend at 0x7fe6c9730610&gt; . Calculate stuff . You can calculate new things and save them as new columns of your dataframe. . def calculate_es(T): es = np.exp((16.78 * T - 116.9) / (T + 237.3)) return es def calculate_ed(es, rh): return es * rh / 100.0 es = calculate_es(df[&#39;T1&#39;]) ed = calculate_ed(es, df[&#39;RH&#39;]) df[&#39;VPD2&#39;] = es - ed . See if what you calculated makes sense. . fig, ax = plt.subplots(1, figsize=(8,6)) ax.plot(df[&#39;VPD&#39;], color=&quot;tab:red&quot;, label=&quot;VPD from ESP32&quot;) ax.plot(df[&#39;VPD2&#39;][::100], &quot;o&quot;, color=&quot;black&quot;, label=&quot;VPD from python&quot;) # add labels and title ax.set(xlabel = &quot;Time&quot;, ylabel = &quot;VPD (kPa)&quot;, title = &quot;VPD calculated twice&quot;, ylim=[0,5], ) # makes slanted dates plt.gcf().autofmt_xdate() ax.legend(loc=&quot;upper right&quot;) . &lt;matplotlib.legend.Legend at 0x7fe6989ca700&gt; . Two y axes . fig, ax = plt.subplots(1, figsize=(8,6)) ax.plot(df[&#39;VPD&#39;], color=&quot;tab:red&quot;, label=&quot;VPD&quot;) plt.gcf().autofmt_xdate() ax2 = ax.twinx() ax2.plot(df[&#39;T1&#39;], color=&quot;tab:cyan&quot;, label=&quot;Temperature&quot;) ax.set(xlabel = &quot;Time&quot;, title = &quot;two y axes&quot;, ylim=[0,5], ) ax.set_ylabel(&#39;VPD (kPa)&#39;, color=&#39;tab:red&#39;) ax.spines[&#39;left&#39;].set_color(&#39;red&#39;) ax2.set_ylabel(&#39;Temperature (deg C)&#39;, color=&#39;tab:cyan&#39;) . Text(0, 0.5, &#39;Temperature (deg C)&#39;) . NaN, Missing data, Outliers . start = &quot;2022-05-03 12:00:00&quot; end = &quot;2022-05-06 00:00:00&quot; fig, ax = plt.subplots(1, figsize=(8,4)) # plot using pandas&#39; plot method df.loc[start:end, &#39;T2&#39;].plot(ax=ax, linestyle=&#39;-&#39;, marker=&#39;o&#39;, color=&quot;tab:blue&quot;, label=&quot;data&quot;) # annotate examples here: # https://jakevdp.github.io/PythonDataScienceHandbook/04.09-text-and-annotation.html ax.annotate(&quot;NaN&quot;, # text to write, if nothing, then &quot;&quot; xy=(&#39;2022-05-03 20:30:00&#39;, 25), # (x,y coordinates for the tip of the arrow) xycoords=&#39;data&#39;, # xy as &#39;data&#39; coordinates xytext=(-20, 60), # xy coordinates for the text textcoords=&#39;offset points&#39;, # xytext relative to xy arrowprops=dict(arrowstyle=&quot;-&gt;&quot;) # pretty arrow ) ax.annotate(&quot;outlier&quot;, xy=(&#39;2022-05-03 22:30:00&#39;, 85), xycoords=&#39;data&#39;, xytext=(40, -20), textcoords=&#39;offset points&#39;, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;) ) ax.annotate(&quot;missing rows&quot;, xy=(&#39;2022-05-05 00:00:00&#39;, 25), xycoords=&#39;data&#39;, xytext=(0, 40), textcoords=&#39;offset points&#39;, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;) ) ax.xaxis.set_major_formatter(mdates.DateFormatter(&#39;%d %b, %H:00&#39;)) plt.gcf().autofmt_xdate() ax.set(xlabel=&quot;&quot;, ylabel=&quot;Temperature (deg C)&quot;) . [Text(0.5, 0, &#39;&#39;), Text(0, 0.5, &#39;Temperature (deg C)&#39;)] . The arrows (annotate) work because the plot was df[&#39;column&#39;].plot() . If you use the usual ax.plot(df[&#39;column&#39;]) then you matplotlib will not understand timestamps as x-positions. In this case follow the instructions below. . start = &quot;2022-05-03 12:00:00&quot; end = &quot;2022-05-06 00:00:00&quot; fig, ax = plt.subplots(1, figsize=(8,4)) ax.plot(df.loc[start:end, &#39;T2&#39;], linestyle=&#39;-&#39;, marker=&#39;o&#39;, color=&quot;tab:blue&quot;, label=&quot;data&quot;) t_nan = &#39;2022-05-03 20:30:00&#39; x_nan = mdates.date2num(dt.datetime.strptime(t_nan, &quot;%Y-%m-%d %H:%M:%S&quot;)) ax.annotate(&quot;NaN&quot;, xy=(x_nan, 25), xycoords=&#39;data&#39;, xytext=(-20, 60), textcoords=&#39;offset points&#39;, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;) ) t_outlier = &#39;2022-05-03 22:30:00&#39; x_outlier = mdates.date2num(dt.datetime.strptime(t_outlier, &quot;%Y-%m-%d %H:%M:%S&quot;)) ax.annotate(&quot;outlier&quot;, xy=(x_outlier, 85), xycoords=&#39;data&#39;, xytext=(40, -20), textcoords=&#39;offset points&#39;, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;) ) t_missing = &#39;2022-05-05 00:00:00&#39; x_missing = mdates.date2num(dt.datetime.strptime(t_missing, &quot;%Y-%m-%d %H:%M:%S&quot;)) ax.annotate(&quot;missing rows&quot;, xy=(x_missing, 25), xycoords=&#39;data&#39;, xytext=(0, 40), textcoords=&#39;offset points&#39;, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;) ) # code for hours, days, etc # https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes ax.xaxis.set_major_formatter(mdates.DateFormatter(&#39;%d %b, %H:00&#39;)) plt.gcf().autofmt_xdate() ax.set(xlabel=&quot;&quot;, ylabel=&quot;Temperature (deg C)&quot;) . [Text(0.5, 0, &#39;&#39;), Text(0, 0.5, &#39;Temperature (deg C)&#39;)] . fig, ax = plt.subplots(1, figsize=(8,4)) delta_index = (df.index.to_series().diff() / pd.Timedelta(&#39;1 sec&#39;) ).values ax.plot(delta_index) ax.set(ylim=[0, 100], xlabel=&quot;running index&quot;, ylabel=r&quot;$ Delta t$ (s)&quot;, title=&quot;Time difference between consecutive rows&quot;) . [(0.0, 100.0), Text(0.5, 0, &#39;running index&#39;), Text(0, 0.5, &#39;$ Delta t$ (s)&#39;), Text(0.5, 1.0, &#39;Time difference between consecutive rows&#39;)] . Resample . Downsampling . fig, ax = plt.subplots(1, figsize=(8,4)) # Downsample to spaced out data points. Change the number below, see what happens. window_size = &#39;15min&#39; df_resampled = (df[&#39;T2&#39;].resample(window_size) # resample doesn&#39;t do anything yet, just divides data into buckets .mean() # this is where stuff happens. you can also choose &quot;sum&quot;, &quot;max&quot;, etc ) # optional, add half a window size to timestamp df_resampled.index = df_resampled.index + to_offset(window_size) / 2 ax.plot(df[&#39;T2&#39;], color=&quot;tab:blue&quot;, label=&quot;original data&quot;) ax.plot(df_resampled, marker=&#39;x&#39;, color=&quot;tab:orange&quot;, zorder=-1, label=f&quot;resampled {window_size} data&quot;) ax.legend() ax.set(xlabel=&quot;time&quot;, ylabel=&quot;temperature (deg C)&quot;) . [Text(0.5, 0, &#39;time&#39;), Text(0, 0.5, &#39;temperature (deg C)&#39;)] . Filling missing data . fig, ax = plt.subplots(1, figsize=(8,4)) # see options for interpolation methods here: # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html df_interpolated1 = df_resampled.interpolate(method=&#39;time&#39;) df_interpolated2 = df_resampled.interpolate(method=&#39;nearest&#39;) ax.plot(df_resampled, color=&quot;tab:orange&quot;, label=&quot;resampled&quot;) ax.plot(df_interpolated1, &#39;.&#39;, color=&quot;tab:purple&quot;, zorder=-1, label=f&quot;time-interpolated&quot;) ax.plot(df_interpolated2, &#39;.&#39;, color=&quot;tab:cyan&quot;, zorder=-2, label=f&quot;nearest-interpolated&quot;) ax.legend() ax.set(xlabel=&quot;time&quot;, ylabel=&quot;temperature (deg C)&quot;) . [Text(0.5, 0, &#39;time&#39;), Text(0, 0.5, &#39;temperature (deg C)&#39;)] . Smoothing noisy data . Let&#39;s first download data from a different project. . filename2 = &quot;test_peleg.csv&quot; # if file is not there, go fetch it from thingspeak if not os.path.isfile(filename2): # define what to download channels = &quot;1708067&quot; fields = &quot;1,2,3,4,5&quot; minutes = &quot;30&quot; # https://www.mathworks.com/help/thingspeak/readdata.html # format YYYY-MM-DD%20HH:NN:SS start = &quot;2022-05-15%2000:00:00&quot; end = &quot;2022-05-25%2000:00:00&quot; # download using Thingspeak&#39;s API # url = f&quot;https://api.thingspeak.com/channels/{channels}/fields/{fields}.csv?minutes={minutes}&quot; url = f&quot;https://api.thingspeak.com/channels/{channels}/fields/{fields}.csv?start={start}&amp;end={end}&quot; data = urllib.request.urlopen(url) d = data.read() # save data to csv file = open(filename2, &quot;w&quot;) file.write(d.decode(&#39;UTF-8&#39;)) file.close() . df = pd.read_csv(filename2) # rename columns df = df.rename(columns={&quot;created_at&quot;: &quot;timestamp&quot;, &quot;field1&quot;: &quot;T&quot;, &quot;field2&quot;: &quot;Tw&quot;, &quot;field3&quot;: &quot;RH&quot;, &quot;field4&quot;: &quot;VPD&quot;, &quot;field5&quot;: &quot;dist&quot;, }) # set timestamp as index df[&#39;timestamp&#39;] = pd.to_datetime(df[&#39;timestamp&#39;]) df = df.set_index(&#39;timestamp&#39;) . df . entry_id T Tw RH VPD dist . timestamp . 2022-05-18 20:09:31+00:00 24716 | 23.85 | 23.3125 | 65.32 | 1.02532 | 7.208 | . 2022-05-18 20:10:32+00:00 24717 | 23.88 | 23.2500 | 65.32 | 1.02717 | 7.208 | . 2022-05-18 20:11:33+00:00 24718 | 23.90 | 23.2500 | 65.23 | 1.03107 | 7.276 | . 2022-05-18 20:12:33+00:00 24719 | 23.90 | 23.2500 | 65.19 | 1.03226 | 7.208 | . 2022-05-18 20:13:34+00:00 24720 | 23.89 | 23.2500 | 65.15 | 1.03282 | 7.633 | . ... ... | ... | ... | ... | ... | ... | . 2022-05-24 12:18:35+00:00 32711 | 27.47 | 26.1250 | 47.49 | 1.92397 | 8.925 | . 2022-05-24 12:19:36+00:00 32712 | 27.47 | 26.1250 | 47.62 | 1.91921 | 8.925 | . 2022-05-24 12:20:39+00:00 32713 | 27.47 | 26.1250 | 47.96 | 1.90675 | 8.925 | . 2022-05-24 12:21:40+00:00 32714 | 27.47 | 26.1875 | 47.75 | 1.91444 | 8.925 | . 2022-05-24 12:22:41+00:00 32715 | 27.49 | 26.1875 | 47.94 | 1.90971 | 8.925 | . 8000 rows Ã— 6 columns . Smoothing noisy data . fig, ax = plt.subplots(1, figsize=(8,4)) ax.plot(df[&#39;RH&#39;], &#39;.&#39;) # add labels and title ax.set(xlabel = &quot;time&quot;, ylabel = &quot;RH (%)&quot;, title = &quot;Relative Humidity&quot;) # makes slanted dates plt.gcf().autofmt_xdate() . Moving average and SavGol . fig, ax = plt.subplots(1, figsize=(8,4)) # apply a rolling average of size &quot;window_size&quot;, # it can be either by number of points, or by window time # window_size = 30 # number of measurements window_size = &#39;120min&#39; # minutes RH_smooth = df[&#39;RH&#39;].rolling(window_size, center=True).mean().to_frame() RH_smooth.rename(columns={&#39;RH&#39;: &#39;rolling_avg&#39;}, inplace=True) RH_smooth[&#39;SG&#39;] = savgol_filter(df[&#39;RH&#39;], window_length=121, polyorder=2) ax.plot(df[&#39;RH&#39;], color=&quot;tab:blue&quot;, label=&quot;data&quot;) ax.plot(RH_smooth[&#39;rolling_avg&#39;], color=&quot;tab:orange&quot;, label=&quot;moving average&quot;) ax.plot(RH_smooth[&#39;SG&#39;], color=&quot;tab:red&quot;, label=&quot;Savitzky-Golay filter&quot;) # add labels and title ax.set(xlabel = &quot;time&quot;, ylabel = &quot;RH (%)&quot;, title = &quot;Relative Humidity&quot;) # makes slanted dates plt.gcf().autofmt_xdate() ax.legend() . &lt;matplotlib.legend.Legend at 0x7fe6a0525730&gt; . How does moving average work? . How does the Savitzkyâ€“Golay filter work? .",
            "url": "https://yairmau.github.io/website/jupyter/2020/03/01/basic-tsa.html",
            "relUrl": "/jupyter/2020/03/01/basic-tsa.html",
            "date": " â€¢ Mar 1, 2020"
        }
        
    
  
    
        ,"post23": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.â†© . 2. This is the other footnote. You can even have a link!â†© .",
            "url": "https://yairmau.github.io/website/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " â€¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post24": {
            "title": "Spatial distribution - lecture",
            "content": "Sources . (Brutsaert, 2005) (Dingman, 2015) (Ward and Trimble, 2003) . The problem . Letâ€™s say we want to calculate the average rainfall on a watershed, and we have data available for 7 stations, as shown in the figure below [(Dingman, 2015), figure 4.26]: . There are a number of methods for calculating the average precipitation. . Thiessen method [Voronoi diagram] . (Brutsaert, 2005), Figure 3.11 . How to compute the areas: (Ward and Trimble, 2003) . Average areal precipitation is a weighted sum: . âŸ¨PâŸ©=âˆ‘iAiPiâˆ‘iAi langle P rangle = frac{ sum_i A_i P_i}{ sum_i A_i}âŸ¨PâŸ©=âˆ‘iâ€‹Aiâ€‹âˆ‘iâ€‹Aiâ€‹Piâ€‹â€‹ . A nice way to understand the Thiessen method is depicted in the gif below (from Wikipedia): . . Inverse distance method . Brutsaert, Figure 3.12 . The precipitation for square 17 is . P17=âˆ‘iÂ =Â allÂ stationsPidi,172âˆ‘iÂ =Â allÂ stations1di,172P_{17} = displaystyle frac { displaystyle sum_ text{$i$ = all stations} frac{P_i}{d_{i,17}^2}} { displaystyle sum_ text{$i$ = all stations} frac{1}{d_{i,17}^2}}P17â€‹=iÂ =Â allÂ stationsâˆ‘â€‹di,172â€‹1â€‹iÂ =Â allÂ stationsâˆ‘â€‹di,172â€‹Piâ€‹â€‹â€‹ . The average precipitation for the whole watershed is the weighted average of all squares, where the weight is their area: . âŸ¨PâŸ©=âˆ‘jÂ =Â allÂ squaresAjPjâˆ‘jÂ =Â allÂ squaresAj langle P rangle = displaystyle frac { displaystyle sum_ text{$j$ = all squares} A_j P_j} { displaystyle sum_ text{$j$ = all squares} A_j}âŸ¨PâŸ©=jÂ =Â allÂ squaresâˆ‘â€‹Ajâ€‹jÂ =Â allÂ squaresâˆ‘â€‹Ajâ€‹Pjâ€‹â€‹ . (Brutsaert, 2005) , page 93: . Dean and Snyder (1977) found that the exponent (for the distance $d^{-b}$) b = 2 yielded the best results in the Piedmont region of the southeastern United States, whereas Simanton and Osborn (1980) concluded from measurements in Arizona that b can range between 1 and 3 without significantly affecting the results. . Isohyetal method . (Brutsaert, 2005) , Figure 3.12 . The same equation of the Thiessen method can be used: . âŸ¨PâŸ©=âˆ‘iAiPiâˆ‘iAi langle P rangle = frac{ sum_i A_i P_i}{ sum_i A_i}âŸ¨PâŸ©=âˆ‘iâ€‹Aiâ€‹âˆ‘iâ€‹Aiâ€‹Piâ€‹â€‹ . How it is actually done . Most often, Geographic Information System (GIS) software is used to analyze spatial data. Two of the most used programs are ArcGIS (proprietary) and QGIS (free). . A good discussion of the different methods can be found on Manuel Gimondâ€™s website, Intro to GIS and Spatial Analysis. . Attention, Donâ€™t mix precision with accuracy. There are many ways of interpolating, just because a result seems detailed, it does not imply that it is accurate! See below three interpolation methods. . . Below you can find a simple Python code that exemplifies some of the methods, producing the following figure: . . %matplotlib notebook import matplotlib.pyplot as plt import numpy as np from scipy.interpolate import griddata from scipy.spatial import Voronoi, voronoi_plot_2d, ConvexHull fig, ax = plt.subplots(1, 3, figsize=(10,7)) fig.subplots_adjust(left=0.0, right=1.0, top=0.96, bottom=0.05, hspace=0.02, wspace=0.02) N = 6 PI = &#39;3141592653589793&#39; points = np.random.rand(N, 2) points = np.vstack([points,[0,0], [0,1], [1,0], [1,1]]) values = np.array([int(x) for x in list(PI)])[:(N+4)] # values = np.array([3, 1, 4, 1, 5, 9, 2, 6, 5, 3]) grid_x, grid_y = np.mgrid[0:1:100j, 0:1:200j] grid_z_nearest = griddata(points, values, (grid_x, grid_y), method=&#39;nearest&#39;) grid_z_cubic = griddata(points, values, (grid_x, grid_y), method=&#39;cubic&#39;) ax[0].plot(points[:,0], points[:,1], &#39;o&#39;, ms=3, markerfacecolor=&quot;red&quot;, markeredgecolor=&quot;red&quot;) ax[0].set_aspect(&#39;equal&#39;, &#39;box&#39;) ax[0].set(xlim=[0,1], ylim=[0,1]) ax[0].set_title(&quot;the stations&quot;) for i, v in enumerate(values): ax[0].text(points[i,0], points[i,1], str(v)) ax[1].imshow(grid_z_nearest.T, extent=(0,1,0,1), origin=&#39;lower&#39;) ax[1].plot(points[:,0], points[:,1], &#39;o&#39;, ms=3, markerfacecolor=&quot;red&quot;, markeredgecolor=&quot;red&quot;) vor = Voronoi(points) voronoi_plot_2d(vor, show_vertices=False, line_colors=&#39;cyan&#39;, line_width=3, line_alpha=1, point_size=0, ax=ax[1]) ax[1].set_title(&quot;Thiessen Method&quot;) ax[2].plot(points[:,0], points[:,1], &#39;o&#39;, ms=3, markerfacecolor=&quot;red&quot;, markeredgecolor=&quot;red&quot;) nlines = int((values.max()-values.min()+1)/2) ax[2].contourf(grid_x, grid_y, grid_z_cubic, nlines) cont = ax[2].contour(grid_x, grid_y, grid_z_cubic, nlines, colors=&quot;black&quot;) ax[2].clabel(cont, inline=1, colors=&#39;white&#39;, fmt=&#39;%.0f&#39;) ax[2].set_title(&quot;Isohyetal Method&quot;) for i, a in enumerate(ax): a.set(xlim=[-0.2,1.2], ylim=[-0.2,1.2]) a.axis(&#39;off&#39;) a.set_aspect(&#39;equal&#39;, &#39;box&#39;) fig.savefig(&quot;spatial-distribution.png&quot;, dpi=500) . References . Brutsaert, W., 2005. Hydrology: An Introduction. Cambridge University Press. | Dingman, S.L., 2015. Physical Hydrology: Third Edition. Waveland Press. | Ward, A.D., Trimble, S.W., 2003. Environmental Hydrology, Second Edition. CRC Press. |",
            "url": "https://yairmau.github.io/website/markdown/2020/02/07/spatial-distribution-lecture.html",
            "relUrl": "/markdown/2020/02/07/spatial-distribution-lecture.html",
            "date": " â€¢ Feb 7, 2020"
        }
        
    
  
    
        ,"post25": {
            "title": "Spatial distribution - lecture",
            "content": "The problem . Let&#39;s say we want to calculate the average rainfall on a watershed, and we have data available for 7 stations, as shown in the figure below [Dingman, figure 4.26]: . There are a number of methods for calculating the average precipitation. . Thiessen method [Voronoi diagram] . Brutsaert, Figure 3.11 . How to compute the areas: . Average areal precipitation is a weighted sum: . $$ langle P rangle = frac{ sum_i A_i P_i}{ sum_i A_i} $$A nice way to understand the Thiessen method is depicted in the gif below (from Wikipedia): . . Inverse distance method . Brutsaert, Figure 3.12 . The precipitation for square 17 is . $$ P_{17} = displaystyle frac { displaystyle sum_ text{$i$ = all stations} frac{P_i}{d_{i,17}^2}} { displaystyle sum_ text{$i$ = all stations} frac{1}{d_{i,17}^2}} $$The average precipitation for the whole watershed is the weighted average of all squares, where the weight is their area: . $$ langle P rangle = displaystyle frac { displaystyle sum_ text{$j$ = all squares} A_j P_j} { displaystyle sum_ text{$j$ = all squares} A_j} $$Brutsaert, page 93: . Dean and Snyder (1977) found that the exponent (for the distance $d^{-b}$) b = 2 yielded the best results in the Piedmont region of the southeastern United States, whereas Simanton and Osborn (1980) concluded from measurements in Arizona that b can range between 1 and 3 without significantly affecting the results. . Isohyetal method . Brutsaert, Figure 3.12 . The same equation of the Thiessen method can be used: . $$ langle P rangle = frac{ sum_i A_i P_i}{ sum_i A_i} $$ How it is actually done . Most often, Geographic Information System (GIS) software is used to analyze spatial data. Two of the most used programs are ArcGIS (proprietary) and QGIS (free). . A good discussion of the different methods can be found on Manuel Gimond&#39;s website, Intro to GIS and Spatial Analysis. . Attention, Don&#39;t mix precision with accuracy. There are many ways of interpolating, just because a result seems detailed, it does not imply that it is accurate! See below three interpolation methods. . . Below you can find a simple Python code that exemplifies some of the methods, producing the following figure: . . %matplotlib notebook import matplotlib.pyplot as plt import numpy as np from scipy.interpolate import griddata from scipy.spatial import Voronoi, voronoi_plot_2d, ConvexHull fig, ax = plt.subplots(1, 3, figsize=(10,7)) fig.subplots_adjust(left=0.0, right=1.0, top=0.96, bottom=0.05, hspace=0.02, wspace=0.02) N = 6 PI = &#39;3141592653589793&#39; points = np.random.rand(N, 2) points = np.vstack([points,[0,0], [0,1], [1,0], [1,1]]) values = np.array([int(x) for x in list(PI)])[:(N+4)] # values = np.array([3, 1, 4, 1, 5, 9, 2, 6, 5, 3]) grid_x, grid_y = np.mgrid[0:1:100j, 0:1:200j] grid_z_nearest = griddata(points, values, (grid_x, grid_y), method=&#39;nearest&#39;) grid_z_cubic = griddata(points, values, (grid_x, grid_y), method=&#39;cubic&#39;) ax[0].plot(points[:,0], points[:,1], &#39;o&#39;, ms=3, markerfacecolor=&quot;red&quot;, markeredgecolor=&quot;red&quot;) ax[0].set_aspect(&#39;equal&#39;, &#39;box&#39;) ax[0].set(xlim=[0,1], ylim=[0,1]) ax[0].set_title(&quot;the stations&quot;) for i, v in enumerate(values): ax[0].text(points[i,0], points[i,1], str(v)) ax[1].imshow(grid_z_nearest.T, extent=(0,1,0,1), origin=&#39;lower&#39;) ax[1].plot(points[:,0], points[:,1], &#39;o&#39;, ms=3, markerfacecolor=&quot;red&quot;, markeredgecolor=&quot;red&quot;) vor = Voronoi(points) voronoi_plot_2d(vor, show_vertices=False, line_colors=&#39;cyan&#39;, line_width=3, line_alpha=1, point_size=0, ax=ax[1]) ax[1].set_title(&quot;Thiessen Method&quot;) ax[2].plot(points[:,0], points[:,1], &#39;o&#39;, ms=3, markerfacecolor=&quot;red&quot;, markeredgecolor=&quot;red&quot;) nlines = int((values.max()-values.min()+1)/2) ax[2].contourf(grid_x, grid_y, grid_z_cubic, nlines) cont = ax[2].contour(grid_x, grid_y, grid_z_cubic, nlines, colors=&quot;black&quot;) ax[2].clabel(cont, inline=1, colors=&#39;white&#39;, fmt=&#39;%.0f&#39;) ax[2].set_title(&quot;Isohyetal Method&quot;) for i, a in enumerate(ax): a.set(xlim=[-0.2,1.2], ylim=[-0.2,1.2]) a.axis(&#39;off&#39;) a.set_aspect(&#39;equal&#39;, &#39;box&#39;) fig.savefig(&quot;spatial-distribution.png&quot;, dpi=500) .",
            "url": "https://yairmau.github.io/website/jupyter/2020/02/07/spatial-distribution.html",
            "relUrl": "/jupyter/2020/02/07/spatial-distribution.html",
            "date": " â€¢ Feb 7, 2020"
        }
        
    
  
    
        ,"post26": {
            "title": "Budyko framework - lecture",
            "content": "Sources . (Daly et al., 2019) (Sposito, 2017) (Jones et al., 2012) (Krajewski et al., 2021) (Berghuijs et al., 2020) (Creed and Spargo, 2012) . Water and surface energy balances . For long-term averages: . P=ET+QP = ET+QP=ET+Q . Rn=Î»wâ‹…ET+HR_n = lambda_w cdot ET + HRnâ€‹=Î»wâ€‹â‹…ET+H . $P$: precipitation (L T$^{-1}$, e.g.: mm/day) | $ET$: evapotranspiration (L T$^{-1}$) | $Q$: streamflow (L T$^{-1}$) | $R_n$: net energy available at soil surface (M T$^{-3}$, e.g.: W m$^{-2}$) | $ lambda_w$: latent heat of vaporization of water (M L$^{-1}$T$^{-2}$, as defined here, the units will be weird) | $H$: sensible heat flux from the surface into the atmosphere (M T$^{-3}$) | $ lambda_w cdot ET$: latent heat flux (M T$^{-3}$) | . Assumptions . because we are dealing with long-term averages, there are negligible changes of watershed stored water. | negligible energy is stored at the soil surface, and heat transfer from soil surface to deeper soil layers ($G$) averages zero. | Question . Given measurements of rainfall and meteorological conditions, can we predict the partitioning of $P$ between $ET$ and $Q$? . Limits . For very dry watersheds (deserts, for example), almost all precipitation ($P$) is lost via evapotranspiration ($ET$). These watersheds are called water limited. . In wet watersheds, at the annual scale, the sensible heat ($H$) is directed from the surface to the atmosphere in almost all climatic zones on Earth (meaning: soil heats air). Therefore, $H$ cannot supply much energy to the soil surface, and it is assumed that $R_n$ provides entirely the energy required for evapotranspiration. Dividing the second equation by $ lambda_w$, we get $R_n/ lambda_w = ET + H/ lambda_w$. It is clear that the maximum possible $ET$ occurs when all incoming radiation energy $R_n$ is consumed by evapotranspiration $ET$, and there is negligible sensible heat flux $H$. As a result, the upper limit of $ lambda_w E$ is $R_n$, in wet watersheds. In these watersheds, called energy limited, $ET$ tends to the potential evapotranspiration ($ET_0$). . Summary: . For energy-limited watersheds . (1) As precipitation $P rightarrow infty$, evapotranspiration $ET rightarrow ET_0$ . For water-limited watersheds . (2) As potential evapotranspiration $ET_0 rightarrow infty$, actual evaporation $ET rightarrow P$ . In general, we can write . ET=f(P,ET0)ET = f(P,ET_0)ET=f(P,ET0â€‹) . The variables $P$ and $ET$ have the same dimenstions (L T$^{-1}$), and we can divide the equation above by $P$: . ETP=f(DI), frac{ET}{P} = f(D_I),PETâ€‹=f(DIâ€‹), . where DI=ET0PD_I = displaystyle frac{ET_0}{P}DIâ€‹=PET0â€‹â€‹ is called the dryness index. A useful classification is . Dryness Index Classification . $D_I &lt; 1.54$ | Humid | . $1.54 &lt; D_I &lt; 2$ | Dry Subhumid | . $2 &lt; D_I &lt; 5$ | Semi-arid | . $5 &lt; D_I &lt; 20$ | Arid | . $20 &lt; D_I$ | Hyper-arid | . ATTENTION. The dryness index can also be called the â€œAridity Indexâ€ ($AI$), however sometimes the $AI$ means the inverse of $D_I$: AI=1/DIAI = 1/D_IAI=1/DIâ€‹ Be careful to check the definitions. . The summary (1) and (2) above can be now represented as: . (1) As $D_I rightarrow 0$, $ displaystyle frac{ET}{P} rightarrow D_I$ . (2) As $D_I rightarrow infty$, $ displaystyle frac{ET}{P} rightarrow 1$ . . . Budyko (1974), proposed the following equation: . ETP=[DItanhâ¡(1DI)(1âˆ’eâˆ’DI)]1/2 frac{ET}{P} = left[ D_I tanh left( frac{1}{D_I} right) left( 1-e^{-D_I} right) right]^{1/2}PETâ€‹=[DIâ€‹tanh(DIâ€‹1â€‹)(1âˆ’eâˆ’DIâ€‹)]1/2 . . Source: (Jones et al., 2012) . Source: (Krajewski et al., 2021) . There are many alternatives to Budykoâ€™s equation. Many equations have adjustable parameters, such as Fuâ€™s equation: . ETP=1+DIâˆ’(1+DIw)1/w, frac{ET}{P} = 1 + D_I - (1 + D_I^w)^{1/w},PETâ€‹=1+DIâ€‹âˆ’(1+DIwâ€‹)1/w, . where $w&gt;1$. Each catchment has its own specific parameter $w$, that may represent biophysical/landscape features. There is no concensus regarding the interpretation of $w$, ranging from an effective empirical parameter, whose relationship to biophysical features can be discerned, to an arbitrary empirical constant with no a priori physical meaning. Source: (Reaver et al., 2020) . Source: (Zhang et al., 2004) . Hypotheses for why dryness index controls so much the partitioning of P into ET and Q . Source: (Berghuijs et al., 2020) . The first is that the Budyko curve is accurate because landscape features (e.g., soils and vegetation) coevolve with the local climate in such a manner that precipitation partitioning into streamflow and evapotranspiration converges towards the Budyko curve | A second hypothesis is that catchments over time evolve towards the supply and demand limits (rather than towards a curve), because landscapes and their vegetation are unaware of the Budyko curve but do evolve to maximize their use of available resources (including water). However, because limiting factors such as climatic variability exist (which will reduce a catchmentâ€™s ability to use all water because it cannot fully buffer the highly variable precipitation input), catchments will tend to not reach these limits. This may lead to an (apparent) existence of the Budyko curve which falls relatively close to the demand and supply limits. | A third hypothesis is that the existence of a strong universal relationship between aridity and catchment water balances might be explained by an underlying organizing principle such as maximum entropy production because the Budyko curve may be consistent with how hydrologic systems optimally partition water and energy | A fourth hypothesis is that virtually any landscape and climate combination (also those in heavily disturbed landscapes: e.g., a city, agricultural lands, etc.) will fall near the Budyko curve because climate aridity will dominate precipitation partitioning largely independent of the climate-landscape configuration or any optimization principle. | Hypotheses for deviations from Budyko curve . Source: (Creed and Spargo, 2012) . Under stationary conditions (naturally occurring oscillations), catchments will fall on the Budyko Curve | Under non-stationary conditions (anthropogenic climate change), catchments will deviate from the Budyko Curve in a predictable manner | Reasons for falling off the Budyko Curve . Inadequate representation of P and T (Loch Vale) | Inadequate representation of ET (Andrews) | Inadequate representation of Q (Marcell) | Forest conversion (Coweeta) | Forest disturbance (Luquillo) | Critique . Source: (Berghuijs et al., 2020) . The (mathematical) specifics of such studies vary, but all approaches are founded on the assumption that catchments follow a (parametric) Budyko curve when aridity changes, and that consequently all other movements in the Budyko space are caused by other factors. The validity of this assumption remains mostly untested, which seems surprising given it underpins all of these studiesâ€™ findings. . References . Daly, E., Calabrese, S., Yin, J., Porporato, A., 2019. Linking parametric and water-balance models of the Budyko and Turc spaces. Advances in Water Resources 134, 103435. | Sposito, G., 2017. Understanding the Budyko equation. Water 9, 236. | Jones, J.A., Creed, I.F., Hatcher, K.L., Warren, R.J., Adams, M.B., Benson, M.H., Boose, E., Brown, W.A., Campbell, J.L., Covich, A., others, 2012. Ecosystem processes and human influences regulate streamflow response to climate change at long-term ecological research sites. BioScience 62, 390â€“404. | Krajewski, A., Sikorska-Senoner, A.E., Hejduk, L., Banasik, K., 2021. An Attempt to Decompose the Impact of Land Use and Climate Change on Annual Runoff in a Small Agricultural Catchment. Water Resources Management 35, 881â€“896. | Berghuijs, W.R., Gnann, S.J., Woods, R.A., 2020. Unanswered questions on the Budyko framework. Hydrological Processes 34, 5699â€“5703. | Creed, I., Spargo, A., 2012. Budyko guide to exploring sustainability of water yields from catchments under changing environmental conditions. London, Ontario. | Reaver, N.G.F., Kaplan, D.A., Klammler, H., Jawitz, J.W., 2020. Reinterpreting the Budyko Framework. Hydrology and Earth System Sciences Discussions 1â€“31. | Zhang, L., Hickel, K., Dawes, W.R., Chiew, F.H.S., Western, A.W., Briggs, P.R., 2004. A rational function approach for estimating mean annual evapotranspiration. Water resources research 40. |",
            "url": "https://yairmau.github.io/website/markdown/2020/02/06/budyko-framework-lecture.html",
            "relUrl": "/markdown/2020/02/06/budyko-framework-lecture.html",
            "date": " â€¢ Feb 6, 2020"
        }
        
    
  
    
        ,"post27": {
            "title": "Budyko framework - lecture",
            "content": "Sources used: . (Daly et al., 2019), (Sposito, 2017), (Jones et al., 2012), (Krajewski et al., 2021), (Berghuijs et al., 2020), (Creed and Spargo, 2012) . Water and surface energy balances . For long-term averages: . $$ P = ET+Q $$$$ R_n = lambda_w cdot ET + H $$ $P$: precipitation (L T$^{-1}$, e.g.: mm/day) | $ET$: evapotranspiration (L T$^{-1}$) | $Q$: streamflow (L T$^{-1}$) | $R_n$: net energy available at soil surface (M T$^{-3}$, e.g.: W m$^{-2}$) | $ lambda_w$: latent heat of vaporization of water (M L$^{-1}$T$^{-2}$, as defined here, the units will be weird) | $H$: sensible heat flux from the surface into the atmosphere (M T$^{-3}$) | $ lambda_w cdot ET$: latent heat flux (M T$^{-3}$) | . Assumptions . because we are dealing with long-term averages, there are negligible changes of watershed stored water. | negligible energy is stored at the soil surface, and heat transfer from soil surface to deeper soil layers ($G$) averages zero. | Question . Given measurements of rainfall and meteorological conditions, can we predict the partitioning of $P$ between $ET$ and $Q$? . Limits . For very dry watersheds (deserts, for example), almost all precipitation ($P$) is lost via evapotranspiration ($ET$). These watersheds are called water limited. . In wet watersheds, at the annual scale, the sensible heat ($H$) is directed from the surface to the atmosphere in almost all climatic zones on Earth (meaning: soil heats air). Therefore, $H$ cannot supply much energy to the soil surface, and it is assumed that $R_n$ provides entirely the energy required for evapotranspiration. Dividing the second equation by $ lambda_w$, we get $R_n/ lambda_w = ET + H/ lambda_w$. It is clear that the maximum possible $ET$ occurs when all incoming radiation energy $R_n$ is consumed by evapotranspiration $ET$, and there is negligible sensible heat flux $H$. As a result, the upper limit of $ lambda_w E$ is $R_n$, in wet watersheds. In these watersheds, called energy limited, $ET$ tends to the potential evapotranspiration ($ET_0$). . Summary: . For energy-limited watersheds . (1) As precipitation $P rightarrow infty$, evapotranspiration $ET rightarrow ET_0$ . For water-limited watersheds . (2) As potential evapotranspiration $ET_0 rightarrow infty$, actual evaporation $ET rightarrow P$ . In general, we can write . $$ ET = f(P,ET_0) $$The variables $P$ and $ET$ have the same dimenstions (L T$^{-1}$), and we can divide the equation above by $P$:$$ frac{ET}{P} = f(D_I), $$ . where $$ D_I = displaystyle frac{ET_0}{P} $$ is called the dryness index. A useful classification is . Dryness Index Classification . $D_I &lt; 1.54$ | Humid | . $1.54 &lt; D_I &lt; 2$ | Dry Subhumid | . $2 &lt; D_I &lt; 5$ | Semi-arid | . $5 &lt; D_I &lt; 20$ | Arid | . $20 &lt; D_I$ | Hyper-arid | . ATTENTION. The dryness index can also be called the &quot;Aridity Index&quot; ($AI$), however sometimes the $AI$ means the inverse of $D_I$: $$AI = 1/D_I$$ Be careful to check the definitions. . The summary (1) and (2) above can be now represented as: . (1) As $D_I rightarrow 0$, $ displaystyle frac{ET}{P} rightarrow D_I$ . (2) As $D_I rightarrow infty$, $ displaystyle frac{ET}{P} rightarrow 1$ . . . Budyko (1974), proposed the following equation: . $$ frac{ET}{P} = left[ D_I tanh left( frac{1}{D_I} right) left( 1-e^{-D_I} right) right]^{1/2} $$ . Source: (Jones et al., 2012) . Source: (Krajewski et al., 2021) . There are many alternatives to Budyko&#39;s equation. Many equations have adjustable parameters, such as Fu&#39;s equation: . $$ frac{ET}{P} = 1 + D_I - (1 + D_I^w)^{1/w}, $$where $w&gt;1$. Each catchment has its own specific parameter $w$, that may represent biophysical/landscape features. There is no concensus regarding the interpretation of $w$, ranging from an effective empirical parameter, whose relationship to biophysical features can be discerned, to an arbitrary empirical constant with no a priori physical meaning. Source: (Reaver et al., 2020) . Source: (Zhang et al., 2004) . Hypotheses for why dryness index controls so much the partitioning of P into ET and Q . Source: (Berghuijs et al., 2020) . The first is that the Budyko curve is accurate because landscape features (e.g., soils and vegetation) coevolve with the local climate in such a manner that precipitation partitioning into streamflow and evapotranspiration converges towards the Budyko curve | A second hypothesis is that catchments over time evolve towards the supply and demand limits (rather than towards a curve), because landscapes and their vegetation are unaware of the Budyko curve but do evolve to maximize their use of available resources (including water). However, because limiting factors such as climatic variability exist (which will reduce a catchment&#39;s ability to use all water because it cannot fully buffer the highly variable precipitation input), catchments will tend to not reach these limits. This may lead to an (apparent) existence of the Budyko curve which falls relatively close to the demand and supply limits. | A third hypothesis is that the existence of a strong universal relationship between aridity and catchment water balances might be explained by an underlying organizing principle such as maximum entropy production because the Budyko curve may be consistent with how hydrologic systems optimally partition water and energy | A fourth hypothesis is that virtually any landscape and climate combination (also those in heavily disturbed landscapes: e.g., a city, agricultural lands, etc.) will fall near the Budyko curve because climate aridity will dominate precipitation partitioning largely independent of the climate-landscape configuration or any optimization principle. | Hypotheses for deviations from Budyko curve . Source: (Creed and Spargo, 2012) . Under stationary conditions (naturally occurring oscillations), catchments will fall on the Budyko Curve | Under non-stationary conditions (anthropogenic climate change), catchments will deviate from the Budyko Curve in a predictable manner | Reasons for falling off the Budyko Curve . Inadequate representation of P and T (Loch Vale) | Inadequate representation of ET (Andrews) | Inadequate representation of Q (Marcell) | Forest conversion (Coweeta) | Forest disturbance (Luquillo) | Critique . Source: (Berghuijs et al., 2020) . The (mathematical) specifics of such studies vary, but all approaches are founded on the assumption that catchments follow a (parametric) Budyko curve when aridity changes, and that consequently all other movements in the Budyko space are caused by other factors. The validity of this assumption remains mostly untested, which seems surprising given it underpins all of these studies&#39; findings. . References . Daly, E., Calabrese, S., Yin, J., Porporato, A., 2019. Linking parametric and water-balance models of the Budyko and Turc spaces. Advances in Water Resources 134, 103435. | Sposito, G., 2017. Understanding the Budyko equation. Water 9, 236. | Jones, J.A., Creed, I.F., Hatcher, K.L., Warren, R.J., Adams, M.B., Benson, M.H., Boose, E., Brown, W.A., Campbell, J.L., Covich, A., others, 2012. Ecosystem processes and human influences regulate streamflow response to climate change at long-term ecological research sites. BioScience 62, 390â€“404. | Krajewski, A., Sikorska-Senoner, A.E., Hejduk, L., Banasik, K., 2021. An Attempt to Decompose the Impact of Land Use and Climate Change on Annual Runoff in a Small Agricultural Catchment. Water Resources Management 35, 881â€“896. | Berghuijs, W.R., Gnann, S.J., Woods, R.A., 2020. Unanswered questions on the Budyko framework. Hydrological Processes 34, 5699â€“5703. | Creed, I., Spargo, A., 2012. Budyko guide to exploring sustainability of water yields from catchments under changing environmental conditions. London, Ontario. | Reaver, N.G.F., Kaplan, D.A., Klammler, H., Jawitz, J.W., 2020. Reinterpreting the Budyko Framework. Hydrology and Earth System Sciences Discussions 1â€“31. | Zhang, L., Hickel, K., Dawes, W.R., Chiew, F.H.S., Western, A.W., Briggs, P.R., 2004. A rational function approach for estimating mean annual evapotranspiration. Water resources research 40. | .",
            "url": "https://yairmau.github.io/website/jupyter/2020/02/06/budyko-framework-lecture.html",
            "relUrl": "/jupyter/2020/02/06/budyko-framework-lecture.html",
            "date": " â€¢ Feb 6, 2020"
        }
        
    
  
    
        ,"post28": {
            "title": "Budyko framework - code",
            "content": "import matplotlib.pyplot as plt import numpy as np import seaborn as sns sns.set(style=&quot;ticks&quot;, font_scale=1.5) . def budyko_original(DI): return (DI * np.tanh(1/DI) * (1-np.exp(-DI)) )**0.5 def fu(DI, omega): return 1 + DI - (1 + DI**omega)**(1/omega) . fig, ax = plt.subplots(1, 1, figsize=(10,7)) ai = np.linspace(0.01, 7, 101) ax.fill_between([0, 1.54], 2, color=&quot;tab:blue&quot;, edgecolor=&#39;face&#39;) ax.fill_between([1.54, 2], 2, color=&quot;tab:blue&quot;, alpha=0.6, edgecolor=&#39;face&#39;) ax.fill_between([2, 5], 2, color=&quot;tab:blue&quot;, alpha=0.4, edgecolor=&#39;face&#39;) ax.fill_between([5, 20], 2, color=&quot;tab:blue&quot;, alpha=0.2, edgecolor=&#39;face&#39;) ax.text(1.54, 1.0, &quot;Humid&quot;, color=&quot;white&quot;, ha=&quot;right&quot;) ax.text(2, 0.8, &quot;Dry Subhumid&quot;, color=&quot;white&quot;, ha=&quot;right&quot;) ax.text(5, 0.6, &quot;Semi-arid&quot;, color=&quot;black&quot;, ha=&quot;right&quot;) ax.text(7, 0.4, &quot;Arid&quot;, color=&quot;black&quot;, ha=&quot;right&quot;) ax.set(xlabel=r&quot;dryness index ($ET_0/P$)&quot;, ylabel=r&quot;evaporation index ($ET/P$)&quot;, xlim=[0, 7], ylim=[0, 1.1]) plt.savefig(&quot;hydrology_figures/budyko0.png&quot;) . fig, ax = plt.subplots(1, 1, figsize=(10,7)) ai = np.linspace(0.01, 4, 101) # ax.plot(ai, turk_pike(ai), color=&quot;black&quot;, lw=3) # ax.plot(ai, fu(ai, 2.7), color=&quot;black&quot;, lw=3) ax.plot([0, 4], 2*[1], color=&quot;tab:blue&quot;, lw=3) ax.plot([0, 1.1], [0, 1.1], color=&quot;tab:red&quot;, lw=3) ax.text(2, 1.02, &quot;water limit&quot;, color=&quot;tab:blue&quot;) ax.text(0.5, 0.65, &quot;energy limit&quot;, rotation=68, color=&quot;tab:red&quot;) # ax.text(2, 0.84, &quot;Budyko curve&quot;) ax.set(xlabel=r&quot;dryness index ($ET_0/P$)&quot;, ylabel=r&quot;evaporation index ($ET/P$)&quot;, xlim=[0, 4], ylim=[0, 1.1]) plt.savefig(&quot;hydrology_figures/budyko1.png&quot;) . fig, ax = plt.subplots(1, 1, figsize=(10,7)) DI = np.linspace(0.01, 4, 101) # ax.plot(ai, fu(ai, 2.7), color=&quot;black&quot;, lw=3) ax.plot(DI, budyko_original(DI), color=&quot;black&quot;, lw=3) ax.plot([0, 4], 2*[1], color=&quot;tab:blue&quot;, lw=3) ax.plot([0, 1.1], [0, 1.1], color=&quot;tab:red&quot;, lw=3) ax.text(2, 1.02, &quot;water limit&quot;, color=&quot;tab:blue&quot;) ax.text(0.5, 0.65, &quot;energy limit&quot;, rotation=68, color=&quot;tab:red&quot;) ax.text(2, 0.84, &quot;Budyko curve&quot;) ax.set(xlabel=r&quot;dryness index ($ET_0/P$)&quot;, ylabel=r&quot;evaporation index ($ET/P$)&quot;, xlim=[0, 4], ylim=[0, 1.1]) plt.savefig(&quot;hydrology_figures/budyko2.png&quot;) . fig, ax = plt.subplots(1, 1, figsize=(10,7)) ai = np.linspace(0.01, 4, 101) ax.plot(ai, turk_pike(ai), color=&quot;black&quot;, lw=3) # ax.plot(ai, fu(ai, 2.7)) ax.plot(ax.get_xlim(), 2*[1], color=&quot;tab:blue&quot;, lw=3) ax.plot([0, 1.1], [0, 1.1], color=&quot;tab:red&quot;, lw=3) ax.text(2, 1.02, &quot;water limit&quot;, color=&quot;tab:blue&quot;) ax.text(0.5, 0.65, &quot;energy limit&quot;, rotation=68, color=&quot;tab:red&quot;) ax.text(2, 0.84, &quot;Budyko curve&quot;) arrow_x = 1.5 ax.annotate(&quot;&quot;, xy=(arrow_x, turk_pike(arrow_x)), #xycoords=&#39;data&#39;, xytext=(arrow_x, 0),# textcoords=&#39;data&#39;, size=26, arrowprops=dict(arrowstyle=&quot;&lt;-&gt;&quot;, shrinkA=0, shrinkB=0, connectionstyle=&quot;arc3&quot;, color=&#39;black&#39;) ) ax.annotate(&quot;&quot;, xy=(arrow_x, turk_pike(arrow_x)), #xycoords=&#39;data&#39;, xytext=(arrow_x, 1),# textcoords=&#39;data&#39;, size=26, arrowprops=dict(arrowstyle=&quot;&lt;-&gt;&quot;, shrinkA=0, shrinkB=0, connectionstyle=&quot;arc3&quot;, color=&#39;black&#39;) ) ax.text(1.55, 0.4, &quot;actual evaporation&quot;) ax.text(1.55, 0.9, &quot;Q&quot;) ax.set(xlabel=&quot;aridity index (PET/P)&quot;, ylabel=&quot;evaporation index (AEP/P)&quot;, xlim=[0, 4], ylim=[0, 1.1]) plt.savefig(&quot;hydrology_figures/budyko2.png&quot;) .",
            "url": "https://yairmau.github.io/website/jupyter/2020/02/06/budyko-framework-code.html",
            "relUrl": "/jupyter/2020/02/06/budyko-framework-code.html",
            "date": " â€¢ Feb 6, 2020"
        }
        
    
  
    
        ,"post29": {
            "title": "Unit Hydrograph - lecture",
            "content": "Sources . Sources used: . (Dingman, 2015) (Ward and Trimble, 2003) . Linear reservoir model . (Dingman, 2015) . The model above is called â€œlinearâ€ because in equation (10B3.2) the event-flow rate $q^* $ is directly proportional to storage $S^* $. Here is a depiction of the linear model, and how outflow depends on storage, from MaÅ„ko and Laskowski (2018). . . . The Unit Hydrograph . Assumptions: . Time Invariance. The direct runoff response for a given effective rainfall in a catchment is always the same, irrespective of when it occurs. | Linear Response. The direct runoff response is a linear function of rainfall excess: $q^* (t) = f(p^* )$. Homogeneity: $f(c times p^* ) = c times f(p^* ) = c times q^* (t)$. | Additivity: $f(p_1^* + p_2^* ) = f(p_1^* )+f(p_2^* ) = q_1^* (t)+q_2^* (t)$. | . | . . . . Rainfall-Runoff Models . The Rational Method . The rational method postulates a simple proportionality between peak discharge, $q_{pk}$, and rainfall intensity, $p^*$: . qpk=ÎµRâ‹…CRâ‹…ADâ‹…pâˆ—q_{pk} = varepsilon_R cdot C_R cdot A_D cdot p^*qpkâ€‹=ÎµRâ€‹â‹…CRâ€‹â‹…ADâ€‹â‹…pâˆ— . $q_{pk}$: peak discharge (m$^3$/s) | $ varepsilon_R=0.278$: unit-conversion factor | $C_R$: dimensionless runoff coefficient | $A_D$: drainage area (km$^2$) | $p^*$: rainfall intensity (mm/h) | . Obviously the results obtained with the method are highly sensitive to the value chosen for CR; values range from 0.05 for gently sloping lawns up to 0.95 for highly urbanized areas of roofs and pavement. The rational method is widely used in urban drainage design, but Pilgrim and Cordery (1992) caution that there are typically few data available to guide the selection of CR, and that CR for a given watershed may vary widely from storm to storm due to differing antecedent conditions. . The Soil Conservation Service Curve-Number Method (SCS-CN) . Also called NRCS curve number procedure. NRCS = Natural Resources Conservation Service - USDA . Qâˆ—=Pâˆ—=(Pâˆ’SI)2Pâˆ’SI+SmaxQ^* = P^* = frac{ left( P-S_{I} right)^2}{P-S_I+S_{max}}Qâˆ—=Pâˆ—=Pâˆ’SIâ€‹+Smaxâ€‹(Pâˆ’SIâ€‹)2â€‹ . The initial abstraction $S_I$ is usually approximated as $0.2 cdot S_{max}$, therefore: . Qâˆ—=Pâˆ—=(Pâˆ’0.2â‹…Smax)2P+0.8â‹…SmaxQ^* = P^* = frac{ left( P-0.2 cdot S_{max} right)^2}{P+0.8 cdot S_{max}}Qâˆ—=Pâˆ—=P+0.8â‹…Smaxâ€‹(Pâˆ’0.2â‹…Smaxâ€‹)2â€‹ . Smax=25.4(1000CNâˆ’10)S_{max} = 25.4 left( frac{1000}{CN}-10 right)Smaxâ€‹=25.4(CN1000â€‹âˆ’10) . The number 25.4 is a conversion factor from inches to millimeters. . (Dingman, 2015) . (Ward and Trimble, 2003) . The curve number (CN) is a function of the ability of soils to infiltrate water, land use, and the soil water conditions at the start of a rainfall event (antecedent soil water condition). To account for the infiltration characteristics of soils, the NRCS has divided soils into four hydrologic soil groups, which are defined as follows (NRCS, 1984): . Group A (low runoff potential): Soils with high infiltration rates even when thoroughly wetted. These consist chiefly of deep, well-drained sands and gravels. These soils have a high rate of water transmission (final infiltration rate greater than 0.3 in./h). | Group B: Soils with moderate infiltration rates when thoroughly wetted. These consist chiefly of soils that are moderately deep to deep, moderately well drained to well drained with moderately fine to moderately coarse textures. These soils have a moderate rate of water transmission (final infiltration rate 0.15 to 0.30 in./h). | Group C: Soils with slow infiltration rates when thoroughly wetted. These consist chiefly of soils with a layer that impedes downward movement of water or soils with moderately fine to fine texture. These soils have a slow rate of water transmission (final infiltration rate 0.05 to 0.15 in./h). | Group D (high runoff potential): Soils with very slow infiltration rates when thoroughly wetted. These consist chiefly of clay soils with a high swelling potential, soils with a permanent high water table, soils with a claypan or clay layer at or near the surface, and shallow soils over nearly impervious materials. These soils have a very slow rate of water transmission (final infiltration rate less than 0.05 in./h). | . There are also three categories for Antecedent Soil Moisture Condition (AMC): . AMC I: Dormant season antecedent soil moisture less than 0.5 in. Growing season antecedent soil moisture less than 1.4 in. | AMC II: Dormant season antecedent soil moisture between 0.5 and 1.1 in. Growing season antecedent soil moisture between 1.4 and 2.1 in. | AMC III: Dormant season antecedent soil moisture greater than 1.1 in. Growing season antecedent soil moisture greater than 2.1 in. | . See the table below to find curve numbers for AMC II: (Ward and Trimble, 2003) . References . Dingman, S.L., 2015. Physical Hydrology: Third Edition. Waveland Press. | Ward, A.D., Trimble, S.W., 2003. Environmental Hydrology, Second Edition. CRC Press. |",
            "url": "https://yairmau.github.io/website/markdown/2020/02/05/unit-hydrograph-lecture.html",
            "relUrl": "/markdown/2020/02/05/unit-hydrograph-lecture.html",
            "date": " â€¢ Feb 5, 2020"
        }
        
    
  
    
        ,"post30": {
            "title": "Streamflow - lecture",
            "content": "Sources . Sources used: . (Dingman, 2015) (Ward and Trimble, 2003) . Watershed - ××’×Ÿ ×”×™×§×•×•×ª . (Dingman, 2015) . . (Dingman, 2015) . Watershed response: . The volume of water appearing in the apparent response hydrograph for a given event is usually only a fraction (often a very small fraction) of the total input. The remainder of the water input ultimately leaves the watershed as: (1) evapotranspiration; (2) streamflow that occurs so long after the event that it cannot be associated with that event; or (3) ground-water outflow from the watershed. | The water identified as the response to a given event may originate on only a fraction of the watershed; this fraction is called the contributing area. | The extent of the contributing area may vary from event to event and during an event. | At least some of the water identified as the response to a given event may be â€œold waterâ€ that entered the watershed in a previous event. | . Base flow separation . (Ward and Trimble, 2003) . Base flow . Base flow is the portion of streamflow that is presumed to have entered the watershed in previous events and to be derived from persistent, slowly varying sources. (Ground water is usually assumed to be the main, if not the only, such source.) . Event flow . Event flow (also called direct runoff, storm runoff, quick flow, or storm flow) is considered to be the direct response to a given water-input event. . Total flow . Total flow rate at any instant $q(t)$ is the sum of event-flow rate $q^*(t)$ and base-flow rate $q_{BF}$(t): . q(t)=qâˆ—(t)+qBF(t)q(t) = q^*(t) + q_{BF}(t)q(t)=qâˆ—(t)+qBFâ€‹(t) . Attention! . Graphical flow separation techniques are heuristic and have no direct scientific basis. . Urbana, IL . . Hyetograph, Hydrograph . . Notation . . Base flow separation . . Effective precipitation = effective discharge . Pâˆ—=Qâˆ—P^* = Q^*Pâˆ—=Qâˆ— . . Time lags . . It is commonly assumed that $T_{LPC} simeq 0.60 cdot T_c$, where $T_c$ is the time of concentration, i.e., the time it takes water to travel from the hydraulically most distant part of the contributing area to the outlet. . The centroid is a weighted-average time, each time instant is multiplied by the amount of flow in that instant. . Time of precipitation centroid: . tpc=âˆ‘i=1npiâˆ—â‹…tiPâˆ—t_{pc} = frac{ displaystyle sum_{i=1}^n p_i^* cdot t_i}{P^*}tpcâ€‹=Pâˆ—i=1âˆ‘nâ€‹piâˆ—â€‹â‹…tiâ€‹â€‹ . Time of streamflow centroid: . tqc=âˆ‘i=1nqiâˆ—â‹…tiQâˆ—t_{qc} = frac{ displaystyle sum_{i=1}^n q_i^* cdot t_i}{Q^*}tqcâ€‹=Qâˆ—i=1âˆ‘nâ€‹qiâˆ—â€‹â‹…tiâ€‹â€‹ . References . Dingman, S.L., 2015. Physical Hydrology: Third Edition. Waveland Press. | Ward, A.D., Trimble, S.W., 2003. Environmental Hydrology, Second Edition. CRC Press. |",
            "url": "https://yairmau.github.io/website/markdown/2020/02/05/streamflow-lecture.html",
            "relUrl": "/markdown/2020/02/05/streamflow-lecture.html",
            "date": " â€¢ Feb 5, 2020"
        }
        
    
  
    
        ,"post31": {
            "title": "Unit Hydrograph - lecture",
            "content": "Linear reservoir model . . . . . Rainfall-Runoff Models . The Rational Method . The rational method postulates a simple proportionality between peak discharge, $q_{pk}$, and rainfall intensity, $p^*$: . $$ q_{pk} = varepsilon_R cdot C_R cdot A_D cdot p^* $$ $q_{pk}$: peak discharge (m$^3$/s) | $ varepsilon_R=0.278$: unit-conversion factor | $C_R$: dimensionless runoff coefficient | $A_D$: drainage area (km$^2$) | $p^*$: rainfall intensity (mm/h) | . Obviously the results obtained with the method are highly sensitive to the value chosen for CR; values range from 0.05 for gently sloping lawns up to 0.95 for highly urbanized areas of roofs and pavement. The rational method is widely used in urban drainage design, but Pilgrim and Cordery (1992) caution that there are typically few data available to guide the selection of CR, and that CR for a given watershed may vary widely from storm to storm due to differing antecedent conditions. . The Soil Conservation Service Curve-Number Method (SCS-CN) . Also called NRCS curve number procedure. NRCS = Natural Resources Conservation Service - USDA . $$ Q^* = P^* = frac{ left( P-S_{I} right)^2}{P-S_I+S_{max}} $$The initial abstraction $S_I$ is usually approximated as $0.2 cdot S_{max}$, therefore: . $$ Q^* = P^* = frac{ left( P-0.2 cdot S_{max} right)^2}{P+0.8 cdot S_{max}} $$$$ S_{max} = 25.4 left( frac{1000}{CN}-10 right) $$The number 25.4 is a conversion factor from inches to millimeters. . . . The curve number (CN) is a function of the ability of soils to infiltrate water, land use, and the soil water conditions at the start of a rainfall event (antecedent soil water condition). To account for the infiltration character- istics of soils, the NRCS has divided soils into four hydrologic soil groups, which are defined as follows (NRCS, 1984): . Group A (low runoff potential): Soils with high infiltration rates even when thoroughly wetted. These consist chiefly of deep, well-drained sands and gravels. These soils have a high rate of water transmission (final infiltration rate greater than 0.3 in./h). | Group B: Soils with moderate infiltration rates when thoroughly wetted. These consist chiefly of soils that are moderately deep to deep, moderately well drained to well drained with moderately fine to moderately coarse textures. These soils have a moderate rate of water transmission (final infil- tration rate 0.15 to 0.30 in./h). | Group C: Soils with slow infiltration rates when thoroughly wetted. These consist chiefly of soils with a layer that impedes downward movement of water or soils with moderately fine to fine texture. These soils have a slow rate of water transmission (final infiltration rate 0.05 to 0.15 in./h). | Group D (high runoff potential): Soils with very slow infiltration rates when thoroughly wetted. These consist chiefly of clay soils with a high swelling potential, soils with a permanent high water table, soils with a claypan or clay layer at or near the surface, and shallow soils over nearly impervious materials. These soils have a very slow rate of water transmission (final infiltration rate less than 0.05 in./h). | . There are also three categories for Antecedent Soil Moisture Condition (AMC): . AMC I: Dormant season antecedent soil moisture less than 0.5 in. Growing season antecedent soil moisture less than 1.4 in. | AMC II: Dormant season antecedent soil moisture between 0.5 and 1.1 in. Growing season anteced- ent soil moisture between 1.4 and 2.1 in. | AMC III: Dormant season antecedent soil mois- ture greater than 1.1 in. Growing season anteced- ent soil moisture greater than 2.1 in. | . See the table below to find curve numbers for AMC II: . P=21 ratio = 4.17e4/2.61e5 CN=86 Smax = 25.4 * (1000/CN - 10) Pmin = 0.2 * Smax Qstar = 0.0 if P &gt; Pmin: Qstar = (P - 0.2*Smax)**2 / (P+0.8*Smax) Qstar/P . 0.14270006393832066 . ratio . 0.15977011494252874 . Qstar / P . 0.9148811393863234 . %matplotlib notebook import numpy as np import matplotlib.pyplot as plt def Qstar_f(pe, CN): # Smax = 25.4*(1000/CN - 10) Smax = (1000/CN - 10) # Smax = (1000/CN - 10) / 25.4 Qstar = (pe - 0.2*Smax)**2 / (pe+0.8*Smax) return Qstar pe = np.linspace(0,8,101) # plt.plot(pe, Qstar_f(pe, 35)) plt.plot(pe, Qstar_f(pe, 50)) # plt.plot(pe, Qstar_f(pe, 85)) . [&lt;matplotlib.lines.Line2D at 0x7fd8b0e66610&gt;] . 8*25.4 . 203.2 .",
            "url": "https://yairmau.github.io/website/jupyter/2020/02/05/unit-hydrograph-lecture.html",
            "relUrl": "/jupyter/2020/02/05/unit-hydrograph-lecture.html",
            "date": " â€¢ Feb 5, 2020"
        }
        
    
  
    
        ,"post32": {
            "title": "Streamflow - lecture",
            "content": "Watershed - &#1488;&#1490;&#1503; &#1492;&#1497;&#1511;&#1493;&#1493;&#1514; . . Watershed response: . The volume of water appearing in the appar- ent response hydrograph for a given event is usually only a fraction (often a very small frac- tion) of the total input. The remainder of the water input ultimately leaves the watershed as: (1) evapotranspiration; (2) streamflow that oc- curs so long after the event that it cannot be associated with that event; or (3) ground-water outflow from the watershed. | The water identified as the response to a given event may originate on only a fraction of the watershed; this fraction is called the contrib- uting area. | The extent of the contributing area may vary from event to event and during an event. | At least some of the water identified as the re- sponse to a given event may be â€œold waterâ€ that entered the watershed in a previous event. | . base flow separation . Base flow . Base flow is the portion of streamflow that is presumed to have entered the watershed in previous events and to be derived from persistent, slowly varying sources. (Ground water is usually assumed to be the main, if not the only, such source.) . Event flow . Event flow (also called direct runoff, storm runoff, quick flow, or storm flow) is considered to be the direct response to a given water-input event. . Total flow . Total flow rate at any instant $q(t)$ is the sum of event-flow rate $q^*(t)$ and base-flow rate $q_{BF}$(t): . $$ q(t) = q^*(t) + q_{BF}(t) $$Attention! . Graphical flow separation techniques are heuristic and have no direct scientific basis. . Urbana, IL . hyetograph, hydrograph . notation . base flow separation . effective precipitation = effective discharge . $$ P^* = Q^* $$ . time lags . . It is commonly assumed that $T_{LPC} simeq 0.60 cdot T_c$, where $T_c$ is the time of concentration, i.e., the time it takes water to travel from the hydraulically most distant part of the contributing area to the outlet. . The centroid is a weighted-average time, each time instant is multiplied by the amount of flow in that instant. . Time of precipitation centroid: . $$ t_{pc} = frac{ displaystyle sum_{i=1}^n p_i^* cdot t_i}{P^*} $$Time of streamflow centroid: . $$ t_{qc} = frac{ displaystyle sum_{i=1}^n q_i^* cdot t_i}{Q^*} $$",
            "url": "https://yairmau.github.io/website/jupyter/2020/02/05/streamflow-lecture.html",
            "relUrl": "/jupyter/2020/02/05/streamflow-lecture.html",
            "date": " â€¢ Feb 5, 2020"
        }
        
    
  
    
        ,"post33": {
            "title": "Streamflow - exercises",
            "content": "Import relevant packages . import pandas as pd import numpy as np import matplotlib.pyplot as plt import matplotlib.dates as mdates import seaborn as sns sns.set(style=&quot;ticks&quot;, font_scale=1.5) from ipywidgets import * # these will be useful when printing superscripts: m^2, m^3 squared = &quot; u00b2&quot; cubed = &quot; u00b3&quot; . . Import streamflow data from USGS&#39;s National Water Information System. We will be using data from Urbana, IL. The time resolution varies, it can be 15 minutes, it can be 5 minutes! . # Drainage area: 4.78 square miles data_file = &quot;USGS 03337100 BONEYARD CREEK AT LINCOLN AVE AT URBANA, IL.dat&quot; df_q_2020 = pd.read_csv(data_file, header=31, # no headers needed, we&#39;ll do that later delim_whitespace=True, # blank spaces separate between columns na_values=[&quot;Bkw&quot;] # substitute these values for missing (NaN) values ) df_q_2020.columns = [&#39;agency_cd&#39;, &#39;site_no&#39;,&#39;datetime&#39;,&#39;tz_cd&#39;,&#39;EDT&#39;,&#39;discharge&#39;,&#39;code&#39;] # rename df columns with headers columns df_q_2020[&#39;date_and_time&#39;] = df_q_2020[&#39;datetime&#39;] + &#39; &#39; + df_q_2020[&#39;tz_cd&#39;] # combine date+time into datetime df_q_2020[&#39;date_and_time&#39;] = pd.to_datetime(df_q_2020[&#39;date_and_time&#39;]) # interpret datetime df_q_2020 = df_q_2020.set_index(&#39;date_and_time&#39;) # make datetime the index df_q_2020[&#39;discharge&#39;] = df_q_2020[&#39;discharge&#39;].astype(float) df_q_2020[&#39;discharge&#39;] = df_q_2020[&#39;discharge&#39;] * 0.0283168 # convert cubic feet per second to m3/s fig, ax = plt.subplots(figsize=(10,7)) ax.plot(df_q_2020[&#39;discharge&#39;], &#39;-o&#39;) plt.gcf().autofmt_xdate() ax.set(xlabel=&quot;date&quot;, ylabel=r&quot;discharge (m$^3$/5min)&quot;); . . Import sub-hourly (5-min) rainfall data from NOAA&#39;s Climate Reference Network Data website. . data_file = &quot;Champaign - IL.txt&quot; df_p_2020 = pd.read_csv(data_file, header=None, # no headers needed, we&#39;ll do that later delim_whitespace=True, # blank spaces separate between columns na_values=[&quot;-99.000&quot;, &quot;-9999.0&quot;] # substitute these values for missing (NaN) values ) headers = pd.read_csv(&quot;HEADERS_sub_hourly.txt&quot;, # load headers file header=1, # skip the first [0] line delim_whitespace=True ) df_p_2020.columns = headers.columns # rename df columns with headers columns # LST = local standard time df_p_2020[&quot;LST_TIME&quot;] = [f&quot;{x:04d}&quot; for x in df_p_2020[&quot;LST_TIME&quot;]] # time needs padding of zeros, then convert to string df_p_2020[&#39;LST_DATE&#39;] = df_p_2020[&#39;LST_DATE&#39;].astype(str) # convert date into string df_p_2020[&#39;datetime&#39;] = df_p_2020[&#39;LST_DATE&#39;] + &#39; &#39; + df_p_2020[&#39;LST_TIME&#39;] # combine date+time into datetime df_p_2020[&#39;datetime&#39;] = pd.to_datetime(df_p_2020[&#39;datetime&#39;]) # interpret datetime df_p_2020 = df_p_2020.set_index(&#39;datetime&#39;) # make datetime the index . . Plot rainfall and streamflow. Does this makes sense? . fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,7)) fig.subplots_adjust(hspace=0.05) start = &quot;2020-10-18&quot; end = &quot;2020-10-25&quot; ax1.plot(df_p_2020[start:end][&#39;PRECIPITATION&#39;]) ax2.plot(df_q_2020[start:end][&#39;discharge&#39;], color=&quot;tab:blue&quot;, lw=2) ax1.set(xticks=[], ylabel=r&quot;precipitation (mm)&quot;) ax2.set(xlabel=&quot;date&quot;, ylabel=r&quot;discharge (m$^3$/s)&quot;) plt.gcf().autofmt_xdate() # makes slated dates . . Define smaller dataframes for $p(t)$ and $q(t)$, between the dates: . start = &quot;2020-10-20 14:00:00&quot; end = &quot;2020-10-21 04:00:00&quot; . Don&#39;t forget to convert the units to SI! . Calculate total rainfall $P$ and total discharge $Q$, in m$^3$. . # Drainage area: 4.78 square miles area = 4.78 / 0.00000038610 # squared miles to squared meters start = &quot;2020-10-20 14:00:00&quot; end = &quot;2020-10-21 04:00:00&quot; # time resolution for p is 5 minutes, for q is 15 minutes delta_t_precipitation = 5 * 60 # in seconds delta_t_discharge = 5 * 60 # in seconds df_p = df_p_2020.loc[start:end, &#39;PRECIPITATION&#39;].to_frame() df_q = df_q_2020.loc[start:end][&#39;discharge&#39;].to_frame() # convert mm to m3. # a) it is understood that 1 mm = 1 L/m2 = 1e-3 m3/m2. # b) divide by 1000 to convert mm to m3/m2 # c) multiply by area (m2) to obtain total volume (m3) in whole watershed df_p[&#39;precipitation&#39;] = df_p[&#39;PRECIPITATION&#39;].values * area / 1000 # mm to m3 in the whole watershed # precipitation depth was originally given in mm in 5-min windows # divide by delta_t_precipitation = 5*60 to obtain finally m3/s df_p[&#39;precipitation&#39;] = df_p[&#39;precipitation&#39;] / delta_t_precipitation # Total volumes (m3) = sum of all bars times bar width (delta_t) P = df_p[&#39;precipitation&#39;].sum() * delta_t_precipitation Q = df_q[&#39;discharge&#39;].sum() * delta_t_discharge print(f&quot;total precipitation during event: nP = {P:.1e} m{cubed}&quot;) print(f&quot;total streamflow during event: nQ = {Q:.1e} m{cubed}&quot;) print(f&quot;Streamflow is {Q/P:.0%} of precipitation&quot;) . . total precipitation during event: P = 2.6e+05 mÂ³ total streamflow during event: Q = 5.2e+04 mÂ³ Streamflow is 20% of precipitation . Make another graph of $p(t)$ and $q(t)$, now with SI units. . fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,7)) fig.subplots_adjust(hspace=0.05) start = &quot;2020-10-18&quot; end = &quot;2020-10-25&quot; ax1.plot(df_p[&#39;precipitation&#39;]) ax2.plot(df_q[&#39;discharge&#39;], color=&quot;tab:blue&quot;, lw=2) ax1.set(xticks=[], ylabel=r&quot;precipitation (m$^3$/s)&quot;, title=&quot;Precipitation and discharge, Boneyard Creek at Urbana, IL n 20-21 October 2020, 5-minute data&quot;) ax2.set(xlabel=&quot;date&quot;, ylabel=r&quot;discharge (m$^3$/s)&quot;) plt.gcf().autofmt_xdate() # makes slated dates . . It&#39;s time for base flow separation! Convert q(t) into q*(t) . from matplotlib.dates import HourLocator, DateFormatter import matplotlib.dates as mdates import matplotlib.ticker as ticker fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,7)) fig.subplots_adjust(wspace=0.05) ax1.plot(df_q[&#39;discharge&#39;], color=&quot;black&quot;, lw=2) # choose 2 by trial and error point1 = pd.to_datetime(&quot;2020-10-20 16:40:00&quot;) point2 = pd.to_datetime(&quot;2020-10-21 00:00:00&quot;) two_points = df_q.loc[[point1, point2], &#39;discharge&#39;] ax1.plot(two_points, &#39;o&#39;, color=&quot;tab:red&quot;) # make new dataframe with the two chosen points df_linear = pd.DataFrame(data=two_points, index=two_points.index) # produce more points every 5 minutes by linear interpolation df_linear = (df_linear.resample(&quot;5min&quot;) # resample .interpolate(method=&#39;time&#39;) # interpolate by time (linear) ) ax1.plot(df_linear, color=&quot;tab:blue&quot;) # shade blue the region between the two curves df_between_2_points = df_q.loc[df_linear.index] ax1.fill_between(df_between_2_points.index, df_between_2_points[&#39;discharge&#39;], y2=df_linear[&#39;discharge&#39;], color=&quot;tab:blue&quot;, alpha=0.3) # calculate qstar = q - q_baseflow qstar = df_q.loc[df_linear.index, &#39;discharge&#39;] - df_linear[&#39;discharge&#39;] Qstar = qstar.sum() * delta_t_discharge ax2.plot(qstar, color=&quot;black&quot;, lw=2) ax2.fill_between(qstar.index, qstar, y2=0.0, color=&quot;tab:blue&quot;, alpha=0.3) ax1.set(xlim=[df_q.index[0], df_q.index[-1]], ylabel=r&quot;discharge (m$^3$/s)&quot;, ylim=[0, 5.5], yticks=[0,1,2,3,4], title=&quot;total discharge, q(t)&quot;) ax2.set(yticks=[], ylim=[0, 5.5], xlim=[df_q.index[0], df_q.index[-1]], title=&quot;effective discharge, q*(t)&quot; ) plt.gcf().autofmt_xdate() # makes slated dates . . We can calculate p* now, using . $$ P^* = Q^* $$One of the simplest methods is to multiply $p(t)$ by a fixed constant (&lt;1) to obtain $p^*$, so that the equation above holds true. . ratio = Qstar/ P print(f&quot;Qstar / P = {ratio:.2f}&quot;) print(f&quot;This means that {Qstar/P:.0%} of total precipitation became stormflow&quot;) print(f&quot;We can find Pstar by calculating Pstar = {ratio:.2f} * P&quot;) pstar = df_p[&#39;precipitation&#39;] * ratio # m3/s Pstar = pstar.sum() * delta_t_precipitation # m3 . . Qstar / P = 0.16 This means that 16% of total precipitation became stormflow We can find Pstar by calculating Pstar = 0.16 * P . Calculate now the centroid ($t_pc$) for effective precipitation p and centroid ($t_{qc}$) of effective discharge q. Calculate also the time of peak discharge ($t_{pk}$). Then, calculate the centroid lag ($T_{LC}$), the centroid lag-to-peak ($T_{LPC}$), and the time of concentration ($T_c$). Use the equations below: . $T_{LPC} simeq 0.60 cdot T_c$ . Time of precipitation centroid: . $$ t_{pc} = frac{ displaystyle sum_{i=1}^n p_i^* cdot t_i}{P^*} $$Time of streamflow centroid: . $$ t_{qc} = frac{ displaystyle sum_{i=1}^n q_i^* cdot t_i}{Q^*} $$Centroid lag: . $$ T_{LC} = t_{qc} - t_{pc} $$Centroid lag-to-peak: $$ T_{LPC} = t_{pk} - t_{pc} $$ . Time of concentration: $$ T_{LPC} simeq 0.60 cdot T_c $$ . # pstar centroid # time of the first (nonzero) rainfall data point t0 = pstar[pstar != 0.0].index[0] # time of the last (nonzero) rainfall data point tf = pstar[pstar != 0.0].index[-1] # duration of the rainfall event, in minutes td = (tf-t0) / pd.Timedelta(&#39;1 min&#39;) # make time array, add 2.5 minutes (half of dt) time = np.arange(0, td+1, 5) + 2.5 # create pi array, only with relevant data (during rainfall duration) pi = pstar.loc[(pstar.index &gt;= t0) &amp; (pstar.index &lt;= tf)] # time of precipitation centroid t_pc = (pi * time).sum() / pi.sum() # add initial time t_pc = t0 + pd.Timedelta(minutes=t_pc) t_pc # qstar centroid # time of the first (nonzero) discharge data point t0 = qstar[qstar != 0.0].index[0] # time of the last (nonzero) discharge data point tf = qstar[pstar != 0.0].index[-1] # duration of the discharge event, in minutes td = (tf-t0) / pd.Timedelta(&#39;1 min&#39;) # make time array, add 2.5 minutes (half of dt) time = np.arange(0, td+1, 5) + 2.5 # create qi array, only with relevant data (during discharge duration) qi = qstar.loc[(qstar.index &gt;= t0) &amp; (qstar.index &lt;= tf)] # time of discharge centroid t_qc = (qi * time).sum() / qi.sum() # add initial time t_qc = t0 + pd.Timedelta(minutes=t_qc) t_qc # time of peak discharge max_discharge = qstar.max() t_pk = qstar[qstar == max_discharge].index[0] # centroid lag T_LC = t_qc - t_pc # centroid lag-to-peak T_LPC = t_pk - t_pc # time of concentration T_c = T_LPC / 0.60 print(f&quot;T_LC = {T_LC}&quot;) print(f&quot;T_LPC = {T_LPC}&quot;) print(f&quot;T_c = {T_c}&quot;) . . T_LC = 0 days 00:53:03.186594 T_LPC = 0 days 01:22:59.857820 T_c = 0 days 02:18:19.763033333 .",
            "url": "https://yairmau.github.io/website/jupyter/2020/02/05/streamflow-exercises.html",
            "relUrl": "/jupyter/2020/02/05/streamflow-exercises.html",
            "date": " â€¢ Feb 5, 2020"
        }
        
    
  
    
        ,"post34": {
            "title": "Streamflow - code",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt import matplotlib.dates as mdates import seaborn as sns sns.set(style=&quot;ticks&quot;, font_scale=1.5) from ipywidgets import * . def linear_reservoir_response(t, p, q0, T): return p + (q0 - p) * np.exp(-t/T) def hydrograph_response(t, t0, p, T, Tp): response = np.zeros_like(t) # time for response starts at t0 index_t0 = np.where(t == t0)[0][0] # we shift the time, because linear_reservoir_response assumes t starts at zero t_response = t[index_t0:] - t0 # calculate rise part rise = linear_reservoir_response(t_response, p, 0, T) # calculate fall only if rainfall events ends before time array if Tp &lt; t[-1]: # find index of Tp index_Tp = np.where(t_response == Tp)[0][0] # calculate initial condition of fall q0 = rise[index_Tp] # compute fall fall = linear_reservoir_response(t_response, 0, q0, T) # we save fall values in the indices of rise after Tp rise[index_Tp:] = fall[:len(t_response)-index_Tp] # save full response (stored on rise array) on response array, after t0=beginning of rain response[index_t0:] = rise return response . %matplotlib notebook fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,7)) t_bars = [0] p_bars = [1] ax1.bar(t_bars, p_bars, width=1, align=&#39;edge&#39;, linewidth=0, alpha=0.5) t = np.arange(0, 10, 0.01) T_watershed = 1.0 Tp = 1.0 response = np.zeros_like(t) for i, prec in enumerate(p_bars): h = hydrograph_response(t, t_bars[i], p_bars[i], T_watershed, 1) response = response + h # ax2.plot(t, h, color=&quot;black&quot;, alpha=0.3, ls=&quot;--&quot;, zorder=2) ax2.plot(t, response, color=&quot;tab:red&quot;, lw=2, zorder=1) ax1.set(ylabel=r&quot;water input (L$^3$ T$^{-1}$)&quot;, xlim=[-0.2, t[-1]], ylim=[0, 1.1], yticks=[0,0.5,1]) ax2.set(xlabel=&quot;time (h)&quot;, ylabel=&quot;stream response (L$^3$ T$^{-1}$)&quot;, xlim=[-0.2, t[-1]], ylim=[0, 1.1], yticks=[0,0.5,1]) ax1.text(0.99, 0.97, &quot;hyetograph&quot;, transform=ax1.transAxes, ha=&#39;right&#39;, va=&#39;top&#39;, fontsize=20) ax2.text(0.99, 0.97, &quot;hydrograph&quot;, transform=ax2.transAxes, ha=&#39;right&#39;, va=&#39;top&#39;, fontsize=20) height = response.max() # ax2.annotate(&quot;&quot;, # xy=(1, height*1.1), #xycoords=&#39;data&#39;, # xytext=(2, height*1.1),# textcoords=&#39;data&#39;, # size=6, # arrowprops=dict(arrowstyle=&quot;|-|&quot;, # shrinkA=0, shrinkB=0, # connectionstyle=&quot;arc3&quot;, # color=&#39;black&#39;), # ) # ax2.text(1.5, height*1.2, &quot;T* = 1 h&quot;, ha=&quot;center&quot;, fontsize=16) # ax2.annotate(&quot;&quot;, # xy=(2, height), #xycoords=&#39;data&#39;, # xytext=(2, height*(1.0-np.exp(-1))),# textcoords=&#39;data&#39;, # size=6, # arrowprops=dict(arrowstyle=&quot;|-|&quot;, # shrinkA=0, shrinkB=0, # connectionstyle=&quot;arc3&quot;, # color=&#39;black&#39;), # ) # ax2.text(2.1, height*(1.0-np.exp(-1)/2), r&quot;decay of 37% = exp($-1$)&quot;, va=&quot;center&quot;, fontsize=16) ax1.set_title(&quot;response of linear watershed, T* = 1 h&quot;) plt.savefig(&quot;hydrology_figures/hyetograph_hydrograph1.png&quot;) . fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,7)) t_bars = [0] p_bars = [1] ax1.bar(t_bars, p_bars, width=1, align=&#39;edge&#39;, linewidth=0, alpha=0.5) t = np.arange(0, 10, 0.01) T_watershed = 2.0 Tp = 1.0 response = np.zeros_like(t) for i, prec in enumerate(p_bars): h = hydrograph_response(t, t_bars[i], p_bars[i], T_watershed, 1) response = response + h # ax2.plot(t, h, color=&quot;black&quot;, alpha=0.3, ls=&quot;--&quot;, zorder=2) ax2.plot(t, response, color=&quot;tab:red&quot;, lw=2, zorder=1) ax1.set(ylabel=r&quot;water input (L$^3$ T$^{-1}$)&quot;, xlim=[-0.2, t[-1]], ylim=[0, 1.1], yticks=[0,0.5,1]) ax2.set(xlabel=&quot;time (h)&quot;, ylabel=&quot;stream response (L$^3$ T$^{-1}$)&quot;, xlim=[-0.2, t[-1]], ylim=[0, 1.1], yticks=[0,0.5,1]) ax1.text(0.99, 0.97, &quot;hyetograph&quot;, transform=ax1.transAxes, ha=&#39;right&#39;, va=&#39;top&#39;, fontsize=20) ax2.text(0.99, 0.97, &quot;hydrograph&quot;, transform=ax2.transAxes, ha=&#39;right&#39;, va=&#39;top&#39;, fontsize=20) height = response.max() ax2.annotate(&quot;&quot;, xy=(1, height*1.15), #xycoords=&#39;data&#39;, xytext=(3, height*1.15),# textcoords=&#39;data&#39;, size=6, arrowprops=dict(arrowstyle=&quot;|-|&quot;, shrinkA=0, shrinkB=0, connectionstyle=&quot;arc3&quot;, color=&#39;black&#39;), ) ax2.text(2, height*1.2, &quot;T* = 2 h&quot;, ha=&quot;center&quot;, fontsize=16) ax2.annotate(&quot;&quot;, xy=(3, height), #xycoords=&#39;data&#39;, xytext=(3, height*(1.0-np.exp(-1))),# textcoords=&#39;data&#39;, size=6, arrowprops=dict(arrowstyle=&quot;|-|&quot;, shrinkA=0, shrinkB=0, connectionstyle=&quot;arc3&quot;, color=&#39;black&#39;), ) ax2.text(3.1, height*(1.0-np.exp(-1)/2), r&quot;decay of 37% = exp($-1$)&quot;, va=&quot;center&quot;, fontsize=16) ax1.set_title(&quot;response of linear watershed, T* = 2 h&quot;) plt.savefig(&quot;hydrology_figures/hyetograph_hydrograph2.png&quot;) . fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,7)) t_bars = [0, 1, 2, 4] p_bars = [3, 1, 2, 1] ax1.bar(t_bars, p_bars, width=1, align=&#39;edge&#39;, linewidth=0, alpha=0.5) t = np.arange(0, 10, 0.01) T_watershed = 1.0 Tp = 1.0 response = np.zeros_like(t) for i, prec in enumerate(p_bars): h = hydrograph_response(t, t_bars[i], p_bars[i], T_watershed, 1) response = response + h ax2.plot(t, h, color=&quot;black&quot;, alpha=0.3, ls=&quot;-&quot;, zorder=2) ax2.plot(t, response, color=&quot;tab:red&quot;, lw=2, zorder=1) ax1.set(ylabel=r&quot;water input (L$^3$ T$^{-1}$)&quot;, xlim=[-0.2, t[-1]], ylim=[0, 3.1], yticks=[0,1,2,3]) ax2.set(xlabel=&quot;time (h)&quot;, ylabel=&quot;stream response (L$^3$ T$^{-1}$)&quot;, xlim=[-0.2, t[-1]], ylim=[0, 3.1], yticks=[0,1,2,3]) ax1.text(0.99, 0.97, &quot;hyetograph&quot;, transform=ax1.transAxes, ha=&#39;right&#39;, va=&#39;top&#39;, fontsize=20) ax2.text(0.99, 0.97, &quot;hydrograph&quot;, transform=ax2.transAxes, ha=&#39;right&#39;, va=&#39;top&#39;, fontsize=20) ax1.set_title(&quot;response of linear watershed, T* = 1 h&quot;) plt.savefig(&quot;hydrology_figures/hyetograph_hydrograph3b.png&quot;) . %matplotlib widget a = 1.0 b = 1.0 c = 1.0 x = np.linspace(0, 2 * np.pi) def parabola(a): return a*x**2 + b*x + c fig = plt.figure() ax = fig.add_subplot(1, 1, 1) line, = ax.plot(x, parabola(x)) def update_graph(a = 1.0): line.set_ydata(parabola(a)) fig.canvas.draw_idle() interact(update_graph); . %matplotlib widget t_bars = [0, 1, 2, 4] p_bars = [3, 1, 2, 1] t = np.arange(0, 10, 0.01) T_watershed = 1.0 Tp = 1.0 def resp(T_star): response = np.zeros_like(t) for i, prec in enumerate(p_bars): h = hydrograph_response(t, t_bars[i], p_bars[i], T_star, 1) response = response + h return response fig, ax = plt.subplots(figsize=(10,7)) ax.bar(t_bars, p_bars, width=1, align=&#39;edge&#39;, linewidth=0, alpha=0.5) line, = ax.plot(t, resp(1.0), color=&quot;tab:red&quot;, lw=2, zorder=10) ax.set(xlabel=&quot;time (h)&quot;, ylabel=&quot;stream response (L$^3$ T$^{-1}$)&quot;, xlim=[-0.2, t[-1]], ylim=[0, 3.1], yticks=[0,1,2,3], title=&quot;response of linear watershed, T* = 1 h&quot;) def update_graph(T_star = 1.0): line.set_ydata(resp(T_star)) fig.canvas.draw_idle() ax.set_title(f&quot;response of linear watershed, T* = {T_star:.2f} h&quot;) interact(update_graph, T_star=(0.01,5,0.01)); . $$ t_{pc} = frac{ sum_{i=1}^n P_i^* cdot t_i}{P^*} $$ fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,7)) t_bars = np.array([0, 1, 2, 3, 4]) p_bars = np.array([1, 3, 2.5, 1.5, 1.8]) ax1.bar(t_bars, p_bars, width=1, align=&#39;edge&#39;, linewidth=0, alpha=0.5) ti = t_bars + 0.5 t_pc = np.sum(p_bars*ti) / np.sum(p_bars) print(t_pc) ax1.plot([t_pc]*2, ax1.get_ylim(), color=&quot;black&quot;, ls=&quot;--&quot;) ax1.text(t_pc, 0.1, r&quot;$t_{pc}=$&quot;+f&quot;{t_pc:.2f}&quot;, fontsize=16) t = np.arange(0, 16, 0.01) T_watershed = 1.5 Tp = 1.0 response = np.zeros_like(t) for i, prec in enumerate(p_bars): h = hydrograph_response(t, t_bars[i], p_bars[i], T_watershed, 1) response = response + h ax2.plot(t, response, color=&quot;tab:red&quot;, lw=2, zorder=1) ax1.set(ylabel=r&quot;water input (L$^3$ T$^{-1}$)&quot;, xlim=[-0.2, t[-1]], ylim=[0, 3.1], xticks=[], yticks=[0,1,2,3]) ax2.set(xlabel=&quot;time (h)&quot;, ylabel=&quot;stream response (L$^3$ T$^{-1}$)&quot;, xlim=[-0.2, t[-1]], ylim=[0, 3.1], yticks=[0,1,2,3]) ti = t + 0.5 * (t[1] - t[0]) t_qc = np.sum(response*ti) / np.sum(response) ax2.plot([t_qc]*2, ax2.get_ylim(), color=&quot;black&quot;, ls=&quot;--&quot;) ax2.text(t_qc, 0.1, r&quot;$t_{qc}=$&quot;+f&quot;{t_qc:.2f}&quot;, fontsize=16) ax1.text(0.99, 0.97, &quot;hyetograph&quot;, transform=ax1.transAxes, ha=&#39;right&#39;, va=&#39;top&#39;, fontsize=20) ax2.text(0.99, 0.97, &quot;hydrograph&quot;, transform=ax2.transAxes, ha=&#39;right&#39;, va=&#39;top&#39;, fontsize=20) ax1.set_title(f&quot;response of linear watershed, T* = {T_watershed:.1f} h&quot;) # plt.savefig(&quot;hydrology_figures/hyetograph_hydrograph3b.png&quot;) . 2.510204081632653 . Text(0.5, 1.0, &#39;response of linear watershed, T* = 1.5 h&#39;) . np.sum(p_bars) . 9.8 . np.sum(response)*0.01 . 9.798227399091306 . import pandas as pd data_file = &quot;cincinnati1.dat&quot; df = pd.read_csv(data_file, header=31, # no headers needed, we&#39;ll do that later delim_whitespace=True, # blank spaces separate between columns na_values=[&quot;Bkw&quot;] # substitute these values for missing (NaN) values ) # headers = pd.read_csv(&quot;HEADERS_sub_hourly.txt&quot;, # load headers file # header=1, # skip the first [0] line # delim_whitespace=True # ) df.columns = [&#39;agency_cd&#39;, &#39;stie_no&#39;,&#39;datetime&#39;,&#39;tz_cd&#39;,&#39;EDT&#39;,&#39;discharge&#39;,&#39;code&#39;] # rename df columns with headers columns # # LST = local standard time # df[&quot;LST_TIME&quot;] = [f&quot;{x:04d}&quot; for x in df[&quot;LST_TIME&quot;]] # time needs padding of zeros, then convert to string # df[&#39;LST_DATE&#39;] = df[&#39;LST_DATE&#39;].astype(str) # convert date into string df[&#39;date_and_time&#39;] = df[&#39;datetime&#39;] + &#39; &#39; + df[&#39;tz_cd&#39;] # combine date+time into datetime df[&#39;date_and_time&#39;] = pd.to_datetime(df[&#39;date_and_time&#39;]) # interpret datetime df = df.set_index(&#39;date_and_time&#39;) # make datetime the index df[&#39;discharge&#39;] = df[&#39;discharge&#39;].astype(float) df[&#39;discharge&#39;] = df[&#39;discharge&#39;] * 0.0283168 # convert cubic feet to m3 . # https://waterdata.usgs.gov/nwis/inventory?agency_code=USGS&amp;site_no=03260015 %matplotlib notebook plt.plot(df[&#39;discharge&#39;]) two_points = df.loc[[pd.to_datetime(&quot;2021-03-18 02:00:00&quot;), pd.to_datetime(&quot;2021-03-18 22:00:00&quot;)]][&#39;discharge&#39;] plt.plot(two_points, &#39;o&#39;) new = pd.DataFrame(data=two_points, index=two_points.index) df_linear = (new.resample(&quot;15min&quot;) #resample hourly .interpolate(method=&#39;time&#39;) #interpolate by time ) # new.resample(&#39;H&#39;).interpolate(method=&#39;time&#39;) plt.plot(df_linear) qstar = df.loc[df_linear.index][&#39;discharge&#39;] - df_linear[&#39;discharge&#39;] plt.plot(qstar) plt.gcf().autofmt_xdate() # plt.xlim([&#39;2021-03-17 18:00:00&#39;, &#39;2021-03-19 04:00:00&#39;]) plt.xlim((pd.to_datetime(&quot;2021-03-17 18:00:00&quot;), pd.to_datetime(&quot;2021-03-19 04:00:00&quot;)) ) plt.ylim([0, 130]) . (0.0, 130.0) . from matplotlib.dates import HourLocator, DateFormatter import matplotlib.dates as mdates import matplotlib.ticker as ticker fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,7)) fig.subplots_adjust(wspace=0.05) ax1.plot(df[&#39;discharge&#39;], color=&quot;black&quot;, lw=2) two_points = df.loc[[pd.to_datetime(&quot;2021-03-18 02:00:00&quot;), pd.to_datetime(&quot;2021-03-18 22:00:00&quot;)]][&#39;discharge&#39;] ax1.plot(two_points, &#39;o&#39;, color=&quot;tab:red&quot;) new = pd.DataFrame(data=two_points, index=two_points.index) df_linear = (new.resample(&quot;15min&quot;) #resample hourly .interpolate(method=&#39;time&#39;) #interpolate by time ) ax1.plot(df_linear) df_between_2_points = df.loc[df_linear.index] ax1.fill_between(df_between_2_points.index, df_between_2_points[&#39;discharge&#39;], y2=df_linear[&#39;discharge&#39;], color=&quot;tab:blue&quot;, alpha=0.3) ax1.annotate(&quot;event flow&quot;, xy=(pd.to_datetime(&quot;2021-03-18 13:00:00&quot;), 1.4), #xycoords=&#39;data&#39;, xytext=(pd.to_datetime(&quot;2021-03-18 16:00:00&quot;), 2.2),# textcoords=&#39;data&#39;, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, color=&#39;black&#39;), ) ax1.annotate(&quot;base flow&quot;, xy=(pd.to_datetime(&quot;2021-03-18 17:00:00&quot;), 0.1), #xycoords=&#39;data&#39;, xytext=(pd.to_datetime(&quot;2021-03-18 17:00:00&quot;), 1.1),# textcoords=&#39;data&#39;, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, color=&#39;black&#39;), ) qstar = df.loc[df_linear.index][&#39;discharge&#39;] - df_linear[&#39;discharge&#39;] ax2.plot(qstar, color=&quot;black&quot;, lw=2) ax2.fill_between(qstar.index, qstar, y2=0.0, color=&quot;tab:blue&quot;, alpha=0.3) ax1.set(xlim=[pd.to_datetime(&quot;2021-03-17 18:00:00&quot;), pd.to_datetime(&quot;2021-03-19 04:00:00&quot;)], ylabel=r&quot;discharge (m$^3$/s)&quot;, ylim=[0, 4], yticks=[0,1,2,3,4]) ax2.set(yticks=[], ylim=[0, 4], xlim=[pd.to_datetime(&quot;2021-03-17 18:00:00&quot;), pd.to_datetime(&quot;2021-03-19 04:00:00&quot;)], ) ax1.text(0.5, 0.97, &quot;total flow rate = event + base n&quot; + r&quot;$q(t) = q !^* !(t) + q_{bf} ,(t)$&quot;, transform=ax1.transAxes, ha=&#39;center&#39;, va=&#39;top&#39;, fontsize=16) ax2.text(0.5, 0.97, r&quot;event flow rate, $q !^* !(t)$&quot;, transform=ax2.transAxes, ha=&#39;center&#39;, va=&#39;top&#39;, fontsize=16) ax1.xaxis.set_major_locator(HourLocator(byhour=[0, 8, 16], interval=1)) ax2.xaxis.set_major_locator(HourLocator(byhour=[0, 8, 16], interval=1)) plt.gcf().autofmt_xdate() # dirty trick to get common title between the two panels: # make a large invisible axes, give it a title ax0 = fig.add_subplot(111, frame_on=False) ax0.tick_params(labelcolor=&quot;none&quot;, bottom=False, left=False) ax0.set_title(&quot;Pleasant Run Creek at Oak St. near Ludlow, KY nMarch 2021, USGS 03260015 &quot;) . Text(0.5, 1.0, &#39;Pleasant Run Creek at Oak St. near Ludlow, KY nMarch 2021, USGS 03260015 &#39;) . Urbana, IL . data_file = &quot;USGS 03337100 BONEYARD CREEK AT LINCOLN AVE AT URBANA, IL.dat&quot; df_q = pd.read_csv(data_file, header=31, # no headers needed, we&#39;ll do that later delim_whitespace=True, # blank spaces separate between columns na_values=[&quot;Bkw&quot;] # substitute these values for missing (NaN) values ) df_q.columns = [&#39;agency_cd&#39;, &#39;site_no&#39;,&#39;datetime&#39;,&#39;tz_cd&#39;,&#39;EDT&#39;,&#39;discharge&#39;,&#39;code&#39;] # rename df columns with headers columns df_q[&#39;date_and_time&#39;] = df_q[&#39;datetime&#39;] + &#39; &#39; + df_q[&#39;tz_cd&#39;] # combine date+time into datetime df_q[&#39;date_and_time&#39;] = pd.to_datetime(df_q[&#39;date_and_time&#39;]) # interpret datetime df_q = df_q.set_index(&#39;date_and_time&#39;) # make datetime the index df_q[&#39;discharge&#39;] = df_q[&#39;discharge&#39;].astype(float) df_q[&#39;discharge&#39;] = df_q[&#39;discharge&#39;] * 0.0283168 # convert cubic feet to m3 . data_file = &quot;Champaign - IL.txt&quot; df_p = pd.read_csv(data_file, header=None, # no headers needed, we&#39;ll do that later delim_whitespace=True, # blank spaces separate between columns na_values=[&quot;-99.000&quot;, &quot;-9999.0&quot;] # substitute these values for missing (NaN) values ) headers = pd.read_csv(&quot;HEADERS_sub_hourly.txt&quot;, # load headers file header=1, # skip the first [0] line delim_whitespace=True ) df_p.columns = headers.columns # rename df columns with headers columns # LST = local standard time df_p[&quot;LST_TIME&quot;] = [f&quot;{x:04d}&quot; for x in df_p[&quot;LST_TIME&quot;]] # time needs padding of zeros, then convert to string df_p[&#39;LST_DATE&#39;] = df_p[&#39;LST_DATE&#39;].astype(str) # convert date into string df_p[&#39;datetime&#39;] = df_p[&#39;LST_DATE&#39;] + &#39; &#39; + df_p[&#39;LST_TIME&#39;] # combine date+time into datetime df_p[&#39;datetime&#39;] = pd.to_datetime(df_p[&#39;datetime&#39;]) # interpret datetime df_p = df_p.set_index(&#39;datetime&#39;) # make datetime the index # plt.plot(df_p[&#39;PRECIPITATION&#39;]) . %matplotlib notebook # Drainage area: 4.78 square miles area = 4.78 / 0.00000038610 # squared miles to squared meters start = &quot;2020-10-20 14:00:00&quot; end = &quot;2020-10-21 04:00:00&quot; df_p_small = df_p.loc[start:end].copy() df_q_small = df_q.loc[start:end].copy() df_p_small[&#39;PRECIPITATION&#39;] = df_p_small[&#39;PRECIPITATION&#39;].values * area / 1000 # mm to m3 in the whole watershed total_p = df_p_small[&#39;PRECIPITATION&#39;].sum() df_p_small[&#39;PRECIPITATION&#39;] = df_p_small[&#39;PRECIPITATION&#39;] / 60 / 5 # convert m3 per 5 min to m3/s fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,7)) fig.subplots_adjust(hspace=0.05) ax1.bar(df_p_small.index, df_p_small[&#39;PRECIPITATION&#39;], width=1/24/12) ax2.plot(df_q_small[&#39;discharge&#39;], color=&quot;tab:blue&quot;, lw=2) ax1.set_xlim([pd.to_datetime(&quot;2020-10-20 14:00:00&quot;), pd.to_datetime(&quot;2020-10-21 04:00:00&quot;)]) ax2.set_xlim([pd.to_datetime(&quot;2020-10-20 14:00:00&quot;), pd.to_datetime(&quot;2020-10-21 04:00:00&quot;)]) # total_p = df_p_small[&#39;PRECIPITATION&#39;].sum() total_q = df_q_small[&#39;discharge&#39;].sum() * 60*5 # dirty trick to get common title between the two panels: # make a large invisible axes, give it a title ax0 = fig.add_subplot(111, frame_on=False) ax0.tick_params(labelcolor=&quot;none&quot;, bottom=False, left=False) ax0.set_ylabel(r&quot;flow (m$^3$/s)&quot;) ax1.text(0.99, 0.97, r&quot;precipitation $p(t)$, hyetograph&quot; + &quot; n&quot; + f&quot;P = {total_p:.2e}&quot; + r&quot; m$^3$&quot;, transform=ax1.transAxes, ha=&#39;right&#39;, va=&#39;top&#39;, fontsize=20) ax2.text(0.99, 0.97, r&quot;discharge $q(t)$, hydrograph&quot; + &quot; n&quot; + f&quot;Q = {total_q:.2e}&quot; + r&quot; m$^3$&quot;, transform=ax2.transAxes, ha=&#39;right&#39;, va=&#39;top&#39;, fontsize=20) ax1.set_xticks([]) locator = mdates.AutoDateLocator(minticks=3, maxticks=7) formatter = mdates.ConciseDateFormatter(locator) ax2.xaxis.set_major_locator(locator) ax2.xaxis.set_major_formatter(formatter) ax2.set_ylim([0,5.5]) ax1.set_title(&quot;Precipitation and discharge, Boneyard Creek at Urbana, IL n 20-21 October 2020, 5-minute data&quot;) fig.savefig(&quot;hydrology_figures/urbana_pq.png&quot;) . total_q/ total_p . 0.19772617793646036 . from matplotlib.dates import HourLocator, DateFormatter import matplotlib.dates as mdates import matplotlib.ticker as ticker fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,7)) fig.subplots_adjust(wspace=0.05) ax1.plot(df_q_small[&#39;discharge&#39;], color=&quot;black&quot;, lw=2) point1 = pd.to_datetime(&quot;2020-10-20 16:40:00&quot;) point2 = pd.to_datetime(&quot;2020-10-21 00:00:00&quot;) # two_points = df_q_small.loc[[&quot;2020-10-20 16:40:00&quot;, &quot;2020-10-20 23:00:00&quot;]] two_points = df_q_small.loc[[point1, point2]][&#39;discharge&#39;] ax1.plot(two_points, &#39;o&#39;, color=&quot;tab:red&quot;) new = pd.DataFrame(data=two_points, index=two_points.index) df_linear = (new.resample(&quot;5min&quot;) #resample .interpolate(method=&#39;time&#39;) #interpolate by time ) ax1.plot(df_linear, color=&quot;tab:blue&quot;) df_between_2_points = df_q_small.loc[df_linear.index] ax1.fill_between(df_between_2_points.index, df_between_2_points[&#39;discharge&#39;], y2=df_linear[&#39;discharge&#39;], color=&quot;tab:blue&quot;, alpha=0.3) ax1.annotate(&quot;event flow&quot;, xy=(pd.to_datetime(&quot;2020-10-20 20:00:00&quot;), 1.0), #xycoords=&#39;data&#39;, xytext=(pd.to_datetime(&quot;2020-10-20 22:00:00&quot;), 2.2),# textcoords=&#39;data&#39;, size=16, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, color=&#39;black&#39;), ) ax1.annotate(&quot;base flow&quot;, xy=(pd.to_datetime(&quot;2020-10-21 00:00:00&quot;), 0.1), #xycoords=&#39;data&#39;, xytext=(pd.to_datetime(&quot;2020-10-21 0:00:00&quot;), 1.1),# textcoords=&#39;data&#39;, size=16, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, color=&#39;black&#39;), ) qstar = df_q_small.loc[df_linear.index][&#39;discharge&#39;] - df_linear[&#39;discharge&#39;] total_qstar = qstar.sum() * 60 * 5 ax2.plot(qstar, color=&quot;black&quot;, lw=2) ax2.fill_between(qstar.index, qstar, y2=0.0, color=&quot;tab:blue&quot;, alpha=0.3) ax1.set(xlim=[df_q_small.index[0], df_q_small.index[-1]], ylabel=r&quot;discharge (m$^3$/s)&quot;, ylim=[0, 5.5], yticks=[0,1,2,3,4]) ax2.set(yticks=[], ylim=[0, 5.5], xlim=[df_q_small.index[0], df_q_small.index[-1]], ) ax1.text(0.5, 0.97, &quot;total flow rate = event + base n&quot; + r&quot;$q(t) = q !^* !(t) + q_{bf} ,(t)$&quot;, transform=ax1.transAxes, ha=&#39;center&#39;, va=&#39;top&#39;, fontsize=16) ax2.text(0.5, 0.97, r&quot;event flow rate, $q !^* !(t)$&quot;, transform=ax2.transAxes, ha=&#39;center&#39;, va=&#39;top&#39;, fontsize=16) ax2.annotate(f&quot;Q* = {total_qstar:.0f}&quot; + r&quot; m$^3$&quot;, xy=(pd.to_datetime(&quot;2020-10-20 20:00:00&quot;), 1.0), #xycoords=&#39;data&#39;, xytext=(pd.to_datetime(&quot;2020-10-20 21:00:00&quot;), 3),# textcoords=&#39;data&#39;, size=16, arrowprops=dict(arrowstyle=&quot;-&gt;&quot;, color=&#39;black&#39;), ) locator = mdates.AutoDateLocator(minticks=3, maxticks=7) formatter = mdates.ConciseDateFormatter(locator) ax1.xaxis.set_major_locator(locator) ax1.xaxis.set_major_formatter(formatter) ax2.xaxis.set_major_locator(locator) ax2.xaxis.set_major_formatter(formatter) # dirty trick to get common title between the two panels: # make a large invisible axes, give it a title ax0 = fig.add_subplot(111, frame_on=False) ax0.tick_params(labelcolor=&quot;none&quot;, bottom=False, left=False) ax0.set_title(&quot;Boneyard Creek at Urbana, IL, USGS 03337100 n 20-21 October 2020, 5-minute data&quot;) fig.savefig(&quot;hydrology_figures/urbana_q_qstar.png&quot;) . %matplotlib notebook # Drainage area: 4.78 square miles # area = 4.78 / 0.00000038610 # squared miles to squared meters # start = &quot;2020-10-20 14:00:00&quot; # end = &quot;2020-10-21 04:00:00&quot; # df_p_small = df_p.loc[start:end].copy() # df_q_small = df_q.loc[start:end].copy() # df_p_small[&#39;PRECIPITATION&#39;] = df_p_small[&#39;PRECIPITATION&#39;].values * area / 1000 # mm to m3 in the whole watershed # total_p = df_p_small[&#39;PRECIPITATION&#39;].sum() # df_p_small[&#39;PRECIPITATION&#39;] = df_p_small[&#39;PRECIPITATION&#39;] / 60 / 5 # convert m3 per 5 min to m3/s fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,7)) fig.subplots_adjust(hspace=0.05) ratio = total_qstar/ total_p pstar = df_p_small[&#39;PRECIPITATION&#39;] * ratio total_pstar = pstar.sum() * 5 * 60 ax1.bar(pstar.index, pstar, width=1/24/12) ax2.plot(qstar, color=&quot;tab:blue&quot;, lw=2) print(total_pstar, total_qstar, total_qstar/total_pstar) ax1.set_xlim([pd.to_datetime(&quot;2020-10-20 14:00:00&quot;), pd.to_datetime(&quot;2020-10-21 04:00:00&quot;)]) ax2.set_xlim([pd.to_datetime(&quot;2020-10-20 14:00:00&quot;), pd.to_datetime(&quot;2020-10-21 04:00:00&quot;)]) # # total_p = df_p_small[&#39;PRECIPITATION&#39;].sum() # total_q = df_q_small[&#39;discharge&#39;].sum() * 60*5 # dirty trick to get common title between the two panels: # make a large invisible axes, give it a title ax0 = fig.add_subplot(111, frame_on=False) ax0.tick_params(labelcolor=&quot;none&quot;, bottom=False, left=False) ax0.set_ylabel(r&quot;flow (m$^3$/s)&quot;) ax1.text(0.99, 0.97, r&quot;effective precipitation $p !* !(t)$&quot; + &quot; n&quot; + f&quot;P* = {total_pstar:.2e}&quot; + r&quot; m$^3$&quot;, transform=ax1.transAxes, ha=&#39;right&#39;, va=&#39;top&#39;, fontsize=20) ax2.text(0.99, 0.97, r&quot;effective discharge $q !* !(t)$&quot; + &quot; n&quot; + f&quot;Q* = {total_qstar:.2e}&quot; + r&quot; m$^3$&quot;, transform=ax2.transAxes, ha=&#39;right&#39;, va=&#39;top&#39;, fontsize=20) ax1.set_xticks([]) locator = mdates.AutoDateLocator(minticks=3, maxticks=7) formatter = mdates.ConciseDateFormatter(locator) ax2.xaxis.set_major_locator(locator) ax2.xaxis.set_major_formatter(formatter) ax2.set_ylim([0,5.5]) ax1.set_title(&quot;Effective precipitation and discharge, Boneyard Creek at Urbana, IL n 20-21 October 2020, 5-minute data&quot;) fig.savefig(&quot;hydrology_figures/urbana_pstar_qstar.png&quot;) . 41692.08473760001 41692.0847376 0.9999999999999998 . t0 = pstar[pstar != 0.0].index[0] tf = pstar[pstar != 0.0].index[-1] td = (tf-t0) / pd.Timedelta(&#39;1 min&#39;) # time difference in minutes time = np.arange(0, td+1, 5) + 2.5 pi = pstar.loc[(pstar.index &gt;= t0) &amp; (pstar.index &lt;= tf)] pi = pi.values * 60 * 5 t_pc = (pi * time).sum() / pi.sum() t_pc = t0 + pd.Timedelta(minutes=t_pc) t_pc # qstar centroid t0 = qstar[qstar != 0.0].index[0] tf = qstar[pstar != 0.0].index[-1] td = (tf-t0) / pd.Timedelta(&#39;1 min&#39;) # time difference in minutes time = np.arange(0, td+1, 5) + 2.5 qi = qstar.loc[(qstar.index &gt;= t0) &amp; (qstar.index &lt;= tf)] qi = qi.values * 60 * 5 t_qc = (qi * time).sum() / qi.sum() t_qc = t0 + pd.Timedelta(minutes=t_qc) t_qc # time of peak discharge max_discharge = qstar.max() t_pk = qstar[qstar == max_discharge].index[0] . qstar . date_and_time 2020-10-20 16:40:00 0.000000 2020-10-20 16:45:00 0.092245 2020-10-20 16:50:00 0.302571 2020-10-20 16:55:00 1.100188 2020-10-20 17:00:00 1.982755 ... 2020-10-20 23:40:00 0.099945 2020-10-20 23:45:00 0.073543 2020-10-20 23:50:00 0.047141 2020-10-20 23:55:00 0.023571 2020-10-21 00:00:00 0.000000 Freq: 5T, Name: discharge, Length: 89, dtype: float64 . %matplotlib notebook # Drainage area: 4.78 square miles # area = 4.78 / 0.00000038610 # squared miles to squared meters # start = &quot;2020-10-20 14:00:00&quot; # end = &quot;2020-10-21 04:00:00&quot; # df_p_small = df_p.loc[start:end].copy() # df_q_small = df_q.loc[start:end].copy() # df_p_small[&#39;PRECIPITATION&#39;] = df_p_small[&#39;PRECIPITATION&#39;].values * area / 1000 # mm to m3 in the whole watershed # total_p = df_p_small[&#39;PRECIPITATION&#39;].sum() # df_p_small[&#39;PRECIPITATION&#39;] = df_p_small[&#39;PRECIPITATION&#39;] / 60 / 5 # convert m3 per 5 min to m3/s fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,7)) fig.subplots_adjust(hspace=0.05) ratio = total_qstar/ total_p pstar = df_p_small[&#39;PRECIPITATION&#39;] * ratio total_pstar = pstar.sum() * 5 * 60 ax1.bar(pstar.index, pstar, width=1/24/12) ax2.plot(qstar, color=&quot;tab:blue&quot;, lw=2) print(total_pstar, total_qstar, total_qstar/total_pstar) ax1.set_xlim([pd.to_datetime(&quot;2020-10-20 14:00:00&quot;), pd.to_datetime(&quot;2020-10-21 04:00:00&quot;)]) ax2.set_xlim([pd.to_datetime(&quot;2020-10-20 14:00:00&quot;), pd.to_datetime(&quot;2020-10-21 04:00:00&quot;)]) # # total_p = df_p_small[&#39;PRECIPITATION&#39;].sum() # total_q = df_q_small[&#39;discharge&#39;].sum() * 60*5 # dirty trick to get common title between the two panels: # make a large invisible axes, give it a title ax0 = fig.add_subplot(111, frame_on=False) ax0.tick_params(labelcolor=&quot;none&quot;, bottom=False, left=False) ax0.set_ylabel(r&quot;flow (m$^3$/s)&quot;) ax1.text(0.99, 0.97, r&quot;effective precipitation $p !* !(t)$&quot; + &quot; n&quot; + f&quot;P* = {total_pstar:.2e}&quot; + r&quot; m$^3$&quot;, transform=ax1.transAxes, ha=&#39;right&#39;, va=&#39;top&#39;, fontsize=20) ax2.text(0.99, 0.97, r&quot;effective discharge $q !* !(t)$&quot; + &quot; n&quot; + f&quot;Q* = {total_qstar:.2e}&quot; + r&quot; m$^3$&quot;, transform=ax2.transAxes, ha=&#39;right&#39;, va=&#39;top&#39;, fontsize=20) ax1.set_xticks([]) locator = mdates.AutoDateLocator(minticks=3, maxticks=7) formatter = mdates.ConciseDateFormatter(locator) ax2.xaxis.set_major_locator(locator) ax2.xaxis.set_major_formatter(formatter) ax2.set_ylim([0,5.5]) ax1.set_title(&quot;precipitation centroid and discharge centroid, Boneyard Creek at Urbana, IL n 20-21 October 2020, 5-minute data&quot;) ax1.plot([t_pc, t_pc], ax1.get_ylim(), color=&quot;tab:red&quot;, lw=2) ax2.plot([t_qc, t_qc], ax2.get_ylim(), color=&quot;tab:red&quot;, lw=2) ax2.plot([t_pk, t_pk], ax2.get_ylim(), color=&quot;tab:olive&quot;, lw=2) ax1.text(t_pc, ax1.get_ylim()[1], r&quot;$t_{pc}=$&quot;+f&quot;{t_pc.hour}:{t_pc.minute}&quot;, ha=&quot;right&quot;, va=&quot;top&quot;, fontsize=16) ax2.text(t_qc, ax2.get_ylim()[0], r&quot;$t_{qc}=$&quot;+f&quot;{t_qc.hour}:{t_qc.minute}&quot;, ha=&quot;right&quot;, va=&quot;bottom&quot;, fontsize=16) ax2.text(t_pk, ax2.get_ylim()[0], r&quot;$t_{pk}=$&quot;+f&quot;{t_pk.hour}:{t_pk.minute}&quot;, ha=&quot;left&quot;, va=&quot;bottom&quot;, fontsize=16) ax2.plot([t_pc, t_pc], ax2.get_ylim(), color=&quot;black&quot;, lw=2, ls=&#39;:&#39;) ax2.annotate(&quot;&quot;, xy=(t_pc, ax2.get_ylim()[1]*0.9), #xycoords=&#39;data&#39;, xytext=(t_qc, ax2.get_ylim()[1]*0.9),# textcoords=&#39;data&#39;, size=16, arrowprops=dict(arrowstyle=&quot;&lt;-&gt;&quot;, shrinkA=0, shrinkB=0, connectionstyle=&quot;arc3&quot;, color=&#39;black&#39;), ) ax2.annotate(&quot;&quot;, xy=(t_pc, ax2.get_ylim()[1]*0.7), #xycoords=&#39;data&#39;, xytext=(t_pk, ax2.get_ylim()[1]*0.7),# textcoords=&#39;data&#39;, size=16, arrowprops=dict(arrowstyle=&quot;&lt;-&gt;&quot;, shrinkA=0, shrinkB=0, connectionstyle=&quot;arc3&quot;, color=&#39;black&#39;), ) centroid_lag = t_qc - t_pc centroid_lag_to_peak = t_pk - t_pc centroid_lag_minutes = int(centroid_lag.total_seconds() / 60) centroid_lag_to_peak_minutes = int(centroid_lag_to_peak.total_seconds() / 60) ax2.text(t_pc, ax2.get_ylim()[1]*0.9, r&quot;$T_{LC}=$&quot;+f&quot;{centroid_lag_minutes} minutes&quot;, ha=&quot;right&quot;, va=&quot;center&quot;, fontsize=16) ax2.text(t_pc, ax2.get_ylim()[1]*0.7, r&quot;$T_{LPC}=$&quot;+f&quot;{centroid_lag_to_peak_minutes} minutes&quot;, ha=&quot;right&quot;, va=&quot;center&quot;, fontsize=16) fig.savefig(&quot;hydrology_figures/urbana_lags.png&quot;) . 41692.08473760001 41692.0847376 0.9999999999999998 . t_pk[0] . Timestamp(&#39;2020-10-20 19:55:00&#39;, freq=&#39;5T&#39;) . t_qc.hour . 19 .",
            "url": "https://yairmau.github.io/website/jupyter/2020/02/05/streamflow-code.html",
            "relUrl": "/jupyter/2020/02/05/streamflow-code.html",
            "date": " â€¢ Feb 5, 2020"
        }
        
    
  
    
        ,"post35": {
            "title": "Assignment 3 - Streamflow",
            "content": "&#128210; instructions . This is where learning happens, not during a lecture. You&#39;ll learn a ton of things by doing them yourself. Much success! ğŸ˜„ . Create a Jupyter Notebook called assignment-03-IDNUMBER, where IDNUMBER is your 9-digit ID. This is the file only file we will check. . &#128204; locations and data . Choose one location in the US. . Import streamflow data from USGS&#39;s National Water Information System. Choose on the map any measuring station you see fit. Make sure there is available discharge data (usually given in cubic feet per second) in small time intervals, e.g., every 15 minutes. . | Go to NOAA&#39;s Climate Reference Network Data website. The sub-hourly (5-min) data contains information on many variables, we are interested in precipitation. . | Attention! Some os the USGS stations provide precipitation data. If you find one such station, step 2 above is unnecessary. If you only find discharge data in the USGS website, then make sure you choose two stations in very close proximity (USGS and NOAA). Because there are only a few high-resolution NOAA stations, you might want to start from there and then find discharge data for a stream near the NOAA station. . Bottom line: you are looking for precipitation and stream discharge data, for stations in close proximity, with a high temporal resolution (5 min, 15 min, etc). . &#128736; tasks . Choose a rain event of a few hours in your data set. Find the rate of effective water input (p) and the event flow rate (q). Analyze the data in a similar was as done during class (various graphs explaining what you see). Find also the characteristic times of the event (centroid lag $T_{LC}$, and centroid lag-to-peak $T_{LPC}$). . Try to find information on the climate, geography, soil, and land use of the watershed. Begin the assignment by explaining about the watershed you chose and characterizing it. When presenting the data and your analyses, discuss what you see based on the concepts learned in class (infiltration, runoff generation, and the factors that affect them). Does the information you found match what you see? What makes sense, and what doesn&#39;t? . Discussion is important! . You will have two weeks to deliver your assignment. You should not hand in a dry document with only figures and code, I&#39;m expecting text before and after each code/graph cell, explaining what you did, why you did it, and how it fits the story you are telling. Don&#39;t forget to put labels on your plot axes, title, legend, etc. . Your Jupyter Notebook should be fully functional: if we press Kernel &gt; Restart &amp; Run All, all the code must work without any errors. . &#127749; presentation . All the assignment must be in one single Jupyter Notebook. Use markdown cells to discuss the analysis and results, and in code cells show all the code you used to produce the figures and data analysis. Leave only the code necessary for your analysis, delete unnecessary lines your wrote while analyzing your data. Don&#39;t forget to comment your code, just like we did during exercise sessions. . You can write in English or in Hebrew, but the text in the figures must be in English. If you choose to write the discussion in Hebrew, be aware that Jupyter Notebooks don&#39;t have native right-to-left language support: . × ×™×ª×Ÿ ×œ×›×ª×•×‘ ×‘×¢×‘×¨×™×ª, ×œ××¨×•×ª ×©×–×” ×œ× × ×¨××” ×›×´×› ×˜×•×‘... . You can use some HTML code to achieve best results in Hebrew. Type the following . &lt;p dir=&quot;rtl&quot; style=&quot;text-align: right;&quot;&gt; ×¢×›×©×™×• ×”×¨×‘×” ×™×•×ª×¨ ×˜×•×‘! &lt;/p&gt; . to get . ×¢×›×©×™×• ×”×¨×‘×” ×™×•×ª×¨ ×˜×•×‘! . If you have many paragraphs in hebrew, do the following: . ×¤×¡×§×” ××¡×¤×¨ 1. . ×¤×¡×§×” ××¡×¤×¨ 2. . ×× ×™×© ×œ×›× ×›××” ×¤×¡×§××•×ª, ×›×œ ××—×ª ××”×Ÿ ×ª×”×™×” ×‘×ª×•×š &quot;dir&quot; ××©×œ×” . In my opinion it is too complicated to write in Hebrew in Jupyter Notebooks, just write in English, your grade will not be affected by typos nor less-than-perfect English proficiency. . &#128175; evaluation . Your assignment will be evaluated according to the following criteria: . 40% Presentation. How the graphs look, labels, general organization, markdown, clean code. | 30% Discussion. This is where you explain what you did, what you found out, etc. | 15% Depth of analysis. You can analyze/explore the data with different levels of complexity, this is where we take that into consideration. | 10% Replicability: Your code runs flawlessly. | 5%: Code commenting. Explain in your code what you are doing, this is good for everyone, especially for yourself! | . &#128666; importing the data . You can use the code from previous assignments and from the exercise lectures. .",
            "url": "https://yairmau.github.io/website/jupyter/2020/02/05/assignment-03-streamflow.html",
            "relUrl": "/jupyter/2020/02/05/assignment-03-streamflow.html",
            "date": " â€¢ Feb 5, 2020"
        }
        
    
  
    
        ,"post36": {
            "title": "Infiltration - lecture",
            "content": "Sources . Sources used: . (Hillel, 1998) (Dingman, 2015) (Ward and Trimble, 2003) . Definitions . (Hillel, 1998), figure 14.6, page 400 . (Dingman, 2015), figure 8.13, page 360 . (Dingman, 2015), figure 8.14, page 360 . (Hillel, 1998), figure 14.3, page 390 . (Dingman, 2015), page 355 . The water-input rate, $w(t)$ [L T$^{-1}$], is the rate at which water arrives at the surface due to rain, snowmelt, or irrigation. A water-input event begins at time $t=0$ and ends at $t=T_w$. | The infiltration rate, $f(t)$ [L T$^{-1}$], is the rate at which water enters the soil from the surface. | The infiltrability, also called infiltration capacity, $f^*(t)$ [L T$^{-1}$], is the maximum rate at which infiltration could occur at any time; note that this changes during the infiltration event. | The depth of ponding, $H(t)$ [L], is the depth of water standing on the surface. | . (Ward and Trimble, 2003), page 63, 64 . Infiltration capacity of absorbent paper is low, there is much runoff . Infiltration capacity of sponge is high, there is little runoff . Infiltration capacity of the sponge is limited by the overlying layer with low permeability . Infiltration capacity of the sponge is limited by the underlying layer . (Ward and Trimble, 2003), page 65 . . . Darcy . Darcyâ€™s equation for vertical flow . q=âˆ’Kâˆ‚Htotalâˆ‚zq = -K frac{ partial H_ text{total}}{ partial z}q=âˆ’Kâˆ‚zâˆ‚Htotalâ€‹â€‹ . where the total head $H_ text{total}=-H_ text{suction}-z_ text{depth}$, and . $H_ text{suction}$ is the suction head (negative pressure head) | $z_ text{depth}$ is the depth, points downward. | . Substituting: . q=Kâˆ‚Hsuctionâˆ‚z+Kq = K frac{ partial H_ text{suction}}{ partial z} + Kq=Kâˆ‚zâˆ‚Hsuctionâ€‹â€‹+K . Substituting the above into the continuity equation . âˆ‚Î¸âˆ‚t=âˆ‚qâˆ‚z frac{ partial theta}{ partial t} = frac{ partial q}{ partial z}âˆ‚tâˆ‚Î¸â€‹=âˆ‚zâˆ‚qâ€‹ . yields the Richards equation. . . . Richards . Richards equation: . âˆ‚Î¸âˆ‚t=âˆ‚âˆ‚z[K(Î¸)âˆ‚Htotalâˆ‚z] frac{ partial theta}{ partial t} = frac{ partial}{ partial z} left[ K( theta) frac{ partial H_ text{total}}{ partial z} right]âˆ‚tâˆ‚Î¸â€‹=âˆ‚zâˆ‚â€‹[K(Î¸)âˆ‚zâˆ‚Htotalâ€‹â€‹] . Substituting $H_ text{total}=-H_ text{suction}-z_ text{depth}$ yields: . âˆ‚Î¸âˆ‚t=âˆ‚âˆ‚z[K(Î¸)(âˆ‚(âˆ’Hsuctionâˆ’z)âˆ‚z)] frac{ partial theta}{ partial t} = frac{ partial}{ partial z} left[ K( theta) left( frac{ partial(-H_ text{suction} - z)}{ partial z} right) right]âˆ‚tâˆ‚Î¸â€‹=âˆ‚zâˆ‚â€‹[K(Î¸)(âˆ‚zâˆ‚(âˆ’Hsuctionâ€‹âˆ’z)â€‹)] . âˆ‚Î¸âˆ‚t=âˆ’âˆ‚âˆ‚z(K(Î¸)âˆ‚Hsuctionâˆ‚z)undefinedmatricâˆ’âˆ‚K(Î¸)âˆ‚zundefinedgravitational frac{ partial theta}{ partial t} = . underbrace{ frac{ partial}{ partial z} left( K( theta) frac{ partial H_ text{suction}}{ partial z} right) } _{ text{matric}} - underbrace{ frac{ partial K( theta)}{ partial z} } _{ text{gravitational}}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;âˆ‚tâˆ‚Î¸â€‹=âˆ’matric . âˆ‚zâˆ‚â€‹(K(Î¸)âˆ‚zâˆ‚Hsuctionâ€‹â€‹)â€‹â€‹âˆ’gravitational . âˆ‚zâˆ‚K(Î¸)â€‹â€‹â€‹&lt;/span&gt;&lt;/span&gt; . Short times . As the water starts to enter the relatively dry soil, the pressure differences in the water at the surface and in the soil are quite large and, as a result, the second term on the right is practically negligible compared to the first one. . âˆ‚Î¸âˆ‚t=âˆ’âˆ‚âˆ‚z(K(Î¸)âˆ‚Hâˆ‚z) frac{ partial theta}{ partial t} = . frac{ partial}{ partial z} left( K( theta) frac{ partial H}{ partial z} right)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;âˆ‚tâˆ‚Î¸â€‹=âˆ’âˆ‚zâˆ‚â€‹(K(Î¸)âˆ‚zâˆ‚Hâ€‹)&lt;/span&gt;&lt;/span&gt; . Long times . As illustrated in the figure below (Davidson et al., 1963), after longer times of infiltration, the water content profile near the surface gradually becomes more uniform and it eventually assumes the satiation value, or $ theta rightarrow theta_0$; similarly, the pressure in the upper layers of the soil becomes gradually atmospheric, or $H rightarrow 0$. Hence, their vertical gradients . âˆ‚Î¸âˆ‚zÂ andÂ âˆ‚Hsuctionâˆ‚zâŸ¶0 frac{ partial theta}{ partial z} text{ and } frac{ partial H_ text{suction}}{ partial z} longrightarrow 0âˆ‚zâˆ‚Î¸â€‹Â andÂ âˆ‚zâˆ‚Hsuctionâ€‹â€‹âŸ¶0 . From Darcyâ€™s equation we have that . q=K(Î¸0)=Ksatq = K( theta_0) = K_ text{sat}q=K(Î¸0â€‹)=Ksatâ€‹ . . . . Rainfall infiltration . Infiltration rate is equal to rainfal rate, at least at first. If rainfall rate $w$ is lower than $K_ text{sat}$, than everything enters the soil, i.e., $f=K_ text{sat}$. However, if $w&gt;K_ text{sat}$, water content $ theta$ will increase at the surface, until it reaches $ theta_0$, and at that moment, called ponding time $t_p$, water will begin to accumulate at the surface. . (Hillel, 1998), figure 14.1, page 386 . (Hillel, 1998), figure 14.2, page 388 . . . Horton equation . One of the most widely used models, developed by R.E. Horton (1939), considered to be the father of modern hydrology. . f=fc+(f0âˆ’fc)eâˆ’Î²tf = f_c+(f_0-f_c)e^{- beta t}f=fcâ€‹+(f0â€‹âˆ’fcâ€‹)eâˆ’Î²t . $f$: infiltration rate | $f_c$: infiltration capacity at large $t$ | $f_0$: initial infiltration capacity | $ beta$: best fit empirical parameter | . Advantages . Simple equation | Usually gives good fit to measured data because it is dependent on three parameters | . Disadvantages . This method has no physical significance, it is not based on any water transport mechanism | Does not describe infiltration prior to ponding | . . . Green &amp; Ampt . (Dingman, 2015), figure 8.11, page 357 . Assumptions: . homogeneous soil, infinite depth (no water table) | horizontal surface | constant water head equal to zero is maintained at the surface | uniform water content prior to wetting, $ theta(t=0,z)= theta_0$ | moving front is characterized by a constant matric suction, $ psi_f$ | . (Dingman, 2015), page 370 . This equation was developed under the scenario of constant rainfall or irrigation on an initially dry soil as a sharp wetting front (such as piston flow). Water penetrates a dry soil with a certain initial moisture content, and wets the layer to a saturated moisture content as it traverses deeper. The connection between soil moisture and infiltration rate is modeled in the Green-Ampt equation: . f(t)=Ksat[1+âˆ£Ïˆfâˆ£â‹…(Ï•âˆ’Î¸0)F(t)]f(t) = K_ text{sat} left[ 1 + frac{| psi_f| cdot left( phi - theta_0 right)}{F(t)} right]f(t)=Ksatâ€‹[1+F(t)âˆ£Ïˆfâ€‹âˆ£â‹…(Ï•âˆ’Î¸0â€‹)â€‹] . $f(t)$: infiltration rate | $F(t)$: cumulative infiltration rate, $F= int f text{ d}t$ | $ psi_f$: effective wetting-front suction | $ phi$: soil porosity | $ theta_0$: initial soil water content | . The same equation can be simply be rewritten as . f=AF+Bf = frac{A}{F} + Bf=FAâ€‹+B . where . A=Ksatâ‹…âˆ£Ïˆfâˆ£â‹…(Ï•âˆ’Î¸0)B=Ksat begin{align} A &amp;= K_ text{sat} cdot| psi_f| cdot left( phi - theta_0 right) B &amp;= K_ text{sat} end{align}ABâ€‹=Ksatâ€‹â‹…âˆ£Ïˆfâ€‹âˆ£â‹…(Ï•âˆ’Î¸0â€‹)=Ksatâ€‹â€‹â€‹ . The porosity $ phi$ and the saturated hydraulic conductivity $K_ text{sat}$ can be estimated from the soil texture. The wetting-front suction $ psi_f$ can be estimated using the Brooks-Corey parameters: . âˆ£Ïˆfâˆ£=2b+32b+6â‹…âˆ£Ïˆaeâˆ£,| psi_f| = frac{2b+3}{2b+6} cdot | psi_{ae}|,âˆ£Ïˆfâ€‹âˆ£=2b+62b+3â€‹â‹…âˆ£Ïˆaeâ€‹âˆ£, . where $ psi_{ae}$ is the air-entry pressure head. Values for the parameters above can be found in this table: . . . . Best Fit, Least Squares Method . . References . Hillel, D., 1998. Environmental Soil Physics. Academic Press. | Dingman, S.L., 2015. Physical Hydrology: Third Edition. Waveland Press. | Ward, A.D., Trimble, S.W., 2003. Environmental Hydrology, Second Edition. CRC Press. |",
            "url": "https://yairmau.github.io/website/markdown/2020/02/04/infiltration-lecture.html",
            "relUrl": "/markdown/2020/02/04/infiltration-lecture.html",
            "date": " â€¢ Feb 4, 2020"
        }
        
    
  
    
        ,"post37": {
            "title": "Infiltration - exercises",
            "content": "Tasks . Google the following: web plot digitizer | Load image &quot;nassif-16percent-slope.png&quot; (see below) | Create four csv files, one for each data set. Call them whatever you want. Legend: white circle = 312 mm/h, triangle = 234 mm/h, x = 156 mm/h, black circle = 78 mm/h. | The image is the second panel of Fig. 8, from . Nassif, S. H., and E. M. Wilson, 1975, &quot;THE INFLUENCE OF SLOPE AND RAIN INTENSITY ON RUNOFF AND INFILTRATION&quot;, Hydrological Sciences Journal. download here . Import relevant packages . import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns.set(style=&quot;ticks&quot;, font_scale=1.5) from scipy.optimize import curve_fit import matplotlib.patches as patches . . Load all four files you created. Use numpy&#39;s function loadtxt. Make sure that the first point in each table corresponds to the appropriate rainfall rate. You can normalize the data if it is not. . d1 = np.loadtxt(&quot;input_rate_078mm_per_h_16percent_slope.csv&quot;, delimiter=&#39;,&#39;) d2 = np.loadtxt(&quot;input_rate_156mm_per_h_16percent_slope.csv&quot;, delimiter=&#39;,&#39;) d3 = np.loadtxt(&quot;input_rate_234mm_per_h_16percent_slope.csv&quot;, delimiter=&#39;,&#39;) d4 = np.loadtxt(&quot;input_rate_312mm_per_h_16percent_slope.csv&quot;, delimiter=&#39;,&#39;) d1[:,1] = 78 * d1[:,1] / d1[:,1].max() d2[:,1] = 156 * d2[:,1] / d2[:,1].max() d3[:,1] = 234 * d3[:,1] / d3[:,1].max() d4[:,1] = 312 * d4[:,1] / d4[:,1].max() . . Reproduce the original figure, make it look good, something like this: . fig, ax = plt.subplots(figsize=(10,7)) ax.plot(d4[:,0], d4[:,1], &#39;o&#39;, markerfacecolor=&quot;None&quot;, label=r&quot;water input = 312 mm h$^{-1}$&quot;) ax.plot(d3[:,0], d3[:,1], &#39;^&#39;, label=r&quot;water input = 234 mm h$^{-1}$&quot;) ax.plot(d2[:,0], d2[:,1], &#39;x&#39;, label=r&quot;water input = 156 mm h$^{-1}$&quot;) ax.plot(d1[:,0], d1[:,1], &#39;o&#39;, label=r&quot;water input = 78 mm h$^{-1}$&quot;) ax.set(xlabel=&quot;Time (min)&quot;, ylabel=r&quot;Infiltration rate (mm h$^{-1}$)&quot;) ax.legend(loc=&quot;upper right&quot;); . . Horton&#39;s equation . $$ f = f_c+(f_0-f_c)e^{- beta t} $$ $f$: infiltration rate | $f_c$: infiltration capacity at large $t$ | $f_0$: initial infiltration capacity | $ beta$: best fit empirical parameter | . Write a function called horton, that receives time t and the three parameters, and returns the right-hand side of the equation above. Plot one of the data sets, together with a guess of the parameters that should roughly fit the data. . def horton(t, fc, f0, beta): return fc + (f0 - fc)*np.exp(-beta*t) fig, ax = plt.subplots(figsize=(10,7)) t = d1[:,0] t = t - t[0] f = d1[:,1] ax.plot(t, f, &#39;o&#39;, label=&quot;data&quot;) ax.plot(t, horton(t, 35, 80, 0.5), &#39;-&#39;, label=&quot;horton&quot;) ax.set(xlabel=&quot;time (min)&quot;, ylabel=&quot;infiltration rate (mm/h)&quot;) ax.legend(loc=&quot;upper right&quot;); . . Find the best fit for the parameters $f_c, f_0, beta$. Calculate the $R^2$ for each data set. . For the best fit, use scipy&#39;s curve_fit. Write a function to compute the R-squared of your fit. . def horton(t, fc, f0, beta): return fc + (f0 - fc)*np.exp(-beta*t) def best_fit(data): t = data[:,0] t0 = t[0] t = t - t0 f = data[:,1] # best fit popt, pcov = curve_fit(f=horton, # model function xdata=t, # x data ydata=f, # y data p0=(130, 800, 0.5), # initial guess of the parameters ) return [popt, pcov] def calculate_r_squared(data, popt): t = data[:,0] t = t - t[0] f = data[:,1] # Calculate residuals residuals = f - horton(t, *popt) # You can get the residual sum of squares (ss_res) with ss_res = np.sum(residuals**2) # You can get the total sum of squares (ss_tot) with ss_tot = np.sum((f - np.mean(f))**2) # And finally, the r_squared-value with, r_squared = 1 - (ss_res / ss_tot) return r_squared def plot_best_fit(data, axis, marker, markercolor): # calculate best fit parameters popt, pcov = best_fit(data) t = data[:,0] f = data[:,1] # plot data points ax.plot(t, f, marker, markerfacecolor=markercolor, markeredgecolor=&quot;black&quot;) # plot best fit line r_squared = calculate_r_squared(data, popt) labeltext = r&quot;$f_c=$ {:.2f}, $f_0=$ {:.2f}, $ beta=$ {:.2f}, $R^2=$ {:.2f}&quot;.format(popt[0],popt[1],popt[2], r_squared) ax.plot(t, horton(t-t[0], *popt), color=markercolor, label=labeltext) fig, ax = plt.subplots(figsize=(10,7)) plot_best_fit(d1, ax, &#39;o&#39;, &quot;tab:red&quot;) plot_best_fit(d2, ax, &#39;x&#39;, &quot;tab:blue&quot;) plot_best_fit(d3, ax, &#39;^&#39;, &quot;tab:orange&quot;) plot_best_fit(d4, ax, &#39;d&#39;, &quot;tab:green&quot;) ax.set(xlabel=&quot;time (min)&quot;, ylabel=&quot;infiltration rate (mm/h)&quot;, title=r&quot;Horton&#39;s Equation: $f = f_c+(f_0-f_c)e^{- beta t}$&quot;) ax.legend(); # fig.savefig(&quot;horton-fit.png&quot;, dpi=500) . . Make a graph of the infiltration rate and of the runoff, as a function of time. Use any of the four data sets you have. . fig, ax = plt.subplots(figsize=(10,7)) data = d4 t = data[:, 0] f = data[:, 1] t = np.concatenate([ [0], t]) f = np.concatenate([ [f[0]], f]) runoff = f[0] - f ax.plot(t, f*0 + f[0], ls=&quot;--&quot;, color=&quot;black&quot;, label=&quot;rainfall&quot;) ax.plot(t, f, color=&quot;tab:blue&quot;, lw=3, label=r&quot;infiltration&quot;) ax.plot(t, runoff, color=&quot;tab:orange&quot;, lw=3, label=r&quot;runoff&quot;) ax.set(xlabel=&quot;Time (min)&quot;, ylabel=r&quot;Rate (mm h$^{-1}$)&quot;) ax.legend(loc=&quot;lower right&quot;); . . Green &amp; Ampt . $$f = frac{A}{F} + B$$ . where . $A = K_ text{sat} cdot| psi_f| cdot left( phi - theta_0 right)$ | $B= K_ text{sat}$ | . Write a function that calculates the cumulative of the infiltration rate. . $$ F(t) = int_0^t f(t) text{ d}t $$Use numpy&#39;s trapz function, that implements the &quot;trapezoidal rule&quot; . . def cumulative_F(t, f): F = np.array([0]) t = t/60 # convert minute to hour for i in np.arange(2,len(t)+1): area = np.trapz(f[:i], t[:i]) F = np.concatenate([F, [area]]) return F fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,7)) t, f = d1[:,0], d1[:,1] F = cumulative_F(t, f) ax1.plot(t, f, label=&quot;f, rate&quot;) ax2.plot(t, F, label=&quot;F, cumulative&quot;) ax1.set(xlabel=&quot;t (min)&quot;, ylabel=&quot;f (mm/h)&quot;) ax2.set(xlabel=&quot;t (min)&quot;, ylabel=&quot;F (mm)&quot;) ax2.yaxis.set_label_position(&quot;right&quot;) . . Plot $f$ as a function of $F$. Try to guess $A$ and $B$ that give reasonable results. . fig, ax = plt.subplots(figsize=(10,7)) t, f = d1[:,0], d1[:,1] F = cumulative_F(t, f) ax.plot(F, f) A=50; B=30; ax.plot(F, A/F + B, &#39;o&#39;) ax.set(xlabel=&quot;F&quot;, ylabel=&quot;f&quot;) . . /Users/yairmau/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: RuntimeWarning: divide by zero encountered in true_divide . [Text(0.5, 0, &#39;F&#39;), Text(0, 0.5, &#39;f&#39;)] . Use the curve_fit to find the optimal values for $A$ and $B$. . def G_and_A(F, A, B): return A/F + B popt, pcov = curve_fit(f=G_and_A, # model function xdata=F[1:], # x data ydata=f[1:], # y data p0=(50, 30), # initial guess of the parameters ) # popt, pcov = curve_fit(G_and_A, F[1:], f[1:], p0=(50, 30)) # p0 = initial guess print(popt) fig, ax = plt.subplots(figsize=(10,7)) ax.plot(F, f) ax.plot(F[1:], popt[0]/F[1:] + popt[1], &#39;o&#39;) ax.set(xlabel=&quot;F&quot;, ylabel=&quot;f&quot;) . . [24.12368526 36.34242813] . [Text(0.5, 0, &#39;F&#39;), Text(0, 0.5, &#39;f&#39;)] . Homework . Go to Soil Texture Calculator, estimate the texture of &quot;standard soil&quot; in Nassif &amp; Wilson, 1975. .",
            "url": "https://yairmau.github.io/website/jupyter/2020/02/04/infiltration-exercises.html",
            "relUrl": "/jupyter/2020/02/04/infiltration-exercises.html",
            "date": " â€¢ Feb 4, 2020"
        }
        
    
  
    
        ,"post38": {
            "title": "Evapotranspiration - lecture",
            "content": "Sources . Sources used: . (Hillel, 1998) (Dingman, 2015) (Ward and Trimble, 2003) (Brutsaert, 2005) . Introduction . (Dingman, 2015), chapter 6. . Globally, about 62% of the precipitation that falls on the continents is evapotranspirated, amounting to 73 thousand km$^3$/yr. Of this, about 42% (29 thousand km$^3$/yr) is transpiration, and about 3% is open-water evaporation. Most of the remainder is interception loss; soil evaporation is a minor component of the total. . . . Potential Evapotranspiration . Potential Evapotranspiration (PET) is the rate at which evapotranspiration would occur from a large area completely and uniformly covered with growing vegetation with access to an unlimited supply of soil water and without advection or heat-storage effects. . Several characteristics of a vegetative surface have a strong influence on ET rate. a) the albedo of the surface, which determines the net radiation; b) the maximum leaf conductance; c) the atmospheric conductance, largely determined by vegetation height; d) presence or absence of intercepted water. . Reference-Crop Evapotranspiration . Reference-crop evapotranspiration (RET) is the amount of water transpired by a short green crop, completely shading the ground, of uniform height, and never short of water. . The magnitude of PET is often calculated from meteorological data collected under conditions in which the actual ET rate is less than the potential rate. If ET had been occurring at the potential rate, the latent- and sensible-heat exchanges between air and the surface, and hence the air temperature and humidity, would have been considerably different. (Brutsaert 1982) . . Review of methods . There are a variety of ways to estimate evaporative flux in nature. The following table categorizes each method based on the data that must be acquired to apply it: . . These methods also vary in the timescales in which they are relevant, typically in correlation with the variety of data needed: . Thornthwaite and SCS Blaney-Criddle: monthly or seasonal estimations (minimal data) | Jensen-Haise: 5-day estimates (good enough timescale and data for irrigation scheduling) | Penman: daily estimates | Penman-Monteith: hourly estimates (requires a lot of data) | . . Thornthwaite . (Ward and Trimble, 2003), pages 107-108. . Thornthwaite (1948) developed an equation to predict monthly evapotranspiration from mean monthly temperature and latitude data (Equation 4.27). The small amount of data needed is attractive because often ET needs to be predicted for sites where few weather data are available. Based on what we know about ET, we should be skeptical about the general applicability of such a simple equation. Thornthwaite (1948) was not satisfied with the proposed approach: â€œThe mathematical development is far from satisfactory. It is empirical. â€¦ The chief obstacle at present to the development of a rational equation is the lack of understanding of why potential ET corresponding to a given temperature is not the same everywhere.â€ Taylor and Ashcroft (1972), as cited in Skaggs (1980), provided insight into the answer to Thornthwaiteâ€™s question. They said: . This equation, being based entirely upon a temperature relationship, has the disadvantage of a rather flimsy physical basis and has only weak theoretical justification. Since temperature and vapor pressure gradients are modified by the movement of air and by the heating of the soil and surroundings, the formula is not generally valid, but must be tested empirically whenever the climate is appreciably different from areas in which it has been tested. â€¦ In spite of these shortcomings, the method has been widely used. Because it is based entirely on temperature data that are available in a large number of localities, it can be applied in situations where the basic data of the Penman method are not available. . M.E. Jensen et al. (1990) warn that Thornthwaiteâ€™s method is generally only applicable to areas that have climates similar to that of the east central U.S., and it is not applicable to arid and semiarid regions. . Thornthwaite (1948) found that evapotranspiration could be predicted from an equation of the form . E=16[10â€‰TmonthlyÂ meanI]a,E = 16 left[ frac{10 ,T^ text{monthly mean}}{I} right]^a,E=16[I10TmonthlyÂ meanâ€‹]a, . where . I=âˆ‘i=112[TimonthlyÂ mean5]1.514,I = sum_{i=1}^{12} left[ frac{T_i^ text{monthly mean}}{5} right]^{1.514},I=i=1âˆ‘12â€‹[5TimonthlyÂ meanâ€‹â€‹]1.514, . and . a=6.75Ã—10âˆ’7I3âˆ’7.71Ã—10âˆ’5I2+1.792Ã—10âˆ’2I+0.49239 begin{align} a &amp;= 6.75 times 10^{-7}I^3 &amp;- 7.71 times 10^{-5}I^2 &amp;+ 1.792 times 10^{-2}I &amp;+ 0.49239 end{align}aâ€‹=6.75Ã—10âˆ’7I3âˆ’7.71Ã—10âˆ’5I2+1.792Ã—10âˆ’2I+0.49239â€‹â€‹ . $E$ is the monthly potential ET (mm) | $T_ text{monthly mean}$ is the mean monthly temperature in Â°C | $I$ is a heat index | $a$ is a location-dependent coefficient | . . Penman . (Brutsaert, 2005), pages 123-127. (Ward and Trimble, 2003), subsections 4.5.2, 4.5.3, 4.5.5, 4.6.6. Allen et al. (1998), â€œCrop evapotranspiration - Guidelines for computing crop water requirements - FAO Irrigation and drainage paper 56â€ . The Penman model is almost entirely a theory-based formula for predicting evaporative flux. It can run on a much finer timescale, and requires a much wider variety of data than most models. In addition to temperature, the Penman functions on measurements of radiation, wind speed, elevation above sea level, vapor-pressure deficit, and heat flux density to the ground. The potential ET (in mm d$^{-1}$) is given by: . E=1Î»[Î”Î”+Î³Qne+Î³Î”+Î³EA],E = frac{1}{ lambda} left[ frac{ Delta}{ Delta+ gamma}Q_{ne}+ frac{ gamma}{ Delta+ gamma}E_A right],E=Î»1â€‹[Î”+Î³Î”â€‹Qneâ€‹+Î”+Î³Î³â€‹EAâ€‹], . where $Q_n$ is the available energy flux density . Qn=Rnâˆ’G,Q_n = R_n - G,Qnâ€‹=Rnâ€‹âˆ’G, . and $E_A$ is the drying power of the air . EA=6.43â‹…f(u)â‹…VPD.E_A = 6.43 cdot f(u) cdot text{VPD}.EAâ€‹=6.43â‹…f(u)â‹…VPD. . The constituents of the equations above are . $E$: potential evapotranspiration (mm d$^{-1}$) | $ Delta$: slope of the saturation water vapor pressure curve (kPa Â°C$^{-1}$) | $ gamma$: psychrometric constant (kPA Â°C$^{-1}$) | $ lambda$: latent heat of vaporization (MJ kg$^{-1}$) | $R_n$: net radiation (MJ m$^{-2} d^{-1}$) | $G$: heat flux density to the ground (MJ m$^{-2} d^{-1}$) | $f(u)$: wind function (dimensionless) | VPD: vapor pressure deficit (kPa) | . and the number 6.43 adjusts the units of $E_A$ so it is in MJ m$^{-2}$ d$^{-1}$. In what follows, we will further discuss these constituents. . Psychrometric Constant . The psychrometric constant $ gamma$ (kPA Â°C$^{-1}$) relates the partial pressure of water in air to the air temperature: . Î³=cpâ€‰PÎ»â‹…MWratio gamma = frac{c_p , P}{ lambda cdot MW_ text{ratio}}Î³=Î»â‹…MWratioâ€‹cpâ€‹Pâ€‹ . P=101.3âˆ’0.01055HP = 101.3-0.01055 HP=101.3âˆ’0.01055H . Î»=2.501âˆ’2.361Ã—10âˆ’3â€‰T lambda = 2.501 - 2.361 times 10^{-3} ,TÎ»=2.501âˆ’2.361Ã—10âˆ’3T . $MW_ text{ratio}=0.622$: ratio molecular weight of water vapor/dry air | $P$: atmospheric pressure (kPa). Can be either measured or inferred from station height above sea level (m). | $ lambda$: latent heat of water vaporization (MJ kg$^{-1}$) | $c_p=0.001013$: specific heat capacity of moist air (MJ kg$^{-1}$ Â°C$^{-1}$) | . Net Radiation . (Ward and Trimble, 2003), page 99. . $R_n$ (MJ m$^{-2} d^{-1}$) is net radiation, the balance between net short wave $R_s$ and the long wave $R_b$ components of the radiation: . Rn=(1âˆ’Î±)Rsâ€‰â£â€‰â£â†“âˆ’Rbâ€‰â£â€‰â£â†‘,R_n = (1- alpha)R_s ! ! downarrow -R_b ! ! uparrow,Rnâ€‹=(1âˆ’Î±)Rsâ€‹â†“âˆ’Rbâ€‹â†‘, . where $ alpha$ (dimensionless) is the albedo. The net outgoing thermal radiation $R_b$ is given by . Rb=(aRsRso+b)Rbo,R_b = left( a frac{R_s}{R_{so}+b} right)R_{bo},Rbâ€‹=(aRsoâ€‹+bRsâ€‹â€‹)Rboâ€‹, . where $R_{so}$ is the solar radiation on a cloudless day, and it depends on latitude and day of the year. $R_{bo}$ is given by . Rbo=Ïµâ€‰Ïƒâ€‰TKelvin4,R_{bo} = epsilon , sigma , T^4_ text{Kelvin},Rboâ€‹=ÏµÏƒTKelvin4â€‹, . where $ sigma=4.903 times 10^{-9}$ MJ m$^{-2}$ d$^{-1}$ K$^{-4}$, and $ epsilon$ is net net emissivity: . Ïµ=âˆ’0.02+0.261expâ¡(âˆ’7.77Ã—10âˆ’4TCelcius2). epsilon=-0.02+0.261 exp left(-7.77 times10^{-4}T_ text{Celcius}^2 right).Ïµ=âˆ’0.02+0.261exp(âˆ’7.77Ã—10âˆ’4TCelcius2â€‹). . The parameters $a$ and $b$ are determined for the climate of the area: . $a=1.0$, $b=0.0$ for humid areas, | $a=1.2$, $b=-0.2$ for arid areas, | $a=1.1$, $b=-0.1$ for semihumid areas. | . We can find below a table for $R_{so}$, from (Ward and Trimble, 2003), page 100. . Heat Flux Density to the Ground . The heat flux density to the ground $G$ (MJ m$^{-2} d^{-1}$) can be calculated using . G=4.2Ti+1âˆ’Tiâˆ’1Î”t,G = 4.2 frac{T_{i+1}-T_{i-1}}{ Delta t},G=4.2Î”tTi+1â€‹âˆ’Tiâˆ’1â€‹â€‹, . where $ Delta t$ is the time in days between midpoints of time periods $i+1$ and $iâˆ’1$, and $T$ is the air temperature (Â°C). This expression is really a finite differences implementation of a time derivative: . dTdt=limâ¡Î”tâ†’0T(t+Î”t)âˆ’T(tâˆ’Î”t)2Î”t. displaystyle frac{ text{d}T}{ text{d}t} = lim_{ Delta t rightarrow 0} frac{T(t+ Delta t) - T(t- Delta t)}{2 Delta t}.dtdTâ€‹=Î”tâ†’0limâ€‹2Î”tT(t+Î”t)âˆ’T(tâˆ’Î”t)â€‹. . Later on, we will take advantage of numpyâ€™s gradient function to calculate $G$. . Vapor Pressure . (Ward and Trimble, 2003), page 95. . The Vapor Pressure Deficit (VPD, in kPa) is the difference between saturation vapor pressure $e_s$ and actual vapor pressure $e_d$: . VPD=esâˆ’ed. text{VPD} = e_s - e_d.VPD=esâ€‹âˆ’edâ€‹. . For temperatures ranging from 0 to 50 Â°C, the saturation vapor pressure can be calculated with . es=expâ¡[16.78â€‰Tâˆ’116.9T+237.3],e_s = exp left[ frac{16.78 , T -116.9}{T+237.3} right],esâ€‹=exp[T+237.316.78Tâˆ’116.9â€‹], . and the actual vapor pressure is given by . ed=esRH100,e_d = e_s frac{RH}{100},edâ€‹=esâ€‹100RHâ€‹, . where $RH$ is the relative humidity (%), and the temperature $T$ in the equations above is in degrees Celcius. . We can see below a graph of $e_s(T)$ ((Ward and Trimble, 2003), page 96) . . The factor $ Delta$ is the slope of $e_s(T)$. See the figure below from Brutsaert, where the saturation vapor pressure is called $e^*$ ((Brutsaert, 2005), page 28)): . . There are a few ways of defining the function for $ Delta(T)$ (kPa Â°C$^{-1}$). (Ward and Trimble, 2003) give the following: . Î”=0.200â‹…(0.00738â€‰T+0.8072)7âˆ’0.000116, Delta = 0.200 cdot (0.00738 ,T + 0.8072)^7 - 0.000116,Î”=0.200â‹…(0.00738T+0.8072)7âˆ’0.000116, . while differentiating the exponential expression given before yields: . Î”=desdT=es(T)â‹…4098.79(T+237.3)2. Delta = frac{ text{d} e_s}{ text{d}T} = e_s(T) cdot frac{4098.79}{(T+237.3)^2}.Î”=dTdesâ€‹â€‹=esâ€‹(T)â‹…(T+237.3)24098.79â€‹. . Wind Function . (Ward and Trimble, 2003), page 108 . f(u)=0.26(1.0+0.54â€‰u2)f(u) = 0.26(1.0 + 0.54 , u_2)f(u)=0.26(1.0+0.54u2â€‹) . Meaning of â€œpotentialâ€ evapotranspiration . . Crop Coefficient . . Et=kcEtr,â€‰tpE_{t} = k_c E_{tr, , tp}Etâ€‹=kcâ€‹Etr,tpâ€‹ . $E_{t}=$ actual ET $k_c=$ crop coefficient $E_{tr}=$ reference crop ET $E_{tp}=$ potential ET . . Pitfalls . Different books and papers will present slightly different versions of the Penman equation. Basically, they differ in the units they use for the various components, and one should be vary aware of what inputs any given equation is expecting to get. . References . Hillel, D., 1998. Environmental Soil Physics. Academic Press. | Dingman, S.L., 2015. Physical Hydrology: Third Edition. Waveland Press. | Ward, A.D., Trimble, S.W., 2003. Environmental Hydrology, Second Edition. CRC Press. | Brutsaert, W., 2005. Hydrology: An Introduction. Cambridge University Press. |",
            "url": "https://yairmau.github.io/website/markdown/2020/02/03/evapotranspiration-lecture.html",
            "relUrl": "/markdown/2020/02/03/evapotranspiration-lecture.html",
            "date": " â€¢ Feb 3, 2020"
        }
        
    
  
    
        ,"post39": {
            "title": "Evapotranspiration - lecture",
            "content": "Dingman, &quot;Physical Hydrology&quot;, chapter 6. . Globally, about 62% of the precipitation that falls on the continents is evapotranspirated, amounting to 73 thousand km$^3$/yr. Of this, about 42% (29 thousand km$^3$/yr) is transpiration, and about 3% is open-water evaporation. Most of the remainder is interception loss; soil evaporation is a minor component of the total. . . . Potential Evapotranspiration . Potential Evapotranspiration (PET) is the rate at which evapotranspiration would occur from a large area completely and uniformly covered with growing vegetation with access to an unlimited supply of soil water and without advection or heat-storage effects. . Several characteristics of a vegetative surface have a strong influence on ET rate. a) the albedo of the surface, which determines the net radiation; b) the maximum leaf conductance; c) the atmospheric conductance, largely determined by vegetation height; d) presence or absence of intercepted water. . Reference-Crop Evapotranspiration . Reference-crop evapotranspiration (RET) is the amount of water transpired by a short green crop, completely shading the ground, of uniform height, and never short of water. . The magnitude of PET is often calculated from meteorological data collected under conditions in which the actual ET rate is less than the potential rate. If ET had been occurring at the potential rate, the latent- and sensible-heat exchanges between air and the surface, and hence the air temperature and humidity, would have been considerably different. (Brutsaert 1982) . . Review of methods . There are a variety of ways to estimate evaporative flux in nature. The following table categorizes each method based on the data that must be acquired to apply it: . . These methods also vary in the timescales in which they are relevant, typically in correlation with the variety of data needed: . Thornthwaite and SCS Blaney-Criddle: monthly or seasonal estimations (minimal data) | Jensen-Haise: 5-day estimates (good enough timescale and data for irrigation scheduling) | Penman: daily estimates | Penman-Monteith: hourly estimates (requires a lot of data) | . . Thornthwaite . Source: Ward &amp; Trimble, &quot;Environmental Hydrology&quot;, 2nd Edition, pages 107-108. . Thornthwaite (1948) developed an equation to predict monthly evapotranspiration from mean monthly tempera- ture and latitude data (Equation 4.27). The small amount of data needed is attractive because often ET needs to be predicted for sites where few weather data are available. Based on what we know about ET, we should be skeptical about the general applicability of such a simple equation. Thornthwaite (1948) was not satisfied with the proposed approach: â€œThe mathematical development is far from satisfactory. It is empirical. ... The chief obstacle at present to the development of a rational equation is the lack of understanding of why potential ET corresponding to a given temperature is not the same everywhere.â€ Taylor and Ashcroft (1972), as cited in Skaggs (1980), provided insight into the answer to Thornthwaiteâ€™s ques- tion. They said: . This equation, being based entirely upon a temperature relationship, has the disadvantage of a rather flimsy phys- ical basis and has only weak theoretical justification. Since temperature and vapor pressure gradients are mod- ified by the movement of air and by the heating of the soil and surroundings, the formula is not generally valid, but must be tested empirically whenever the climate is appreciably different from areas in which it has been tested. ... In spite of these shortcomings, the method has been widely used. Because it is based entirely on temper- ature data that are available in a large number of localities, it can be applied in situations where the basic data of the Penman method are not available. . M.E. Jensen et al. (1990) warn that Thornthwaiteâ€™s method is generally only applicable to areas that have climates similar to that of the east central U.S., and it is not applicable to arid and semiarid regions. . Thornthwaite (1948) found that evapotranspiration could be predicted from an equation of the form . $$ begin{equation} E = 16 left[ frac{10 ,T^ text{monthly mean}}{I} right]^a, end{equation} $$where $$ begin{equation} I = sum_{i=1}^{12} left[ frac{T_i^ text{monthly mean}}{5} right]^{1.514}, end{equation} $$ and $$ begin{align} a &amp;= 6.75 times 10^{-7}I^3 &amp;- 7.71 times 10^{-5}I^2 nonumber &amp;+ 1.792 times 10^{-2}I nonumber &amp;+ 0.49239 nonumber end{align} $$ . $E$ is the monthly potential ET (mm) | $T_ text{monthly mean}$ is the mean monthly temperature in Â°C | $I$ is a heat index | $a$ is a location-dependent coefficient | . . Penman . Sources: Brutsaert, &quot;Hydrology: An Introduction&quot;, pages 123-127. Ward &amp; Trimble, &quot;Environmental Hydrology&quot;, 2nd Edition, subsections 4.5.2, 4.5.3, 4.5.5, 4.6.6. Allen et al. (1998), &quot;Crop evapotranspiration - Guidelines for computing crop water requirements - FAO Irrigation and drainage paper 56&quot; . The Penman model is almost entirely a theory-based formula for predicting evaporative flux. It can run on a much finer timescale, and requires a much wider variety of data than most models. In addition to temperature, the Penman functions on measurements of radiation, wind speed, elevation above sea level, vapor-pressure deficit, and heat flux density to the ground. The potential ET (in mm d$^{-1}$) is given by: . $$ begin{equation} E = frac{1}{ lambda} left[ frac{ Delta}{ Delta+ gamma}Q_{ne}+ frac{ gamma}{ Delta+ gamma}E_A right], end{equation} $$where $Q_n$ is the available energy flux density . $$ begin{equation} Q_n = R_n - G, end{equation} $$and $E_A$ is the drying power of the air . $$ begin{equation} E_A = 6.43 cdot f(u) cdot text{VPD}. end{equation} $$The constituents of the equations above are . $E$: potential evapotranspiration (mm d$^{-1}$) | $ Delta$: slope of the saturation water vapor pressure curve (kPa Â°C$^{-1}$) | $ gamma$: psychrometric constant (kPA Â°C$^{-1}$) | $ lambda$: latent heat of vaporization (MJ kg$^{-1}$) | $R_n$: net radiation (MJ m$^{-2} d^{-1}$) | $G$: heat flux density to the ground (MJ m$^{-2} d^{-1}$) | $f(u)$: wind function (dimensionless) | VPD: vapor pressure deficit (kPa) | . and the number 6.43 adjusts the units of $E_A$ so it is in MJ m$^{-2} d^{-1}$. In what follows, we will further discuss these constituents. . Psychrometric Constant . The psychrometric constant $ gamma$ (kPA Â°C$^{-1}$) relates the partial pressure of water in air to the air temperature: . $$ begin{equation} gamma = frac{c_p , P}{ lambda cdot MW_ text{ratio}} end{equation} $$$$ begin{equation} P = 101.3-0.01055 H end{equation} $$ . $$ begin{equation} lambda = 2.501 - 2.361 times 10^{-3} ,T end{equation} $$ . $MW_ text{ratio}=0.622$: ratio molecular weight of water vapor/dry air | $P$: atmospheric pressure (kPa). Can be either measured or inferred from station height above sea level (m). | $ lambda$: latent heat of water vaporization (MJ kg$^{-1}$) | $c_p=0.001013$: specific heat capacity of moist air (MJ kg$^{-1}$ Â°C$^{-1}$) | . Net Radiation . Source: Ward &amp; Trimble, &quot;Environmental Hydrology&quot;, 2nd Edition, page 99. . $R_n$ (MJ m$^{-2} d^{-1}$) is net radiation, the balance between net short wave $R_s$ and the long wave $R_b$ components of the radiation: . $$R_n = (1- alpha)R_s ! ! downarrow -R_b ! ! uparrow,$$ . where $ alpha$ (dimensionless) is the albedo. The net outgoing thermal radiation $R_b$ is given by . $$R_b = left( a frac{R_s}{R_{so}+b} right)R_{bo},$$ . where $R_{so}$ is the solar radiation on a cloudless day, and it depends on latitude and day of the year. $R_{bo}$ is given by . $$R_{bo} = epsilon , sigma , T^4_{Kelvin},$$ . where $ sigma=4.903 times 10^{-9}$ MJ m$^{-2}$ d$^{-1}$ K$^{-4}$, and $ epsilon$ is net net emissivity: . $$ epsilon=-0.02+0.261 exp left(-7.77 times10^{-4}T_{Celcius}^2 right).$$ . The parameters $a$ and $b$ are determined for the climate of the area: . $a=1.0$, $b=0.0$ for humid areas, | $a=1.2$, $b=-0.2$ for arid areas, | $a=1.1$, $b=-0.1$ for semihumid areas. | . We can find below a table for $R_{so}$, from Ward &amp; Trimble, &quot;Environmental Hydrology&quot;, 2nd Edition, page 100. . Heat Flux Density to the Ground . The heat flux density to the ground $G$ (MJ m$^{-2} d^{-1}$) can be calculated using . $$ begin{equation} G = 4.2 frac{T_{i+1}-T_{i-1}}{ Delta t}, end{equation} $$where $ Delta t$ is the time in days between midpoints of time periods $i+1$ and $iâˆ’1$, and $T$ is the air temperature (Â°C). This expression is really a finite differences implementation of a time derivative: . $$ displaystyle frac{ text{d}T}{ text{d}t} = lim_{ Delta t rightarrow 0} frac{T(t+ Delta t) - T(t- Delta t)}{2 Delta t}. $$Later on, we will take advantage of numpy&#39;s gradient function to calculate $G$. . Vapor Pressure . from: Ward &amp; Trimble, &quot;Environmental Hydrology&quot;, 2nd Edition, page 95. . The Vapor Pressure Deficit (VPD, in kPa) is the difference between saturation vapor pressure $e_s$ and actual vapor pressure $e_d$: . $$ text{VPD} = e_s - e_d.$$ . For temperatures ranging from 0 to 50 Â°C, the saturation vapor pressure can be calculated with . $$ begin{equation} e_s = exp left[ frac{16.78 , T -116.9}{T+237.3} right], end{equation} $$and the actual vapor pressure is given by . $$ begin{equation} e_d = e_s frac{RH}{100}, end{equation} $$where $RH$ is the relative humidity (%), and the temperature $T$ in the equations above is in degrees Celcius. . We can see below a graph of $e_s(T)$ (Ward &amp; Trimble, &quot;Environmental Hydrology&quot;, 2nd Edition, page 96) . . The factor $ Delta$ is the slope of $e_s(T)$. See the figure below from Brutsaert, where the saturation vapor pressure is called $e^*$ (Brutsaert, &quot;Hydrology: An Introduction&quot;, page 28)): . . There are a few ways of defining the function for $ Delta(T)$ (kPa Â°C$^{-1}$). Ward &amp; Trimble give the following: . $$ begin{equation} Delta = 0.200 cdot (0.00738 ,T + 0.8072)^7 - 0.000116, end{equation} $$while differentiating the exponential expression given before yields: . $$ begin{equation} Delta = frac{ text{d} e_s}{ text{d}T} = e_s(T) cdot frac{4098.79}{(T+237.3)^2}. end{equation} $$ Wind Function . Source: (Ward &amp; Trimble, &quot;Environmental Hydrology&quot;, 2nd Edition, page 108) . $$ begin{equation} f(u) = 0.26(1.0 + 0.54 , u_2) end{equation} $$ Meaning of &quot;potential&quot; evapotranspiration . Crop Coefficient . $$ E_{t} = k_c E_{tr, , tp} $$ . $E_{t}=$ actual ET $k_c=$ crop coefficient $E_{tr}=$ reference crop ET $E_{tp}=$ potential ET . . Pitfalls . Different books and papers will present slightly different versions of the Penman equation. Basically, they differ in the units they use for the various components, and one should be vary aware of what inputs any given equation is expecting to get. .",
            "url": "https://yairmau.github.io/website/jupyter/2020/02/03/evapotranspiration-lecture.html",
            "relUrl": "/jupyter/2020/02/03/evapotranspiration-lecture.html",
            "date": " â€¢ Feb 3, 2020"
        }
        
    
  
    
        ,"post40": {
            "title": "Evapotranspiration - exercises",
            "content": "We will calculate the evapotranspiration using two methods: Thornthwaite and Penman. . Download data from the IMS . Go to the Israel Meteorological Service website, and download the following data: . hourly data on the first page, choose all options and press continue. | on the next page, choose the following date range: 01/01/2020 to 01/01/2021, then press continue. | Choose station Bet Dagan (×‘×™×ª ×“×’×Ÿ 2523), then select (×‘×—×¨), then continue. | Choose option &quot;by station&quot; (×œ×¤×™ ×ª×—× ×•×ª), then produce report. | Download report as csv, call it &quot;bet-dagan-3h.csv&quot;. | . | daily data on the first page, choose all options and press continue. | on the next page, choose the following date range: 01/01/2020 to 01/01/2021, then press continue. | Choose station Bet Dagan Meuyeshet (×‘×™×ª ×“×’×Ÿ ×××•×™×©×ª 2520), then select (×‘×—×¨), then continue. | Choose option &quot;by station&quot; (×œ×¤×™ ×ª×—× ×•×ª), then produce report. | Download report as csv, call it &quot;bet-dagan-day-pan.csv&quot;. | . | radiation data on the first page, choose all options, then on the bottom right option &quot;radiation&quot; (×§×¨×™× ×”), choose kJ/m2, and then press continue. | on the next page, choose the following date range: 01/01/2020 to 01/01/2021, then press continue. | Choose station Bet Dagan Krina (×‘×™×ª ×“×’×Ÿ ×§×¨×™× ×” 2524), then select (×‘×—×¨), then continue. | Choose option &quot;by station&quot; (×œ×¤×™ ×ª×—× ×•×ª), then produce report. | Download report as csv, call it &quot;bet-dagan-radiation.csv&quot;. | . | Import relevant packages . import matplotlib.pyplot as plt import matplotlib import numpy as np import pandas as pd from pandas.plotting import register_matplotlib_converters register_matplotlib_converters() # datetime converter for a matplotlib import seaborn as sns sns.set(style=&quot;ticks&quot;, font_scale=1.5) . . import hourly data . df = pd.read_csv(&#39;bet-dagan-3h.csv&#39;, encoding = &#39;unicode_escape&#39;, na_values=[&quot;-&quot;]) # find out what hebrew gibberish means: http://www.pixiesoft.com/flip/ name_conversion_dictionary = {&quot;Ã¹Ã­ ÃºÃ§Ã°Ã¤&quot;: &quot;station name&quot;, &quot;Ã®Ã±Ã´Ã¸ ÃºÃ§Ã°Ã¤&quot;: &quot;station number&quot;, &quot;ÃºÃ Ã¸Ã©Ãª&quot;: &quot;Date&quot;, &quot;Ã¹Ã²Ã¤-LST&quot;: &quot;LST time&quot;, &quot;Ã¨Ã®Ã´Ã¸Ã¨Ã¥Ã¸Ã¤(CÂ°)&quot;: &quot;T&quot;, &quot;Ã¨Ã®Ã´Ã¸Ã¨Ã¥Ã¸Ã¤ Ã¬Ã§Ã¤(CÂ°)&quot;: &quot;wet-bulb temperature (Â°C)&quot;, &quot;Ã¨Ã®Ã´Ã¸Ã¨Ã¥Ã¸Ãº Ã°Ã·Ã¥Ã£Ãº Ã¤Ã¨Ã¬(CÂ°)&quot;: &quot;dew_point_T&quot;, &quot;Ã¬Ã§Ã¥Ãº Ã©Ã§Ã±Ã©Ãº(%)&quot;: &quot;relative humidity (%)&quot;, &quot;Ã®Ã¤Ã©Ã¸Ã¥Ãº Ã¤Ã¸Ã¥Ã§(m/s)&quot;: &quot;wind_speed&quot;, &quot;Ã«Ã©Ã¥Ã¥Ã¯ Ã¤Ã¸Ã¥Ã§(Ã®Ã²Ã¬Ã¥Ãº)&quot;: &quot;wind direction (degrees)&quot;, &quot;Ã¬Ã§Ãµ Ã¡Ã¢Ã¥Ã¡Ã¤ Ã¤ÃºÃ§Ã°Ã¤(hPa)&quot;: &quot;Pressure&quot;, &quot;Ã¬Ã§Ãµ Ã¡Ã¢Ã¥Ã¡Ã¤ Ã´Ã°Ã© Ã¤Ã©Ã­(hPa)&quot;: &quot;pressure at sea level (hPa)&quot;, &quot;&quot;&quot;Ã¤ÃºÃ Ã£Ã¥Ãº Ã©Ã¥Ã®Ã©Ãº Ã®Ã¢Ã©Ã¢Ã©Ãº Ã±Ã¥Ã¢ Ã &#39;(Ã®&quot;Ã®)&quot;&quot;&quot;: &quot;pan evaporation (mm)&quot;, &quot;Ã±Ã¥Ã¢ Ã·Ã¸Ã©Ã°Ã¤()&quot;: &quot;radiation type&quot;, } # units # T = temperature (Â°C) # dew_point_T = dew point temperature (Â°C) # wind_speed = wind speed (m/s) # Pressure = pressure at station height (hPa = 0.1 kPa) df = df.rename(columns=name_conversion_dictionary) df[&#39;timestamp&#39;] = df[&#39;Date&#39;] + &#39; &#39; + df[&#39;LST time&#39;] df[&#39;timestamp&#39;] = pd.to_datetime(df[&#39;timestamp&#39;], dayfirst=True) df = df.set_index(&#39;timestamp&#39;) df . . station name station number Date LST time T wet-bulb temperature (Â°C) dew_point_T relative humidity (%) wind_speed wind direction (degrees) ... pressure at sea level (hPa) Ã«Ã®Ã¥Ãº Ã²Ã°Ã°Ã©Ã­ Ã«Ã¥Ã¬Ã¬Ãº(Ã·Ã¥Ã£) Ã«Ã®Ã¥Ãº Ã²Ã°Ã°Ã©Ã­ Ã°Ã®Ã¥Ã«Ã©Ã­(Ã·Ã¥Ã£) Ã¢Ã¥Ã¡Ã¤ Ã¡Ã±Ã©Ã± Ã²Ã°Ã°Ã©Ã­ Ã°Ã®Ã¥Ã«Ã©Ã­(Ã·Ã¥Ã£) Ã±Ã¥Ã¢ Ã¤Ã²Ã°Ã°Ã©Ã­ Ã¤Ã°Ã®Ã¥Ã«Ã©Ã­(Ã·Ã¥Ã£) Ã±Ã¥Ã¢ Ã¤Ã²Ã°Ã°Ã©Ã­ Ã¤Ã¡Ã©Ã°Ã¥Ã°Ã©Ã©Ã­(Ã·Ã¥Ã£) Ã±Ã¥Ã¢ Ã¤Ã²Ã°Ã°Ã©Ã­ Ã¤Ã¢Ã¡Ã¥Ã¤Ã©Ã­(Ã·Ã¥Ã£) Ã®Ã¦Ã¢ Ã Ã¥Ã¥Ã©Ã¸ Ã°Ã¥Ã«Ã§Ã©(Ã·Ã¥Ã£) Ã®Ã¦Ã¢ Ã Ã¥Ã¥Ã©Ã¸ Ã¹Ã§Ã¬Ã³(Ã·Ã¥Ã£) Ã¸Ã Ã¥Ãº Ã Ã´Ã·Ã©Ãº(Ã·Ã¥Ã£) . timestamp . 2020-01-01 02:00:00 Ã¡Ã©Ãº Ã£Ã¢Ã¯ ... | 2523 | 01-01-2020 | 02:00 | 7.9 | 7.2 | 6.4 | 90 | 1.7 | 117.0 | ... | 1018.8 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2020-01-01 05:00:00 Ã¡Ã©Ãº Ã£Ã¢Ã¯ ... | 2523 | 01-01-2020 | 05:00 | 7.5 | 7.0 | 6.4 | 93 | 1.2 | 116.0 | ... | 1018.1 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2020-01-01 08:00:00 Ã¡Ã©Ãº Ã£Ã¢Ã¯ ... | 2523 | 01-01-2020 | 08:00 | 8.6 | 8.3 | 8.0 | 96 | 1.1 | 107.0 | ... | 1018.2 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2020-01-01 11:00:00 Ã¡Ã©Ãº Ã£Ã¢Ã¯ ... | 2523 | 01-01-2020 | 11:00 | 15.9 | 13.1 | 10.6 | 71 | 2.4 | 196.0 | ... | 1017.4 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2020-01-01 14:00:00 Ã¡Ã©Ãº Ã£Ã¢Ã¯ ... | 2523 | 01-01-2020 | 14:00 | 18.1 | 14.0 | 10.4 | 61 | 2.8 | 264.0 | ... | 1015.3 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2021-01-31 11:00:00 Ã¡Ã©Ãº Ã£Ã¢Ã¯ ... | 2523 | 31-01-2021 | 11:00 | 19.0 | 13.7 | 8.9 | 52 | 5.6 | 235.0 | ... | 1017.3 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2021-01-31 14:00:00 Ã¡Ã©Ãº Ã£Ã¢Ã¯ ... | 2523 | 31-01-2021 | 14:00 | 19.2 | 14.7 | 11.0 | 59 | 4.6 | 252.0 | ... | 1016.7 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2021-01-31 17:00:00 Ã¡Ã©Ãº Ã£Ã¢Ã¯ ... | 2523 | 31-01-2021 | 17:00 | 18.2 | 14.8 | 12.2 | 68 | 0.8 | 203.0 | ... | 1017.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2021-01-31 20:00:00 Ã¡Ã©Ãº Ã£Ã¢Ã¯ ... | 2523 | 31-01-2021 | 20:00 | 13.1 | 12.3 | 11.7 | 91 | 1.2 | 79.0 | ... | 1018.2 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2021-01-31 23:00:00 Ã¡Ã©Ãº Ã£Ã¢Ã¯ ... | 2523 | 31-01-2021 | 23:00 | 10.8 | 10.6 | 10.3 | 97 | 1.7 | 111.0 | ... | 1018.9 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3172 rows Ã— 21 columns . import daily data with pan evaporation . df2 = pd.read_csv(&#39;bet-dagan-day-pan.csv&#39;, encoding = &#39;unicode_escape&#39;, na_values=[&quot;-&quot;]) df2 = df2.rename(columns=name_conversion_dictionary) df2[&#39;Date&#39;] = pd.to_datetime(df2[&#39;Date&#39;], dayfirst=True) df2 = df2.set_index(&#39;Date&#39;) df2 . . station name station number Ã¨Ã®Ã´Ã¸Ã¨Ã¥Ã¸Ãº Ã®Ã·Ã±Ã©Ã®Ã¥Ã­(CÂ°) Ã¨Ã®Ã´Ã¸Ã¨Ã¥Ã¸Ãº Ã®Ã©Ã°Ã©Ã®Ã¥Ã­(CÂ°) Ã¨Ã®Ã´Ã¸Ã¨Ã¥Ã¸Ãº Ã®Ã©Ã°Ã©Ã®Ã¥Ã­ Ã¬Ã©Ã£ Ã¤Ã·Ã¸Ã·Ã²(CÂ°) Ã®Ã¹Ãª Ã¦Ã¤Ã©Ã¸Ãº Ã¹Ã®Ã¹(Ã£Ã·Ã¥Ãº) pan evaporation (mm) Ã·Ã¥Ã£ Ã¤ÃºÃ Ã£Ã¥Ãº Ã©Ã¥Ã®Ã©Ãº() . Date . 2020-01-01 Ã¡Ã©Ãº Ã£Ã¢Ã¯ Ã®Ã Ã¥Ã©Ã¹Ãº ... | 2520 | NaN | NaN | NaN | NaN | 0.8 | 0.0 | . 2020-01-02 Ã¡Ã©Ãº Ã£Ã¢Ã¯ Ã®Ã Ã¥Ã©Ã¹Ãº ... | 2520 | NaN | NaN | NaN | NaN | NaN | NaN | . 2020-01-03 Ã¡Ã©Ãº Ã£Ã¢Ã¯ Ã®Ã Ã¥Ã©Ã¹Ãº ... | 2520 | NaN | NaN | NaN | NaN | NaN | NaN | . 2020-01-04 Ã¡Ã©Ãº Ã£Ã¢Ã¯ Ã®Ã Ã¥Ã©Ã¹Ãº ... | 2520 | NaN | NaN | NaN | NaN | NaN | NaN | . 2020-01-05 Ã¡Ã©Ãº Ã£Ã¢Ã¯ Ã®Ã Ã¥Ã©Ã¹Ãº ... | 2520 | NaN | NaN | NaN | NaN | 2.4 | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 2021-01-27 Ã¡Ã©Ãº Ã£Ã¢Ã¯ Ã®Ã Ã¥Ã©Ã¹Ãº ... | 2520 | NaN | NaN | NaN | NaN | 2.5 | 0.0 | . 2021-01-28 Ã¡Ã©Ãº Ã£Ã¢Ã¯ Ã®Ã Ã¥Ã©Ã¹Ãº ... | 2520 | NaN | NaN | NaN | NaN | 1.2 | 0.0 | . 2021-01-29 Ã¡Ã©Ãº Ã£Ã¢Ã¯ Ã®Ã Ã¥Ã©Ã¹Ãº ... | 2520 | NaN | NaN | NaN | NaN | NaN | NaN | . 2021-01-30 Ã¡Ã©Ãº Ã£Ã¢Ã¯ Ã®Ã Ã¥Ã©Ã¹Ãº ... | 2520 | NaN | NaN | NaN | NaN | NaN | NaN | . 2021-01-31 Ã¡Ã©Ãº Ã£Ã¢Ã¯ Ã®Ã Ã¥Ã©Ã¹Ãº ... | 2520 | NaN | NaN | NaN | NaN | 2.6 | 0.0 | . 397 rows Ã— 8 columns . import daily data with radiation . df3 = pd.read_csv(&#39;bet-dagan-radiation.csv&#39;, encoding = &#39;unicode_escape&#39;, na_values=[&quot;-&quot;]) df3 = df3.rename(columns=name_conversion_dictionary) df3[&#39;Date&#39;] = pd.to_datetime(df3[&#39;Date&#39;], dayfirst=True) df3 = df3.set_index(&#39;Date&#39;) df3 = df3.replace({&quot;Ã©Ã¹Ã©Ã¸Ã¤&quot;: &quot;direct&quot;, &quot;Ã®Ã´Ã¥Ã¦Ã¸Ãº&quot;: &quot;diffuse&quot;, &quot;Ã¢Ã¬Ã¥Ã¡Ã Ã¬Ã©Ãº&quot;: &quot;global&quot;}) df3[&#39;daily_radiation_MJ_per_m2_per_day&#39;] = df3.iloc[:, 3:].sum(axis=1)/1000 df_radiation = df3.loc[df3[&quot;radiation type&quot;] == &quot;global&quot;, &quot;daily_radiation_MJ_per_m2_per_day&quot;].to_frame() df_radiation . . daily_radiation_MJ_per_m2_per_day . Date . 2020-01-01 10.0296 | . 2020-01-02 4.3128 | . 2020-01-03 11.6748 | . 2020-01-04 1.6452 | . 2020-01-05 6.8544 | . ... ... | . 2021-01-27 12.2652 | . 2021-01-28 7.1640 | . 2021-01-29 7.2936 | . 2021-01-30 9.3276 | . 2021-01-31 13.5468 | . 396 rows Ã— 1 columns . Part 1: Thornthwaite estimation . $$ E = 16 left[ frac{10 ,T^ text{monthly mean}}{I} right]^a, $$where . $$ I = sum_{i=1}^{12} left[ frac{T_i^ text{monthly mean}}{5} right]^{1.514}, $$and . $$ begin{align} a &amp;= 6.75 times 10^{-7}I^3 &amp;- 7.71 times 10^{-5}I^2 nonumber &amp;+ 1.792 times 10^{-2}I nonumber &amp;+ 0.49239 nonumber end{align} $$ $E$ is the monthly potential ET (mm) | $T_ text{monthly mean}$ is the mean monthly temperature in Â°C | $I$ is a heat index | $a$ is a location-dependent coefficient | . From df, make a new dataframe, df_th, that stores monthly temperatures means. Use resample function. . # monthly data df_th = (df[&#39;T&#39;].resample(&#39;MS&#39;) # MS assigns mean to first day in the month .mean() .to_frame() ) # we now add 14 days to the index, so that all monthly data is in the middle of the month # not really necessary, makes plot look better df_th.index = df_th.index + pd.DateOffset(days=14) df_th . . T . timestamp . 2020-01-15 12.484274 | . 2020-02-15 14.046983 | . 2020-03-15 16.439113 | . 2020-04-15 18.512500 | . 2020-05-15 23.166532 | . 2020-06-15 24.600000 | . 2020-07-15 27.353226 | . 2020-08-15 28.090323 | . 2020-09-15 28.462500 | . 2020-10-15 25.120161 | . 2020-11-15 19.308475 | . 2020-12-15 15.916129 | . 2021-01-15 14.123790 | . Calculate $I$, then $a$, and finally $E_p$. Add $E_p$ as a new column in df_th. . # Preparing &quot;I&quot; for the Thornthwaite equation I = np.sum( (df_th[&#39;T&#39;]/5)**(1.514) ) # Preparing &quot;a&quot; for the Thornthwaite equation a = (+6.75e-7 * I**3 -7.71e-5 * I**2 +1.792e-2 * I + 0.49239) # The final Thornthwaite model for monthly potential ET (mm) df_th[&#39;Ep&#39;] = 16*((10*df_th[&#39;T&#39;]/I)**a) df_th . . T Ep . timestamp . 2020-01-15 12.484274 | 20.163427 | . 2020-02-15 14.046983 | 27.179636 | . 2020-03-15 16.439113 | 40.472053 | . 2020-04-15 18.512500 | 54.671821 | . 2020-05-15 23.166532 | 96.461219 | . 2020-06-15 24.600000 | 112.296873 | . 2020-07-15 27.353226 | 146.898516 | . 2020-08-15 28.090323 | 157.128632 | . 2020-09-15 28.462500 | 162.453109 | . 2020-10-15 25.120161 | 118.406386 | . 2020-11-15 19.308475 | 60.820862 | . 2020-12-15 15.916129 | 37.291178 | . 2021-01-15 14.123790 | 27.557481 | . Plot the Thornthwaite ET that you calculated. . fig, ax = plt.subplots(1, figsize=(10,7)) ax.plot(df_th[&#39;Ep&#39;]) ax.set(xlabel=&quot;date&quot;, ylabel=r&quot;$E_p$ (mm)&quot;, title=&quot;Thornthwaite potential evapotranspiration&quot;); . . Part 2: Penman . The Penman model is almost entirely a theory based formula for predicting evaporative flux. It can run on a much finer timescale, and requires a much wider variety of data than most models. In addition to temperature, the Penman functions on measurements of radiation, wind speed, elevation above sea level, vapour-pressure deficit, and heat flux density to the ground. . $$ E = frac{1}{ lambda} left[ frac{ Delta}{ Delta+ gamma}Q_{ne}+ frac{ gamma}{ Delta+ gamma}E_A right], $$where $Q_n$ is the available energy flux density . $$ Q_n = R_n - G, $$and $E_A$ is the drying power of the air . $$ E_A = 6.43 cdot f(u) cdot text{VPD}. $$$$ gamma = frac{c_p , P}{ lambda cdot MW_ text{ratio}} $$$$ P = 101.3-0.01055 H $$$$ lambda = 2.501 - 2.361 times 10^{-3} ,T $$ $MW_ text{ratio}=0.622$: ratio molecular weight of water vapor/dry air | $P$: atmospheric pressure (kPa). Can be either measured or inferred from station height above sea level (m). | $ lambda$: latent heat of water vaporization (MJ kg$^{-1}$) | . $$ R_n = (1- alpha)R_s ! ! downarrow -R_b ! ! uparrow, $$where $ alpha$ (dimensionless) is the albedo. The net outgoing thermal radiation $R_b$ is given by . $$ R_b = left( a frac{R_s}{R_{so}+b} right)R_{bo}, $$where $R_{so}$ is the solar radiation on a cloudless day, and it depends on latitude and day of the year. $R_{bo}$ is given by . $$ R_{bo} = epsilon , sigma , T^4_{Kelvin}, $$where $ sigma=4.903 times 10^{-9}$ MJ m$^{-2}$ d$^{-1}$ K$^{-4}$, and $ epsilon$ is net net emissivity: . $$ epsilon=-0.02+0.261 exp left(-7.77 times10^{-4}T_{Celcius}^2 right). $$The parameters $a$ and $b$ are determined for the climate of the area: . $a=1.0$, $b=0.0$ for humid areas, | $a=1.2$, $b=-0.2$ for arid areas, | $a=1.1$, $b=-0.1$ for semihumid areas. | . $$ G = 4.2 frac{T_{i+1}-T_{i-1}}{ Delta t} $$$$ text{VPD} = e_s - e_d. $$For temperatures ranging from 0 to 50 Â°C, the saturation vapor pressure can be calculated with . $$ e_s = exp left[ frac{16.78 , T -116.9}{T+237.3} right], $$and the actual vapor pressure is given by . $$ e_d = e_s frac{RH}{100}, $$$$ Delta = frac{ text{d} e_s}{ text{d}T} = e_s(T) cdot frac{4098.79}{(T+237.3)^2}. $$$$ f(u) = 0.26(1.0 + 0.54 , u_2) $$The various components of the equations above are: . $$ Delta = 0.200 cdot (0.00738 ,T + 0.8072)^7 - 0.000116 $$ . $$ gamma = frac{c_p , P}{0.622 lambda} $$ . $$ P = 101.3-0.01055 H $$ . $$ lambda = 2.501 - 2.361 times 10^{-3} ,T $$ . $$ f_e(u) = 1.0 + 0.53 , u_2 $$ . $$ G = 4.2 frac{T_{i+1}-T_{i-1}}{ Delta t} $$ . $$ e_s = exp left[ frac{16.78 , T -116.9}{T+237.3} right] $$ . $$ e_d = e_s frac{RH}{100} $$ where $ Delta t$ is the time in days between midpoints of time periods $i+1$ and $iâˆ’1$, and $T$ is the air temperature (Â°C). . $ Delta$: slope of the saturation water vapor pressure curve (kPa Â°C$^{-1}$) | $ gamma$: psychrometric constant (kPA Â°C$^{-1}$) | $c_p=0.001013$: specific heat of water at constant pressure (MJ kg$^{-1}$ Â°C$^{-1}$) | $P$: atmospheric pressure (kPa) | $H$: elevation above sea level (m) | $ lambda$: latent heat of vaporization (MJ kg$^{-1}$) | $R_n$: net radiation (MJ m$^{-2} d^{-1}$) | $G$: heat flux density to the ground (MJ m$^{-2} d^{-1}$) | $u_{2}$: wind speed measured 2 m above ground (m s$^{-1}$) | $e_{s} - e_{d}$: vapor pressure deficit (kPa) | $e_{s}$: saturation vapor pressure (kPa) | $e_{d}$: actual vapor pressure (kPa) | . Calculate daily means for the following columns: temperature T, wind speed wind_speed, atmospheric pressure Pressure, and relative humidity relative humidity (%). Remember that pressure data was given in hectopascal, 1 hPa = 0.1 kPa. Store all the calculated values in a new dataframe, called df_pen. . # Resampling hourly data over same day and taking mean, to obtain daily averages df_pen = (df[&#39;T&#39;].resample(&#39;D&#39;) .mean() .to_frame() ) df_pen[&#39;dew_point&#39;] = (df[&#39;dew_point_T&#39;].resample(&#39;D&#39;) .mean() ) df_pen[&#39;u&#39;] = (df[&#39;wind_speed&#39;].resample(&#39;D&#39;) .mean() ) df_pen[&#39;P&#39;] = (df[&#39;Pressure&#39;].resample(&#39;D&#39;) .mean() )/10 df_pen[&#39;RH&#39;] = (df[&#39;relative humidity (%)&#39;].resample(&#39;D&#39;) .mean() ) df_pen . . T dew_point u P RH . timestamp . 2020-01-01 12.3625 | 9.0625 | 1.5250 | 101.30875 | 81.500 | . 2020-01-02 11.9750 | 9.8250 | 1.9250 | 101.20125 | 87.000 | . 2020-01-03 13.0500 | 4.9750 | 5.1750 | 101.37125 | 58.500 | . 2020-01-04 10.8625 | 6.6875 | 5.5625 | 101.15500 | 78.375 | . 2020-01-05 12.9375 | 9.2125 | 4.5625 | 101.23625 | 79.125 | . ... ... | ... | ... | ... | ... | . 2021-01-27 13.8125 | 8.2375 | 1.8875 | 100.83750 | 72.375 | . 2021-01-28 14.4000 | 10.2250 | 3.5250 | 101.12750 | 76.750 | . 2021-01-29 12.3500 | 7.9125 | 5.0250 | 101.22125 | 75.250 | . 2021-01-30 12.9625 | 7.6500 | 4.4250 | 101.49500 | 71.375 | . 2021-01-31 15.0625 | 8.3125 | 3.7500 | 101.32500 | 66.000 | . 397 rows Ã— 5 columns . With average $T$ for every day of the year, we can now calculate daily latent heat of vaporization $ lambda$, the slope of the saturation-vapor pressure-temperature curve $ Delta$, and the heat flux density to the ground $G$. Add each of these to dataframe df_pen. . Calculate also the wind function using the data for wind speed, and add this to df_pen. . def lambda_latent_heat(T): &quot;&quot;&quot;daily latent heat of vaporization (MJ/kg)&quot;&quot;&quot; return 2.501 - 2.361e-3*T def Delta(T): &quot;&quot;&quot;slope of saturation-vapor curve (kPa/Â°C)&quot;&quot;&quot; return 0.2000*(0.00738*T + 0.8072)**7 - 0.000116 def G(T): &quot;&quot;&quot;heat flux density to the ground, G (MJ/m2/d)&quot;&quot;&quot; return 4.2*np.gradient(T.values) cp = 0.001013 # (MJ kgâˆ’1 Â°Câˆ’1) df_pen[&#39;lambda&#39;] = lambda_latent_heat(df_pen[&#39;T&#39;]) df_pen[&#39;Delta&#39;] = Delta(df_pen[&#39;T&#39;]) df_pen[&#39;G&#39;] = G(df_pen[&#39;T&#39;]) df_pen[&#39;gamma&#39;] = (cp*df_pen[&#39;P&#39;])/(0.622*df_pen[&#39;lambda&#39;]) df_pen[&#39;f_wind&#39;] = 1.0 + 0.53 * df_pen[&#39;u&#39;] df_pen . . T dew_point u P RH lambda Delta G gamma f_wind . timestamp . 2020-01-01 12.3625 | 9.0625 | 1.5250 | 101.30875 | 81.500 | 2.471812 | 0.094385 | -1.62750 | 0.066750 | 1.808250 | . 2020-01-02 11.9750 | 9.8250 | 1.9250 | 101.20125 | 87.000 | 2.472727 | 0.092300 | 1.44375 | 0.066654 | 2.020250 | . 2020-01-03 13.0500 | 4.9750 | 5.1750 | 101.37125 | 58.500 | 2.470189 | 0.098185 | -2.33625 | 0.066835 | 3.742750 | . 2020-01-04 10.8625 | 6.6875 | 5.5625 | 101.15500 | 78.375 | 2.475354 | 0.086530 | -0.23625 | 0.066553 | 3.948125 | . 2020-01-05 12.9375 | 9.2125 | 4.5625 | 101.23625 | 79.125 | 2.470455 | 0.097554 | 4.25250 | 0.066739 | 3.418125 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2021-01-27 13.8125 | 8.2375 | 1.8875 | 100.83750 | 72.375 | 2.468389 | 0.102551 | 4.77750 | 0.066532 | 2.000375 | . 2021-01-28 14.4000 | 10.2250 | 3.5250 | 101.12750 | 76.750 | 2.467002 | 0.106028 | -3.07125 | 0.066760 | 2.868250 | . 2021-01-29 12.3500 | 7.9125 | 5.0250 | 101.22125 | 75.250 | 2.471842 | 0.094317 | -3.01875 | 0.066691 | 3.663250 | . 2021-01-30 12.9625 | 7.6500 | 4.4250 | 101.49500 | 71.375 | 2.470396 | 0.097694 | 5.69625 | 0.066911 | 3.345250 | . 2021-01-31 15.0625 | 8.3125 | 3.7500 | 101.32500 | 66.000 | 2.465437 | 0.110070 | 8.82000 | 0.066933 | 2.987500 | . 397 rows Ã— 10 columns . It&#39;s time to calculate net radiation $R_n$. The monthly mean solar radiation $R_{so}$ for latitude 30 degrees is [17.46, 21.65, 25.96, 29.85, 32.11, 33.20, 32.66, 30.44, 26.67, 22.48, 18.30, 16.04] (MJ m$^{-2}$ d$^{-1}$) (Israel&#39;s latitude is ~ 31 degrees). . Add a new column Rso_monthly to df_pen, where each day has the appropriate $R_{so}$ given by the data above. | Add a new columns Rs with the global radiation data imported in the 3rd file. | . # Rso: mean solar radiation from a cloudless sky (based on latitude) # MJ/m2/d Rso_monthly = np.array([17.46, 21.65, 25.96, 29.85, 32.11, 33.20, 32.66, 30.44, 26.67, 22.48, 18.30, 16.04]) # create empty columns df_pen[&quot;Rso_monthly&quot;] = &quot;&quot; # every day in the month will have the same values for Rso for i in range(12): df_pen.loc[df_pen.index.month==(i+1), &quot;Rso_monthly&quot;] = Rso_monthly[i] df_pen[&quot;Rs&quot;] = df_radiation[&quot;daily_radiation_MJ_per_m2_per_day&quot;] fig, ax = plt.subplots(1, figsize=(10,7)) ax.plot(df_pen[&#39;Rso_monthly&#39;]) plt.gcf().autofmt_xdate() ax.set_ylabel(r&quot;$R_{so}$ (MJ m$^{-2} d^{-1}$)&quot;) . . Text(0, 0.5, &#39;$R_{so}$ (MJ m$^{-2} d^{-1}$)&#39;) . middle = pd.date_range(start=&#39;1/1/2020&#39;, periods=13, freq=&#39;MS&#39;) + pd.DateOffset(days=14) new = df_pen.loc[middle, &#39;Rso_monthly&#39;].astype(&#39;float&#39;) new df_i = (pd.DataFrame(data=new, index=new.index) #create the dataframe .resample(&quot;D&quot;) #resample daily .interpolate(method=&#39;time&#39;) #interpolate by time ) fig, ax = plt.subplots(1, figsize=(10,7)) ax.plot(df_i, &#39;o&#39;) ax.plot(df_pen[&#39;Rso_monthly&#39;]) plt.gcf().autofmt_xdate() ax.set_title(&quot;time interpolation&quot;) ax.set_ylabel(r&quot;$R_{so}$ (MJ m$^{-2} d^{-1}$)&quot;) . . Text(0, 0.5, &#39;$R_{so}$ (MJ m$^{-2} d^{-1}$)&#39;) . from: Ward &amp; Trimble, &quot;Environmental Hydrology&quot;, 2nd Edition, page 99. . Calculate $$ R_{bo} = epsilon , sigma , T^4_{Kelvin}, $$ where $$ epsilon=-0.02+0.261 exp left(-7.77 times10^{-4}T_{Celcius}^2 right), $$ $$ sigma=4.903 times 10^{-9} text{ MJ m$^{-2}$ d$^{-1}$ K$^{-4}$}, $$ and $$ T_{Kelvin}=T_{Celcius}+273.15 $$ | Calculate $$ R_b = left( a frac{R_s}{R_{so}+b} right)R_{bo}, $$ where for humid areas, $a=1.0$ and $b=0$, | for arid areas, $a=1.2$ and $b=-0.2$, | for semihumid areas, $a=1.1$ and $b=-0.1$ | . | Finally, calculate $$ R_n = (1- alpha)R_s ! ! downarrow -R_b ! ! uparrow, $$ where $ alpha= 0.23$ for most green crops with a full cover | $ alpha= 0.04$ for fresh asphalt | $ alpha= 0.12$ for worn-out asphalt | $ alpha= 0.55$ for fresh concrete | . | . Add a new column Rn to df_pen dataframe. . # Stefan-Boltzmann constant sigma = 4.903e-9 emissivity = -0.02 + 0.261 * np.exp(-7.77e-4 * df_pen[&#39;T&#39;]**2) # Rbo: net longwave radiation for clear skies, otherwise known as diffuse radiation or emitted radiation from the # atmosphere - &#39;how hot is it?&#39; Rbo = emissivity*sigma*((df_pen[&#39;T&#39;]+273.15)**4) # net outgoing long-wave radiation (note: Rs/Rso = proportion of how clear the day is) # for humid areas, a=1.0 and b=0 # for arid areas, a=1.2 and b=-0.2 # for semihumid areas, a=1.1 and b=-0.1 a = 1.2 b = -0.2 Rb = (a*df_pen[&#39;Rs&#39;]/df_pen[&#39;Rso_monthly&#39;] + b)*Rbo # Î± is the albedo, or short-wave reflectance (dimensionless) alpha = 0.23 # net radiation Rn = (1 - alpha) * df_pen[&#39;Rs&#39;] - Rb # (MJ/m2/d) df_pen[&#39;Rn&#39;] = Rn df_pen . . T dew_point u P RH lambda Delta G gamma f_wind Rso_monthly Rs Rn . timestamp . 2020-01-01 12.3625 | 9.0625 | 1.5250 | 101.30875 | 81.500 | 2.471812 | 0.094385 | -1.62750 | 0.066750 | 1.808250 | 17.46 | 10.0296 | 4.346568 | . 2020-01-02 11.9750 | 9.8250 | 1.9250 | 101.20125 | 87.000 | 2.472727 | 0.092300 | 1.44375 | 0.066654 | 2.020250 | 17.46 | 4.3128 | 2.653905 | . 2020-01-03 13.0500 | 4.9750 | 5.1750 | 101.37125 | 58.500 | 2.470189 | 0.098185 | -2.33625 | 0.066835 | 3.742750 | 17.46 | 11.6748 | 4.854942 | . 2020-01-04 10.8625 | 6.6875 | 5.5625 | 101.15500 | 78.375 | 2.475354 | 0.086530 | -0.23625 | 0.066553 | 3.948125 | 17.46 | 1.6452 | 1.871722 | . 2020-01-05 12.9375 | 9.2125 | 4.5625 | 101.23625 | 79.125 | 2.470455 | 0.097554 | 4.25250 | 0.066739 | 3.418125 | 17.46 | 6.8544 | 3.415474 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2021-01-27 13.8125 | 8.2375 | 1.8875 | 100.83750 | 72.375 | 2.468389 | 0.102551 | 4.77750 | 0.066532 | 2.000375 | 17.46 | 12.2652 | 5.060995 | . 2021-01-28 14.4000 | 10.2250 | 3.5250 | 101.12750 | 76.750 | 2.467002 | 0.106028 | -3.07125 | 0.066760 | 2.868250 | 17.46 | 7.1640 | 3.534995 | . 2021-01-29 12.3500 | 7.9125 | 5.0250 | 101.22125 | 75.250 | 2.471842 | 0.094317 | -3.01875 | 0.066691 | 3.663250 | 17.46 | 7.2936 | 3.537119 | . 2021-01-30 12.9625 | 7.6500 | 4.4250 | 101.49500 | 71.375 | 2.470396 | 0.097694 | 5.69625 | 0.066911 | 3.345250 | 17.46 | 9.3276 | 4.152687 | . 2021-01-31 15.0625 | 8.3125 | 3.7500 | 101.32500 | 66.000 | 2.465437 | 0.110070 | 8.82000 | 0.066933 | 2.987500 | 17.46 | 13.5468 | 5.513874 | . 397 rows Ã— 13 columns . Calculate the vapor pressure deficit, VPD, add a new column to df_pen. . $$ e_d = e_s cdot frac{RH}{100} $$$$ e_s = exp left( frac{16.78 ,T-116.9}{T+237.3} right) $$ # vapor pressure deficit = VPD def vp_sat(T): return np.exp((16.78*T - 116.9)/(T + 237.3)) df_pen[&#39;es&#39;] = vp_sat(df_pen[&#39;T&#39;]) df_pen[&#39;ed&#39;] = df_pen[&#39;es&#39;] * df_pen[&#39;RH&#39;] / 100 df_pen[&#39;VPD&#39;] = df_pen[&#39;es&#39;] - df_pen[&#39;ed&#39;] df_pen . . T dew_point u P RH lambda Delta G gamma f_wind Rso_monthly Rs Rn es ed VPD . timestamp . 2020-01-01 12.3625 | 9.0625 | 1.5250 | 101.30875 | 81.500 | 2.471812 | 0.094385 | -1.62750 | 0.066750 | 1.808250 | 17.46 | 10.0296 | 4.346568 | 1.437148 | 1.171276 | 0.265872 | . 2020-01-02 11.9750 | 9.8250 | 1.9250 | 101.20125 | 87.000 | 2.472727 | 0.092300 | 1.44375 | 0.066654 | 2.020250 | 17.46 | 4.3128 | 2.653905 | 1.400935 | 1.218813 | 0.182122 | . 2020-01-03 13.0500 | 4.9750 | 5.1750 | 101.37125 | 58.500 | 2.470189 | 0.098185 | -2.33625 | 0.066835 | 3.742750 | 17.46 | 11.6748 | 4.854942 | 1.503424 | 0.879503 | 0.623921 | . 2020-01-04 10.8625 | 6.6875 | 5.5625 | 101.15500 | 78.375 | 2.475354 | 0.086530 | -0.23625 | 0.066553 | 3.948125 | 17.46 | 1.6452 | 1.871722 | 1.301383 | 1.019959 | 0.281424 | . 2020-01-05 12.9375 | 9.2125 | 4.5625 | 101.23625 | 79.125 | 2.470455 | 0.097554 | 4.25250 | 0.066739 | 3.418125 | 17.46 | 6.8544 | 3.415474 | 1.492399 | 1.180860 | 0.311538 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2021-01-27 13.8125 | 8.2375 | 1.8875 | 100.83750 | 72.375 | 2.468389 | 0.102551 | 4.77750 | 0.066532 | 2.000375 | 17.46 | 12.2652 | 5.060995 | 1.580054 | 1.143564 | 0.436490 | . 2021-01-28 14.4000 | 10.2250 | 3.5250 | 101.12750 | 76.750 | 2.467002 | 0.106028 | -3.07125 | 0.066760 | 2.868250 | 17.46 | 7.1640 | 3.534995 | 1.641414 | 1.259785 | 0.381629 | . 2021-01-29 12.3500 | 7.9125 | 5.0250 | 101.22125 | 75.250 | 2.471842 | 0.094317 | -3.01875 | 0.066691 | 3.663250 | 17.46 | 7.2936 | 3.537119 | 1.435967 | 1.080565 | 0.355402 | . 2021-01-30 12.9625 | 7.6500 | 4.4250 | 101.49500 | 71.375 | 2.470396 | 0.097694 | 5.69625 | 0.066911 | 3.345250 | 17.46 | 9.3276 | 4.152687 | 1.494843 | 1.066944 | 0.427899 | . 2021-01-31 15.0625 | 8.3125 | 3.7500 | 101.32500 | 66.000 | 2.465437 | 0.110070 | 8.82000 | 0.066933 | 2.987500 | 17.46 | 13.5468 | 5.513874 | 1.713106 | 1.130650 | 0.582456 | . 397 rows Ã— 16 columns . Now that all variables have been defined, daily E_penman can be calculated. . $$ E_{tp} = frac{ Delta}{ Delta+ gamma}Q_{ne}+ frac{ gamma}{ Delta+ gamma}E_A $$$Q_n$ is the available energy flux density: . $$ Q_n = R_n - G, $$and $E_A$ is the drying power of the air: . $$ E_A = f_e(u) cdot text{VPD} $$Add a new column E_penman to df_pen. . def E_penman(df): T = df[&#39;T&#39;] Delta = df[&#39;Delta&#39;] gamma = df[&#39;gamma&#39;] Rn = df[&#39;Rn&#39;] G = df[&#39;G&#39;] EA = 6.43*df[&#39;f_wind&#39;] * df[&#39;VPD&#39;] lambd = df[&#39;lambda&#39;] return ((Delta / (Delta + gamma))*(Rn - G) + ((gamma / (Delta + gamma))*EA)) / lambd # daily_data df_pen[&#39;E_penman&#39;] = E_penman(df_pen) fig, ax = plt.subplots(1, figsize=(10,7)) ax.plot(df_pen[&#39;E_penman&#39;]) plt.gcf().autofmt_xdate() ax.set_ylabel(r&quot;$ET_{penman}$ (mm d$^{-1}$)&quot;) . . Text(0, 0.5, &#39;$ET_{penman}$ (mm d$^{-1}$)&#39;) . Make a plot with the following: . the Penman (daily) estimate of the potential evapotranspiration. | the Thornthwaite (monthly) estimate of the potential ET. | daily evaporation pan data. | fig, ax = plt.subplots(1, 1, figsize=(10,7)) ax.plot(df_pen[&#39;E_penman&#39;], color=&quot;tab:red&quot;, label=&quot;Penman&quot;, linewidth=2) ax.plot(df_th[&#39;Ep&#39;]/30, color=&quot;tab:blue&quot;, label=&quot;Thornthwaite&quot;, linewidth=2) ax.plot(1*df2[&#39;pan evaporation (mm)&#39;], color=&quot;black&quot;, label=&quot;pan&quot;, linewidth=2) ax.set(xlabel=&quot;date&quot;, ylabel=&quot;evaporation (mm)&quot;) ax.legend(); . . Plot the mean temperatures used in the Penman calculation (daily mean) and in the Thornthwaite calculation (monthly mean). . fig, ax = plt.subplots(1, 1, figsize=(10,7)) ax.plot(df_pen[&#39;T&#39;], color=&quot;tab:blue&quot;, label=&quot;Penman&quot;, linewidth=2) ax.plot(df_th[&#39;T&#39;], color=&quot;tab:orange&quot;, label=&quot;Thornthwaite&quot;, linewidth=2) ax.set(xlabel=&quot;date&quot;, ylabel=&quot;temperature (Â°C)&quot;) ax.legend(); . .",
            "url": "https://yairmau.github.io/website/jupyter/2020/02/03/evapotranspiration-exercises.html",
            "relUrl": "/jupyter/2020/02/03/evapotranspiration-exercises.html",
            "date": " â€¢ Feb 3, 2020"
        }
        
    
  
    
        ,"post41": {
            "title": "Assignment 2 - Evapotranspiration",
            "content": "&#128210; instructions . This is where learning happens, not during a lecture. You&#39;ll learn a ton of things by doing them yourself. Much success! ğŸ˜„ . Create a Jupyter Notebook called assignment-02-IDNUMBER, where IDNUMBER is your 9-digit ID. This is the file only file we will check. . &#128204; locations and data . Choose two stations with different climates. . Go to NOAA&#39;s Climate Reference Network Data website. The sub-hourly (5-min) data contains information on . air temperature, | precipitation, | global solar radiation, | surface infrared temperature, | relative humidity, | soil moisture and temperature, | wetness, and | 1.5 meter wind speed. | . There is no data on air pressure, so one needs to use the stations coordinates (lat, lon) to find its height above sea level, and from that infer the air pressure. You can use Google Earth or any other means to find the station&#39;s height. . In the Data Access link, choose a year and a station you would like to analyze. If you are not sure where the stations are, find them using the 2-letter state abbreviation and the station name. . Download the following files: . One full year of data for each station. Make sure important data we need to calculate Penman&#39;s ET estimation is available. | The headers file | The documentation file | Make sure you understand what are the units provided for each measurement (see documentation). . &#128736; tasks . Produce potential ET estimates using Thornthwaite&#39;s equation and Penman&#39;s equation. Produce plots of ET as a function of time for each station, comparing the two methods you used. Also, using Penman&#39;s ET estimates, compare the two stations and discuss about their differences/similarities. . You might find interesting things in the data, such as periods of unusually high/low temperatures, radiation, etc. Discuss how these factors might have affected the ET estimates that you calculated. . You will have two weeks to deliver your assignment. You should not hand in a dry document with only figures and code, I&#39;m expecting text before and after each code/graph cell, explaining what you did, why you did it, and how it fits the story you are telling. Don&#39;t forget to put labels on your plot axes, title, legend, etc. . Your Jupyter Notebook should be fully functional: if we press Kernel &gt; Restart &amp; Run All, all the code must work without any errors. . &#127749; presentation . All the assignment must be in one single Jupyter Notebook. Use markdown cells to discuss the analysis and results, and in code cells show all the code you used to produce the figures and data analysis. Leave only the code necessary for your analysis, delete unnecessary lines your wrote while analyzing your data. Don&#39;t forget to comment your code, just like we did during exercise sessions. . You can write in English or in Hebrew, but the text in the figures must be in English. If you choose to write the discussion in Hebrew, be aware that Jupyter Notebooks don&#39;t have native right-to-left language support: . × ×™×ª×Ÿ ×œ×›×ª×•×‘ ×‘×¢×‘×¨×™×ª, ×œ××¨×•×ª ×©×–×” ×œ× × ×¨××” ×›×´×› ×˜×•×‘... . You can use some HTML code to achieve best results in Hebrew. Type the following . &lt;p dir=&quot;rtl&quot; style=&quot;text-align: right;&quot;&gt; ×¢×›×©×™×• ×”×¨×‘×” ×™×•×ª×¨ ×˜×•×‘! &lt;/p&gt; . to get . ×¢×›×©×™×• ×”×¨×‘×” ×™×•×ª×¨ ×˜×•×‘! . If you have many paragraphs in hebrew, do the following: . ×¤×¡×§×” ××¡×¤×¨ 1. . ×¤×¡×§×” ××¡×¤×¨ 2. . ×× ×™×© ×œ×›× ×›××” ×¤×¡×§××•×ª, ×›×œ ××—×ª ××”×Ÿ ×ª×”×™×” ×‘×ª×•×š &quot;dir&quot; ××©×œ×” . In my opinion it is too complicated to write in Hebrew in Jupyter Notebooks, just write in English, your grade will not be affected by typos nor less-than-perfect English proficiency. . &#128175; evaluation . Your assignment will be evaluated according to the following criteria: . 40% Presentation. How the graphs look, labels, general organization, markdown, clean code. | 30% Discussion. This is where you explain what you did, what you found out, etc. | 15% Depth of analysis. You can analyze/explore the data with different levels of complexity, this is where we take that into consideration. | 10% Replicability: Your code runs flawlessly. | 5%: Code commenting. Explain in your code what you are doing, this is good for everyone, especially for yourself! | . &#128666; importing the data . Below you can find an example of how to import the data file provided by NOAA&#39;s Climate Reference Network Data website. You might have to make some adjustments to it. . data_file = &quot;CRNS0101-05-2020-CO_Boulder_14_W.txt&quot; df = pd.read_csv(data_file, header=None, # no headers needed, we&#39;ll do that later delim_whitespace=True, # blank spaces separate between columns na_values=[&quot;-99.000&quot;, &quot;-9999.0&quot;] # substitute these values for missing (NaN) values ) headers = pd.read_csv(&quot;HEADERS_sub_hourly.txt&quot;, # load headers file header=1, # skip the first [0] line delim_whitespace=True ) df.columns = headers.columns # rename df columns with headers columns # LST = local standard time df[&quot;LST_TIME&quot;] = [f&quot;{x:04d}&quot; for x in df[&quot;LST_TIME&quot;]] # time needs padding of zeros, then convert to string df[&#39;LST_DATE&#39;] = df[&#39;LST_DATE&#39;].astype(str) # convert date into string df[&#39;datetime&#39;] = df[&#39;LST_DATE&#39;] + &#39; &#39; + df[&#39;LST_TIME&#39;] # combine date+time into datetime df[&#39;datetime&#39;] = pd.to_datetime(df[&#39;datetime&#39;]) # interpret datetime df = df.set_index(&#39;datetime&#39;) # make datetime the index df .",
            "url": "https://yairmau.github.io/website/jupyter/2020/02/03/assignment-02-ET.html",
            "relUrl": "/jupyter/2020/02/03/assignment-02-ET.html",
            "date": " â€¢ Feb 3, 2020"
        }
        
    
  
    
        ,"post42": {
            "title": "Intra-annual variability of precipitation",
            "content": ". Letâ€™s shift the months according the the hydrological year: . . Seasonality Index, Walsh and Lawler (1981) . Source: leddris.aegean.gr . $R=$ mean annual precipitation $m_i=$ precipitation mean for month $i$ . SI=1Râˆ‘n=1n=12âˆ£miâˆ’R12âˆ£SI = displaystyle frac{1}{R} sum_{n=1}^{n=12} left| m_i - frac{R}{12} right|SI=R1â€‹n=1âˆ‘n=12â€‹âˆ£ . âˆ£â€‹miâ€‹âˆ’12Râ€‹âˆ£ . âˆ£â€‹ . $SI$ Precipitation Regime . &lt;0.19 | Precipitation spread throughout the year | . 0.20-0.39 | Precipitation spread throughout the year, but with a definite wetter season | . 0.40-0.59 | Rather seasonal with a short dry season | . 0.60-0.79 | Seasonal | . 0.80-0.99 | Marked seasonal with a long dry season | . 1.00-1.19 | Most precipitation in &lt;3 months | . Toggle Code â€ƒ . # import packages import numpy as np import pandas as pd from calendar import month_abbr # load data month_numbers = np.arange(1,13) month_names = [month_abbr[i] for i in month_numbers] def monthly_mean(station_name, freq): # import daily data df = pd.read_csv(station_name + &#39;_&#39; + freq + &#39;.csv&#39;, sep=&quot;,&quot;) # make &#39;DATE&#39; the dataframe index df[&#39;DATE&#39;] = pd.to_datetime(df[&#39;DATE&#39;]) df = df.set_index(&#39;DATE&#39;) # print(df.index[0], df.index[-1]) if freq == &#39;daily&#39;: # resample data by month df_month = df[&#39;PRCP&#39;].resample(&#39;M&#39;).sum() # sum is labeled at the last day of the month df_month = df_month/10 # PRCP is given in tens of mm (see readme) if freq == &#39;monthly&#39;: df_month = df[&#39;PRCP&#39;] # calculate monthly mean monthly_mean = np.array([]) # empty array for m in month_numbers: # cycle over months (1, 2, 3, etc) this_month_all_indices = (df_month.index.month == m) # indices in df_month belonging to month m this_month_mean = df_month[this_month_all_indices].mean() # this is the monthly mean monthly_mean = np.append(monthly_mean, this_month_mean) # append # make new df and return it df_return = pd.DataFrame({&#39;monthly rainfall (mm)&#39;:monthly_mean, &#39;month names&#39;:month_names, &#39;month number&#39;:month_numbers }) return df_return # load monthly mean df_london = monthly_mean(&quot;LONDON HEATHROW&quot;, &#39;monthly&#39;) df_telaviv = monthly_mean(&quot;TEL AVIV READING&quot;, &#39;monthly&#39;) #collapse-hide def walsh_index(df): m = df[&quot;monthly rainfall (mm)&quot;] R = df[&quot;monthly rainfall (mm)&quot;].sum() SI = np.sum(np.abs(m-R/12)) / R return SI london_index = walsh_index(df_london) telaviv_index = walsh_index(df_telaviv) print(&quot;Seasonality index (Walsh and Lawler, 1981)&quot;) print(f&quot;London: {london_index:.2f}&quot;) print(f&quot;Tel Aviv: {telaviv_index:.2f}&quot;) . .",
            "url": "https://yairmau.github.io/website/markdown/2020/02/02/intra-annual-variability-of-precipitation-seasonality-lecture.html",
            "relUrl": "/markdown/2020/02/02/intra-annual-variability-of-precipitation-seasonality-lecture.html",
            "date": " â€¢ Feb 2, 2020"
        }
        
    
  
    
        ,"post43": {
            "title": "Interannual variability of precipitation",
            "content": ". hydrological year . A time period of 12 months for which precipitation totals are measured. The hydrological year is designated by the calendar year in which it ends. Letâ€™s define the hydrological year for Tel Aviv from 1 October to 30 September. . ×”×× ××§×œ×™× ×”×’×©× ×©×œ× ×• ××©×ª× ×” . . . coefficient of variation . $ langle{P} rangle=$ average precipitation $ sigma=$ standard deviation . CV=ÏƒâŸ¨PâŸ©CV = frac{ sigma}{ langle{P} rangle}CV=âŸ¨PâŸ©Ïƒâ€‹ . Assuming that the inter-annual distribution is a gaussian: 67% of the time, rainfall will vary +/- 30% from its long term average in Tel Aviv. . Precipitation averages are usually calculated for time intervals of 30 years. . . . Toggle Code â€ƒ . import altair as alt import pandas as pd df = pd.read_csv(&quot;TEL AVIV READING_monthly.csv&quot;, sep=&quot;,&quot;) # make &#39;DATE&#39; the dataframe index df[&#39;DATE&#39;] = pd.to_datetime(df[&#39;DATE&#39;]) df = df.set_index(&#39;DATE&#39;) df_year_all = df[&#39;PRCP&#39;].resample(&#39;A-SEP&#39;).sum().to_frame() # annual frequency, anchored end of September df_year_all.columns = [&#39;rain (mm)&#39;] # rename &#39;PRCP&#39; column to &#39;rain (mm)&#39; df_year = df_year_all.iloc[:-1] # exclude last row # Altair only recognizes column data; it ignores index values. # You can plot the index data by first resetting the index # I know that I&#39;ve just made &#39;DATE&#39; the index, but I want to have this here nonetheless so I can refer to this in the future source = df_year.reset_index() brush = alt.selection(type=&#39;interval&#39;, encodings=[&#39;x&#39;]) # T: temporal, a time or date value # Q: quantitative, a continuous real-valued quantity # https://altair-viz.github.io/user_guide/encoding.html#encoding-data-types bars = alt.Chart().mark_bar().encode( x=alt.X(&#39;DATE:T&#39;, axis=alt.Axis(title=&#39;date&#39;)), y=alt.Y(&#39;rain (mm):Q&#39;, axis=alt.Axis(title=&#39;annual precipitation (mm) and average&#39;)), opacity=alt.condition(brush, alt.OpacityValue(1), alt.OpacityValue(0.2)), ).add_selection( brush ).properties( title=&#39;Select year range and drag for rolling average of annual precipitation in Tel Aviv&#39; ).properties( width=600, height=400 ) line = alt.Chart().mark_rule(color=&#39;orange&#39;).encode( y=&#39;mean(rain (mm)):Q&#39;, size=alt.SizeValue(3) ).transform_filter( brush ) alt.layer(bars, line, data=source) . .",
            "url": "https://yairmau.github.io/website/markdown/2020/02/02/interannual-variability-of-precipitation-lecture.html",
            "relUrl": "/markdown/2020/02/02/interannual-variability-of-precipitation-lecture.html",
            "date": " â€¢ Feb 2, 2020"
        }
        
    
  
    
        ,"post44": {
            "title": "Exercises, inter- and intra-annual variability of precipitation",
            "content": "Import relevant packages . import matplotlib.pyplot as plt import numpy as np import pandas as pd from calendar import month_abbr import seaborn as sns sns.set(style=&quot;ticks&quot;, font_scale=1.5) import urllib.request from pandas.plotting import register_matplotlib_converters register_matplotlib_converters() . . intra-annual variability . Go to NOAA&#39;s National Centers for Environmental Information (NCEI) Climate Data Online: Dataset Discovery . Find station codes in this map. On the left, click on the little wrench (ğŸ”§) next to &quot;Global Summary of the Month&quot;, then click on &quot;identify&quot; on the panel that just opened, and click on a station (purple circle). You will see the station&#39;s name, it&#39;s ID, and the period of record. For example, for Ben-Gurion&#39;s Airport in Israel: BEN GURION, IS STATION ID: ISM00040180 Period of Record: 1951-01-01 to 2020-03-01 . You can download daily or monthly data for each station. Use the function below to download this data to your computer. . def download_data(station_name, station_code): url_daily = &#39;https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access/&#39; url_monthly = &#39;https://www.ncei.noaa.gov/data/gsom/access/&#39; # download daily data - uncomment the next 2 lines to make this work # urllib.request.urlretrieve(url_daily + station_code + &#39;.csv&#39;, # station_name + &#39;_daily.csv&#39;) # download monthly data urllib.request.urlretrieve(url_monthly + station_code + &#39;.csv&#39;, station_name + &#39;_monthly.csv&#39;) . . Now, choose any station with a period of record longer than 30 years, and download its data: . download_data(&#39;BEN_GURION&#39;, &#39;ISM00040180&#39;) . Load the data into a datafram, and before you continue with the analysis, plot the rainfall data, to see how it looks like. . download_data(&#39;BEN_GURION&#39;, &#39;ISM00040180&#39;) df = pd.read_csv(&#39;BEN_GURION_monthly.csv&#39;, sep=&quot;,&quot;) # make &#39;DATE&#39; the dataframe index df[&#39;DATE&#39;] = pd.to_datetime(df[&#39;DATE&#39;]) df = df.set_index(&#39;DATE&#39;) plt.plot(df[&#39;PRCP&#39;]) . . [&lt;matplotlib.lines.Line2D at 0x7fe9a0b4f2d0&gt;] . It doesn&#39;t look great for Ben-Gurion airport, lots of missing data! You might need to choose another station... Download data for Beer Sheva, ID IS000051690. . download_data(&#39;BEER_SHEVA&#39;, &#39;IS000051690&#39;) df = pd.read_csv(&#39;BEER_SHEVA_monthly.csv&#39;, sep=&quot;,&quot;) # make &#39;DATE&#39; the dataframe index df[&#39;DATE&#39;] = pd.to_datetime(df[&#39;DATE&#39;]) df = df.set_index(&#39;DATE&#39;) plt.plot(df[&#39;PRCP&#39;]) . . [&lt;matplotlib.lines.Line2D at 0x7fe9a1973d10&gt;] . That&#39;s much better! We need to aggregate all data from each month, so we can calculate monthly averages. How to do that? . # choose only the precipitation column df_month = df[&#39;PRCP&#39;] # calculate monthly mean monthly_mean = np.array([]) # empty array month_numbers = np.arange(1,13) month_names = [month_abbr[i] for i in month_numbers] for m in month_numbers: # cycle over months (1, 2, 3, etc) this_month_all_indices = (df_month.index.month == m) # indices in df_month belonging to month m this_month_mean = df_month[this_month_all_indices].mean() # this is the monthly mean monthly_mean = np.append(monthly_mean, this_month_mean) # append . . Now it is time to create a new dataframe with the monthly means. . df_beersheva = pd.DataFrame({&#39;monthly rainfall (mm)&#39;:monthly_mean, &#39;month names&#39;:month_names, &#39;month number&#39;:month_numbers }) df_beersheva . . monthly rainfall (mm) month names month number . 0 | 48.743158 | Jan | 1 | . 1 | 37.347368 | Feb | 2 | . 2 | 26.551579 | Mar | 3 | . 3 | 9.038947 | Apr | 4 | . 4 | 2.735789 | May | 5 | . 5 | 0.013830 | Jun | 6 | . 6 | 0.000000 | Jul | 7 | . 7 | 0.002128 | Aug | 8 | . 8 | 0.271277 | Sep | 9 | . 9 | 6.669474 | Oct | 10 | . 10 | 21.850526 | Nov | 11 | . 11 | 41.786316 | Dec | 12 | . Plot the data and see if it makes sense. Try to get a figure like this one. . fig, ax = plt.subplots(figsize=(10,7)) ax.bar(df_beersheva[&#39;month number&#39;], df_beersheva[&#39;monthly rainfall (mm)&#39;]) ax.set(xlabel=&quot;months&quot;, ylabel=&quot;monthly average (mm)&quot;, title=&quot;Beer Sheva&quot;, xticks=df_beersheva[&#39;month number&#39;], xticklabels=df_beersheva[&#39;month names&#39;]); plt.savefig(&quot;hydrology_figures/beersheva_monthly_average.png&quot;) . . Let&#39;s calculate now the Walsh and Lawler Seasonality Index. Write a function that receives a dataframe like the one we have just created, and returns the seasonality index. http://leddris.aegean.gr/ses-parameters/293-rainfall-seasonality.html#:~:text=Rainfall%20seasonality%20index%20is%20a,in%20relation%20to%20water%20availability . $R=$ mean annual precipitation $m_i$ precipitation mean for month $i$ . $$ SI = displaystyle frac{1}{R} sum_{n=1}^{n=12} left| m_i - frac{R}{12} right| $$ . SI Precipitation Regime . &lt;0.19 | Precipitation spread throughout the year | . 0.20-0.39 | Precipitation spread throughout the year, but with a definite wetter season | . 0.40-0.59 | Rather seasonal with a short dry season | . 0.60-0.79 | Seasonal | . 0.80-0.99 | Marked seasonal with a long dry season | . 1.00-1.19 | Most precipitation in &lt; 3 months | . def walsh_index(df): mi = df[&quot;monthly rainfall (mm)&quot;] R = df[&quot;monthly rainfall (mm)&quot;].sum() SI = np.sum(np.abs(mi - R/12)) / R return SI beersheva_SI = walsh_index(df_beersheva) print(f&quot;Beer Sheva, SI = {beersheva_SI:.2f}&quot;) . . Beer Sheva, SI = 0.97 . interannual variability . Plot monthly rainfall for your station. . fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,7)) ax1.plot(df[&#39;PRCP&#39;]) ax2.plot(df[&#39;PRCP&#39;][&#39;2010-07-01&#39;:&#39;2015-07-01&#39;]) . . [&lt;matplotlib.lines.Line2D at 0x7fe9a1f4f150&gt;] . How to aggregate rainfall accoding to the hydrological year? We use the function resample. . read more about resampling options: https://pandas.pydata.org/pandas-docs/version/0.12.0/timeseries.html#offset-aliases . also, annual resampling can be anchored to the end of specific months: https://pandas.pydata.org/pandas-docs/version/0.12.0/timeseries.html#anchored-offsets . # annual frequency, anchored 31 December df_year_all = df[&#39;PRCP&#39;].resample(&#39;A&#39;).sum().to_frame() # annual frequency, anchored 01 January df_year_all = df[&#39;PRCP&#39;].resample(&#39;AS&#39;).sum().to_frame() # annual frequency, anchored end of September df_year_all = df[&#39;PRCP&#39;].resample(&#39;A-SEP&#39;).sum().to_frame() # rename &#39;PRCP&#39; column to &#39;rain (mm)&#39; df_year_all.columns = [&#39;rain (mm)&#39;] df_year_all . . rain (mm) . DATE . 1922-09-30 | 136.6 | . 1923-09-30 | 144.5 | . 1924-09-30 | 130.4 | . 1925-09-30 | 165.3 | . 1926-09-30 | 188.7 | . ... | ... | . 2012-09-30 | 145.7 | . 2013-09-30 | 175.3 | . 2014-09-30 | 259.2 | . 2015-09-30 | 249.3 | . 2016-09-30 | 257.6 | . 95 rows Ã— 1 columns . You might need to exclude the first or the last line, since their data might have less that 12 months. For example: . # exclude 1st row df_year = df_year_all.iloc[1:] # exclude last row df_year = df_year_all.iloc[:-1] # exclude both 1st and last rows df_year = df_year_all.iloc[1:-1] df_year . . rain (mm) . DATE . 1923-09-30 | 144.5 | . 1924-09-30 | 130.4 | . 1925-09-30 | 165.3 | . 1926-09-30 | 188.7 | . 1927-09-30 | 130.2 | . ... | ... | . 2011-09-30 | 151.6 | . 2012-09-30 | 145.7 | . 2013-09-30 | 175.3 | . 2014-09-30 | 259.2 | . 2015-09-30 | 249.3 | . 93 rows Ã— 1 columns . Calculate the average annual rainfall. Plot annual rainfall for the whole range, together with the average. You should get something like this: . fig, ax = plt.subplots(figsize=(10,7)) # plot YEARLY precipitation ax.bar(df_year.index, df_year[&#39;rain (mm)&#39;], width=365, align=&#39;edge&#39;, color=&quot;tab:blue&quot;) # plot mean rain_mean = df_year[&#39;rain (mm)&#39;].mean() ax.plot(ax.get_xlim(), [rain_mean]*2, linewidth=3, color=&quot;tab:orange&quot;) ax.set(xlabel=&quot;date&quot;, ylabel=&quot;yearly rainfall (mm)&quot;, title=f&quot;Beer Sheva, mean = {rain_mean:.0f} mm&quot;); # save figure plt.savefig(&quot;hydrology_figures/beersheva_yearly_rainfall_1923_2016.png&quot;) . . /Users/yairmau/opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py:2127: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version. Instead of adding/subtracting `n`, use `n * self.freq` dx = [convert(x0 + ddx) - x for ddx in dx] . Plot a histogram of annual rainfall, with the mean and standard deviation. Calculate the coefficient of variation. Try to plot something like this: . fig, ax = plt.subplots(figsize=(10,7)) # calculate mean and standard deviation rain_mean = df_year[&#39;rain (mm)&#39;].mean() rain_std = df_year[&#39;rain (mm)&#39;].std() # plot histogram b = np.arange(0, 401, 50) # bins from 0 to 400, width = 50 ax.hist(df_year[&#39;rain (mm)&#39;], bins=b) # plot vertical lines with mean, std, etc ylim = np.array(ax.get_ylim()) ylim[1] = ylim[1]*1.1 ax.plot([rain_mean]*2, ylim, linewidth=3, color=&quot;tab:orange&quot;) ax.plot([rain_mean+rain_std]*2, ylim, linewidth=3, linestyle=&quot;--&quot;, color=&quot;tab:olive&quot;) ax.plot([rain_mean-rain_std]*2, ylim, linewidth=3, linestyle=&quot;--&quot;, color=&quot;tab:olive&quot;) ax.set(ylim=ylim, xlabel=&quot;annual rainfall (mm)&quot;, ylabel=&quot;number of years&quot;, title=f&quot;Beer Sheva, 1922â€“2016. Mean={rain_mean:.0f} mm, STD={rain_std:.0f} mm&quot;) ax.text(300, 25, f&quot;CV = {rain_std/rain_mean:.2f}&quot;) plt.savefig(&quot;histogram_beersheva.png&quot;) . . Calculate the mean annual rainfall for various 30-year intervals . ####### the hard way ####### # fig, ax = plt.subplots(figsize=(10,7)) # mean_30_59 = df_year.loc[&#39;1930-09-30&#39;:&#39;1959-09-01&#39;,&#39;rain (mm)&#39;].mean() # mean_40_69 = df_year.loc[&#39;1940-09-30&#39;:&#39;1969-09-01&#39;,&#39;rain (mm)&#39;].mean() # mean_50_79 = df_year.loc[&#39;1950-09-30&#39;:&#39;1979-09-01&#39;,&#39;rain (mm)&#39;].mean() # mean_60_89 = df_year.loc[&#39;1960-09-30&#39;:&#39;1989-09-01&#39;,&#39;rain (mm)&#39;].mean() # mean_70_99 = df_year.loc[&#39;1970-09-30&#39;:&#39;1999-09-01&#39;,&#39;rain (mm)&#39;].mean() # mean_80_09 = df_year.loc[&#39;1980-09-30&#39;:&#39;2009-09-01&#39;,&#39;rain (mm)&#39;].mean() # ax.plot([mean_30_59, # mean_40_69, # mean_50_79, # mean_60_89, # mean_70_99, # mean_80_09]) ####### the easy way ####### fig, ax = plt.subplots(figsize=(10,7)) # use list comprehension windows = [[x, x+29] for x in [1930,1940,1950,1960,1970,1980]] mean = [df_year.loc[f&#39;{w[0]:d}-09-30&#39;:f&#39;{w[1]:d}-09-01&#39;,&#39;rain (mm)&#39;].mean() for w in windows] ax.plot(mean) ax.set(xticks=np.arange(len(mean)), xticklabels=[str(w) for w in windows], ylabel=&quot;window average (mm)&quot; ); . . homework . Download both daily and monthly data for London (LONDON HEATHROW, ID: UKM00003772). You should be aware that &#39;PRCP&#39; for monthly data is in millimeters, while &#39;PRCP&#39; for daily data is in tens of millimiters. | Aggregate daily data into monthly intervals using resample(&#39;MS&#39;).sum(). &#39;MS&#39; means that the sum of all days in the month will be stored in the first day of the month. Supposedly both datasets are equal now. | Calculate the average annual rainfall, using each of these datasets. | Why is there such a big difference? |",
            "url": "https://yairmau.github.io/website/jupyter/2020/02/02/variability-of-precipitation-exercises.html",
            "relUrl": "/jupyter/2020/02/02/variability-of-precipitation-exercises.html",
            "date": " â€¢ Feb 2, 2020"
        }
        
    
  
    
        ,"post45": {
            "title": "Return period",
            "content": "Bilbao, Spain . Today . August 1983 . more photos . On Friday, August 26, 1983, Bilbao was celebrating its Aste Nagusia or Great Week, the main annual festivity in the city, when it and other municipalities of the Basque Country, Burgos, and Cantabria suffered devastating flooding due to heavy rains. In 24 hours, the volume of water registered 600 liters per square meter. Across all the affected areas, the weather service recorded 1.5 billion tons of water. In areas of Bilbao, the water reached a height of 5 meters (15 feet). Transportation, electricity and gas services, drinking water, food, telephone, and many other basic services were severely affected. 32 people died in Biscay, 4 people died in Cantabria, 2 people died in Alava, and 2 people died Burgos. 5 more people went missing. . How often will such rainfall happen? . How often does it rain 50 mm in 1 day? What about 100 mm in 1 day? How big is a &quot;once-in-a-century event&quot;? . Let&#39;s examine Bilbao&#39;s daily rainfall (mm), between 1947 to 2021 . . On the week of 22-28 August 1983, Bilbao&#39;s weather station measured 4.5 m of rainfall! . . Let&#39;s analyze this data and find out how rare such events are. First we need to find the annual maximum for each hydrological year. . import matplotlib.pyplot as plt import numpy as np import pandas as pd df = pd.read_csv(&#39;BILBAO_daily.csv&#39;, sep=&quot;,&quot;) df[&#39;DATE&#39;] = pd.to_datetime(df[&#39;DATE&#39;]) df = df.set_index(&#39;DATE&#39;) # IMPORTANT!! daily precipitation data is in tenths of mm, divide by 10 to get it in mm. df[&#39;PRCP&#39;] = df[&#39;PRCP&#39;] / 10 import altair as alt alt.data_transformers.disable_max_rows() # Altair only recognizes column data; it ignores index values. # You can plot the index data by first resetting the index # I know that I&#39;ve just made &#39;DATE&#39; the index, but I want to have this here nonetheless so I can refer to this in the future df_new = df.reset_index()#.replace({0.0:np.nan}) source = df_new[[&#39;DATE&#39;, &#39;PRCP&#39;]] brush = alt.selection(type=&#39;interval&#39;, encodings=[&#39;x&#39;]) base = alt.Chart(source).mark_line().encode( x = &#39;DATE:T&#39;, y = &#39;PRCP:Q&#39; ).properties( width=600, height=200 ) upper = base.encode( alt.X(&#39;DATE:T&#39;, scale=alt.Scale(domain=brush)), alt.Y(&#39;PRCP:Q&#39;, scale=alt.Scale(domain=(0,100))) ) lower = base.properties( height=60 ).add_selection(brush) alt.vconcat(upper, lower) . . We will consider a hydrological year starting on 1 August. . Histogram of annual maximum events . . How many years, on average, do we have to wait to get an annual maximum above a given threshold? . . . . . . Now everything together in one gif: . . Return Period . We will follow Brutsaert&#39;s derivation (&quot;Hydrology, an introduction&quot;, page 513). It defines quantities is a little different from what we did above. . $F(x)$ is the CDF of the PDF $f(x)$. $F(x)$ indicates the non-exceedance probability, i.e., the probability that a certain event above $x$ has not occurred (or that an event below $x$ has occurred, same thing). Modifying the graph shown above, we have . . $1-F(x)$ is the probability that a certain event above $x$ has occurred. It&#39;s reciprocal is the return period: . $$ T_r(x) = frac{1}{1-F(x)} $$This return period is the expected number of observations required until $x$ is exceeded once. In our case, we can ask the question: how many years will pass (on average) until we see a rainfall event greater that that of 26 August 1983? . Let&#39;s call $p=F(x)$ the probability that we measured once and that an event greater than $x$ has not occurred. What is the probability that a rainfall above $x$ will occur only on year number $k$? . it hasn&#39;t occurred on year 1 (probability p) | it hasn&#39;t occurred on year 2 (probability p) | it hasn&#39;t occurred on year 3 (probability p) | ... | it has occurred on year k (probability 1-p) | . $P {k text{ trials until }X&gt;x } = p^{k-1}(1-p)$ . Every time the number $k$ will be different. What will be $k$ on average? . $$ bar{k} = displaystyle sum_{k=1}^{ infty} k P(k) = displaystyle sum_{k=1}^{ infty} k p^{k-1}(1-p)$$ . Let&#39;s open that up: . $$ begin{align} bar{k} &amp;= 1-p + 2p(1-p) + 3p^2(1-p) + 4p^3(1-p)+ cdots bar{k} &amp;= 1-p + 2p - 2p^2 + 3p^2 - 3p^4 + 4p^3 - 4p^4+ cdots bar{k} &amp;= 1 + p + p^2 + p^3 + p^4 + cdots end{align} $$For $p&lt;1$, the series converges to $$ 1 + p + p^2 + p^3 + p^4 + cdots = frac{1}{1-p}, $$ therefore $$ bar{k} = frac{1}{1-p}. $$ . We conclude that if we know the exceedance probability, we immediately can say what the return times are. We now need a way of estimating this exceedance probability. . Plotting Position . Source: Brutsaert, Hydrology, pages 514-516 . The Plotting Position is used as an estimate of the exceedance probability. Many formulas have been suggested (see source above), we will use the Weibull plotting position: . $P_m=$ plotting position, or probability of occurence for each event $n=$ total number of events $m=$ rank of each event, where $m=1$ is the lowest value, and $m=n$ is the highest . Return period: . $$ text{Return period} = T_r = frac{1}{1-P_m} $$Weibull plotting position: . $$ P_m = frac{m}{n+1} $$Now let&#39;s calculate that for Bilbao: . # resample daily data into yearly data (maximum yearly value) max_annual = df[&#39;PRCP&#39;].resample(&#39;A-JUL&#39;).max().to_frame() # sort yearly max from lowest to highest max_annual = max_annual.sort_values(by=[&#39;PRCP&#39;], ascending=True) max_annual[&#39;m&#39;] = np.arange(1, len(max_annual) + 1) n = len(max_annual[&#39;m&#39;]) max_annual[&#39;Pm&#39;] = max_annual[&#39;m&#39;] / (n+1) max_annual[&#39;Tr&#39;] = 1 / (1 - max_annual[&#39;Pm&#39;]) max_annual . . PRCP m Pm Tr . DATE . 2011-07-31 27.0 | 1 | 0.013158 | 1.013333 | . 2002-07-31 28.5 | 2 | 0.026316 | 1.027027 | . 2021-07-31 35.8 | 3 | 0.039474 | 1.041096 | . 2001-07-31 38.6 | 4 | 0.052632 | 1.055556 | . 2004-07-31 41.1 | 5 | 0.065789 | 1.070423 | . ... ... | ... | ... | ... | . 2010-07-31 108.1 | 71 | 0.934211 | 15.200000 | . 1960-07-31 137.2 | 72 | 0.947368 | 19.000000 | . 1964-07-31 143.5 | 73 | 0.960526 | 25.333333 | . 1954-07-31 172.6 | 74 | 0.973684 | 38.000000 | . 1984-07-31 252.6 | 75 | 0.986842 | 76.000000 | . 75 rows Ã— 4 columns . How well does $P_m$ approximate $F(x)$? . . We can now see in this graph how long it takes, on average, for an annual maximum event above any threshold. . . For times longer than $n$, we need to extrapolate from the curve above. . . . . .",
            "url": "https://yairmau.github.io/website/jupyter/2020/02/02/return-period-lecture.html",
            "relUrl": "/jupyter/2020/02/02/return-period-lecture.html",
            "date": " â€¢ Feb 2, 2020"
        }
        
    
  
    
        ,"post46": {
            "title": "Return period - exercises",
            "content": "Import relevant packages . import matplotlib.pyplot as plt import numpy as np import pandas as pd from functools import reduce import re import probscale import seaborn as sns sns.set(style=&quot;ticks&quot;, font_scale=1.5) from pandas.plotting import register_matplotlib_converters register_matplotlib_converters() import urllib.request . . Go to NOAA&#39;s National Centers for Environmental Information (NCEI) Climate Data Online: Dataset Discovery . Find station codes in this map. On the left, click on the little wrench next to &quot;Global Summary of the Month&quot;, then click on &quot;identify&quot; on the panel that just opened, and click on a station (purple circle). You will see the station&#39;s name, it&#39;s ID, and the period of record. For example, for Ben-Gurion&#39;s Airport in Israel: BEN GURION, IS STATION ID: ISM00040180 Period of Record: 1951-01-01 to 2020-03-01 . You can download daily or monthly data for each station. Use the function below to download this data to your computer. station_name can be whatever you want, station_code is the station ID. . def download_data(station_name, station_code): url_daily = &#39;https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access/&#39; url_monthly = &#39;https://www.ncei.noaa.gov/data/gsom/access/&#39; # download daily data - uncomment the following 2 lines to make this work # urllib.request.urlretrieve(url_daily + station_code + &#39;.csv&#39;, # station_name + &#39;_daily.csv&#39;) # download monthly data urllib.request.urlretrieve(url_monthly + station_code + &#39;.csv&#39;, station_name + &#39;_monthly.csv&#39;) . . Download daily rainfall data for Eilat, Israel. ID: IS000009972 . download_data(&#39;Eilat&#39;, &#39;IS000009972&#39;) . . Then load the data into a dataframe. IMPORTANT!! daily precipitation data is in tenths of mm, divide by 10 to get it in mm. . df = pd.read_csv(&#39;Eilat_daily.csv&#39;, sep=&quot;,&quot;) # make &#39;DATE&#39; the dataframe index df[&#39;DATE&#39;] = pd.to_datetime(df[&#39;DATE&#39;]) df = df.set_index(&#39;DATE&#39;) # IMPORTANT!! daily precipitation data is in tenths of mm, divide by 10 to get it in mm. df[&#39;PRCP&#39;] = df[&#39;PRCP&#39;] / 10 df . . STATION LATITUDE LONGITUDE ELEVATION NAME PRCP PRCP_ATTRIBUTES TMAX TMAX_ATTRIBUTES TMIN TMIN_ATTRIBUTES TAVG TAVG_ATTRIBUTES . DATE . 1949-11-30 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,E | NaN | NaN | NaN | NaN | NaN | NaN | . 1949-12-01 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,E | NaN | NaN | NaN | NaN | NaN | NaN | . 1949-12-02 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,E | NaN | NaN | NaN | NaN | NaN | NaN | . 1949-12-03 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,E | NaN | NaN | NaN | NaN | NaN | NaN | . 1949-12-04 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,E | NaN | NaN | NaN | NaN | NaN | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2021-03-24 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,S | 287.0 | ,,S | NaN | NaN | 227.0 | H,,S | . 2021-03-25 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | NaN | NaN | 253.0 | ,,S | 154.0 | ,,S | 202.0 | H,,S | . 2021-03-26 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | NaN | NaN | 251.0 | ,,S | 134.0 | ,,S | 186.0 | H,,S | . 2021-03-27 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | NaN | NaN | 222.0 | ,,S | 119.0 | ,,S | 173.0 | H,,S | . 2021-03-28 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | NaN | NaN | 238.0 | ,,S | 119.0 | ,,S | 188.0 | H,,S | . 26045 rows Ã— 13 columns . Plot precipitation data (&#39;PRCP&#39; column) and see if everything is all right. . fig, ax = plt.subplots(figsize=(10,7)) ax.plot(df[&#39;PRCP&#39;]) ax.set_xlabel(&quot;date&quot;) ax.set_ylabel(&quot;daily rainfall (mm)&quot;) ax.set_title(&quot;Eilat, 1949â€“2021&quot;) . . Text(0.5, 1.0, &#39;Eilat, 1949â€“2021&#39;) . Based on what you see, you might want to exclude certain periods, e.g.: . last_date = &#39;2018-08-01&#39; first_date = &#39;1950-08-01&#39; df = df[((df.index &lt; last_date) &amp; (df.index &gt; first_date))] df . . STATION LATITUDE LONGITUDE ELEVATION NAME PRCP PRCP_ATTRIBUTES TMAX TMAX_ATTRIBUTES TMIN TMIN_ATTRIBUTES TAVG TAVG_ATTRIBUTES . DATE . 1950-08-02 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,E | 400.0 | ,,G | 240.0 | ,,G | NaN | NaN | . 1950-08-03 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,E | 410.0 | ,,G | 260.0 | ,,G | NaN | NaN | . 1950-08-04 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,E | 400.0 | ,,G | 260.0 | ,,G | NaN | NaN | . 1950-08-05 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,E | NaN | NaN | 240.0 | ,,G | NaN | NaN | . 1950-08-06 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,E | 370.0 | ,,G | 240.0 | ,,G | NaN | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2018-07-27 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,S | 414.0 | ,,S | NaN | NaN | 359.0 | H,,S | . 2018-07-28 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,S | 386.0 | ,,S | NaN | NaN | 329.0 | H,,S | . 2018-07-29 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,S | NaN | NaN | 268.0 | ,,S | 334.0 | H,,S | . 2018-07-30 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,S | 375.0 | ,,S | 277.0 | ,,S | 327.0 | H,,S | . 2018-07-31 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,S | 390.0 | ,,S | NaN | NaN | 336.0 | H,,S | . 24836 rows Ã— 13 columns . The rainfall data for Eilat is VERY seasonal, it&#39;s easy to see that there is no rainfall at all during the summer. We can assume a hydrological year starting on 1 August. If you&#39;re not sure, you can plot the monthly means (see last week&#39;s lecture) and find what date makes sense best. . df_month = df[&#39;PRCP&#39;].resample(&#39;M&#39;).sum().to_frame() month_numbers = np.arange(1,13) monthly_mean = np.array([]) # empty array for m in month_numbers: # cycle over months (1, 2, 3, etc) this_month_mean = df_month[df_month.index.month == m].mean() # this is the monthly mean monthly_mean = np.append(monthly_mean, this_month_mean) # append # make new df and return it df_month = pd.DataFrame({&#39;monthly rainfall (mm)&#39;:monthly_mean, &#39;month number&#39;:month_numbers }) fig, ax = plt.subplots(figsize=(10,7)) ax.bar(df_month[&#39;month number&#39;], df_month[&#39;monthly rainfall (mm)&#39;]) ax.set(xlabel=&quot;month&quot;, ylabel=&quot;monthly rainfall (mm)&quot;, title=&quot;Monthly average, Eilat, 1949--2018&quot;, xticks=np.arange(1,13)); . . Let&#39;s resample the data according to the hydrological year (1 August), and we&#39;ll keep the maximum value: . max_annual = (df[&#39;PRCP&#39;].resample(&#39;A-JUL&#39;) .max() .to_frame() ) max_annual . . PRCP . DATE . 1951-07-31 10.8 | . 1952-07-31 15.0 | . 1953-07-31 34.4 | . 1954-07-31 24.3 | . 1955-07-31 19.0 | . ... ... | . 2014-07-31 11.5 | . 2015-07-31 2.4 | . 2016-07-31 8.5 | . 2017-07-31 34.5 | . 2018-07-31 11.7 | . 68 rows Ã— 1 columns . Make two graphs: a) the histogram for the annual maximum (pdf) b) the cumulative probability (cdf) . fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,8)) h=max_annual[&#39;PRCP&#39;].values ax1.hist(h, bins=np.arange(0,100,10), density=True) ax2.hist(h, bins=np.arange(0,100,10), cumulative=1, density=True) ax1.set(ylabel=&quot;pdf&quot;) ax2.set(xlabel=&quot;annual max (mm)&quot;, ylabel=&quot;cdf&quot;, ); . . Compute the plotting position and return time. You&#39;ll need to order the data in ascending order: . max_annual = max_annual.sort_values(by=[&#39;PRCP&#39;], ascending=True) . $P_m=$ plotting position, or probability of occurence for each event $n=$ total number of events $m=$ rank of each event, where $m=1$ is the lowest value, and $m=n$ is the highest . Weibull plotting position: . $$ P_m = frac{m}{n+1} $$Return period: . $$ text{Return period} = T_r = frac{1}{1-P_m} $$Plot the annual maximum against $P_m$ or against $T_r$. . fig, ax = plt.subplots(figsize=(10, 7)) # resample daily data into yearly data (maximum yearly value) max_annual = df[&#39;PRCP&#39;].resample(&#39;A-JUL&#39;).max().to_frame() # sort yearly max from lowest to highest max_annual = max_annual.sort_values(by=[&#39;PRCP&#39;], ascending=True) max_annual[&#39;rank&#39;] = np.arange(1, len(max_annual) + 1) print(max_annual) n = len(max_annual[&#39;rank&#39;]) m = max_annual[&#39;rank&#39;] Pm = m / (n+1) Tr = 1 / (1 - Pm) # ax.plot(Tr, max_annual[&#39;PRCP&#39;]) # ax.set(xlabel=&quot;return period (y)&quot;, # ylabel=&quot;annual maximum (mm/24h)&quot;) ax.plot(Pm, max_annual[&#39;PRCP&#39;]) ax.set(xlabel=&quot;non-exeedance probability&quot;, ylabel=&quot;annual maximum (mm/24h)&quot;); . . PRCP rank DATE 1996-07-31 0.5 1 2008-07-31 0.9 2 2000-07-31 1.2 3 2012-07-31 1.3 4 1959-07-31 1.5 5 ... ... ... 1966-07-31 33.8 64 1953-07-31 34.4 65 2017-07-31 34.5 66 1981-07-31 40.6 67 1975-07-31 64.3 68 [68 rows x 2 columns] . Plot the annual maximum against the exceedance probability ($1-P_m$), in a log-log scale. Use . ax.set(xscale=&quot;log&quot;, yscale(&quot;log&quot;) ) . See what data you&#39;ll want to use for a linear fit. . fig, ax = plt.subplots(figsize=(10, 6)) depth = max_annual[&#39;PRCP&#39;].values exc_prob = (1-Pm).values ax.plot(exc_prob, depth, lw=3) exclude = 40 depth_tofit = depth[exclude:] exc_prob_tofit = exc_prob[exclude:] ax.plot(exc_prob_tofit, depth_tofit, &#39;o&#39;) ax.set(ylabel=&quot;annual maximum (mm/24h)&quot;, xlabel=&quot;exceedance probability&quot;, xscale=&quot;log&quot;, yscale=&quot;log&quot;, ); . . Let&#39;s make a linear fit. Attention! Our data is not annual_max and exceedance_prob, but their log. . We make a linear fit using: . slope, intercept = np.polyfit(xdata, ydata, 1) # the number 1 in the order of the polynomial = linear . Write a function that receives an exceedance probability and returns the corresponding rainfall depth. . fig, ax = plt.subplots(figsize=(10, 6)) depth = max_annual[&#39;PRCP&#39;].values exc_prob = (1-Pm).values ax.plot(exc_prob, depth, lw=3, label=&quot;Weibull plotting position&quot;) ax.set(ylabel=&quot;annual maximum (mm/24h)&quot;, xlabel=&quot;exceedance probability&quot;) ax.set_xscale(&quot;log&quot;) ax.set_yscale(&quot;log&quot;) exclude = 40 depth_tofit = depth[exclude:] exc_prob_tofit = exc_prob[exclude:] ax.plot(exc_prob_tofit, depth_tofit, &#39;o&#39;) exc_prob_tofit_log = np.log(exc_prob_tofit) depth_tofit_log = np.log(depth_tofit) slope, intercept = np.polyfit(exc_prob_tofit_log, depth_tofit_log, 1) def equation(p): return np.exp(slope*np.log(p) + intercept) prob = [1e-3,1-1e-3] ax.plot(prob, equation(prob), lw=3, color=&quot;tab:red&quot;, alpha=0.4) . . [&lt;matplotlib.lines.Line2D at 0x7fc1aa9feb50&gt;] . Homework . Everything we did today was for 24h rainfall events. We might be interested in extreme events in longer or shorter time scales. Using the following code, calculate the return time for 3-day rainfall events: . number_of_days = 3 df2 = (df[&#39;PRCP&#39;].rolling(number_of_days) .sum() .dropna() ) . All the rest after that is the same... .",
            "url": "https://yairmau.github.io/website/jupyter/2020/02/02/return-period-exercises.html",
            "relUrl": "/jupyter/2020/02/02/return-period-exercises.html",
            "date": " â€¢ Feb 2, 2020"
        }
        
    
  
    
        ,"post47": {
            "title": "Return period - code",
            "content": "import matplotlib.pyplot as plt import numpy as np import pandas as pd from functools import reduce import re import probscale import seaborn as sns sns.set(style=&quot;ticks&quot;, font_scale=1.5) from pandas.plotting import register_matplotlib_converters register_matplotlib_converters() import urllib.request . def download_data(station_name, station_code): url_daily = &#39;https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access/&#39; url_monthly = &#39;https://www.ncei.noaa.gov/data/gsom/access/&#39; # download daily data - uncomment to make this work urllib.request.urlretrieve(url_daily + station_code + &#39;.csv&#39;, station_name + &#39;_daily.csv&#39;) # download monthly data urllib.request.urlretrieve(url_monthly + station_code + &#39;.csv&#39;, station_name + &#39;_monthly.csv&#39;) . download_data(&#39;BILBAO&#39;, &#39;SPE00120611&#39;) . df = pd.read_csv(&#39;BILBAO_daily.csv&#39;, sep=&quot;,&quot;) # make &#39;DATE&#39; the dataframe index df[&#39;DATE&#39;] = pd.to_datetime(df[&#39;DATE&#39;]) df = df.set_index(&#39;DATE&#39;) # IMPORTANT!! daily precipitation data is in tenths of mm, divide by 10 to get it in mm. df[&#39;PRCP&#39;] = df[&#39;PRCP&#39;] / 10 . fig, ax = plt.subplots(figsize=(10,7)) ax.plot(df[&#39;PRCP&#39;]) ax.set_xlabel(&quot;date&quot;) ax.set_ylabel(&quot;daily rainfall (mm)&quot;) ax.set_title(&quot;Bilbao, Spain, 1947--2021&quot;) ax.annotate(&quot;26 August 1983&quot;, xy=(&#39;1983-08-26&#39;, 2500), xycoords=&#39;data&#39;, xytext=(0.7, 0.95), textcoords=&#39;axes fraction&#39;, fontsize=16, va=&quot;center&quot;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) plt.savefig(&quot;hydrology_figures/bilbao-1947-2021.png&quot;) . from matplotlib.dates import DateFormatter import matplotlib.dates as mdates import matplotlib fig, ax = plt.subplots(figsize=(10,7)) one_week = df.loc[&#39;1983-08-22&#39;:&#39;1983-08-28&#39;, &#39;PRCP&#39;] bars = ax.bar(one_week.index, one_week) ax.set_xlabel(&quot;date&quot;) ax.set_ylabel(&quot;daily rainfall (mm)&quot;) ax.set_title(&quot;Bilbao, Spain, August 1983&quot;) # write daily rainfall for i in range(len(one_week)): ax.text(one_week.index[i], one_week[i], f&quot;{one_week[i]:.0f}&quot;, ha=&quot;center&quot;, fontsize=16) ax.text(0.1, 0.8, f&quot;Total rainfall during this week: n{one_week.sum():.0f} mm&quot;, transform=ax.transAxes, fontsize=16) # Define the date format # https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior date_form = DateFormatter(&quot;%b-%d&quot;) ax.xaxis.set_major_formatter(date_form) # Ensure a major tick for each day using (interval=1) # https://matplotlib.org/stable/api/dates_api.html#date-tickers ax.xaxis.set_major_locator(mdates.DayLocator(interval=1)) plt.gcf().autofmt_xdate() plt.savefig(&quot;hydrology_figures/bilbao-august-1983.png&quot;) . import altair as alt alt.data_transformers.disable_max_rows() df_new = df.reset_index()#.replace({0.0:np.nan}) source = df_new[[&#39;DATE&#39;, &#39;PRCP&#39;]] brush = alt.selection(type=&#39;interval&#39;, encodings=[&#39;x&#39;]) base = alt.Chart(source).mark_line().encode( x = &#39;DATE:T&#39;, y = &#39;PRCP:Q&#39; ).properties( width=600, height=200 ) upper = base.encode( alt.X(&#39;DATE:T&#39;, scale=alt.Scale(domain=brush)), alt.Y(&#39;PRCP:Q&#39;, scale=alt.Scale(domain=(0,500))) ) lower = base.properties( height=60 ).add_selection(brush) alt.vconcat(upper, lower) . df_month = df[&#39;PRCP&#39;].resample(&#39;M&#39;).sum().to_frame() month_numbers = np.arange(1,13) monthly_mean = np.array([]) # empty array for m in month_numbers: # cycle over months (1, 2, 3, etc) this_month_mean = df_month[df_month.index.month == m].mean() # this is the monthly mean monthly_mean = np.append(monthly_mean, this_month_mean) # append # make new df and return it df_month = pd.DataFrame({&#39;monthly rainfall (mm)&#39;:monthly_mean, &#39;month number&#39;:month_numbers }) df_month . monthly rainfall (mm) month number . 0 130.254054 | 1 | . 1 103.468919 | 2 | . 2 95.350667 | 3 | . 3 107.420270 | 4 | . 4 84.037838 | 5 | . 5 64.563514 | 6 | . 6 50.702703 | 7 | . 7 69.922973 | 8 | . 8 85.070270 | 9 | . 9 117.237838 | 10 | . 10 153.709459 | 11 | . 11 136.832432 | 12 | . fig, ax = plt.subplots(figsize=(10,7)) ax.bar(df_month[&#39;month number&#39;], df_month[&#39;monthly rainfall (mm)&#39;]) ax.set(xlabel=&quot;month&quot;, ylabel=&quot;monthly rainfall (mm)&quot;, title=&quot;Monthly average, Bilbao, 1947--2021&quot;, xticks=np.arange(1,13)) plt.savefig(&quot;hydrology_figures/monthly_average_bilbao.png&quot;) . Let&#39;s rank the annual max . # hydrologic year starts in August max_annual = df[&#39;PRCP&#39;].resample(&#39;A-JUL&#39;).max().to_frame() # sort yearly max from highest to lowest max_annual = max_annual.sort_values(by=[&#39;PRCP&#39;], ascending=False) max_annual[&#39;rank&#39;] = np.arange(1, len(max_annual) + 1) # print(max_annual) max_annual . PRCP rank . DATE . 1984-07-31 252.6 | 1 | . 1954-07-31 172.6 | 2 | . 1964-07-31 143.5 | 3 | . 1960-07-31 137.2 | 4 | . 2010-07-31 108.1 | 5 | . ... ... | ... | . 2004-07-31 41.1 | 71 | . 2001-07-31 38.6 | 72 | . 2021-07-31 35.8 | 73 | . 2002-07-31 28.5 | 74 | . 2011-07-31 27.0 | 75 | . 75 rows Ã— 2 columns . %matplotlib notebook fig, ax = plt.subplots(figsize=(10,6)) # plot annual max vs. rank ax.plot(max_annual[&#39;rank&#39;], max_annual[&#39;PRCP&#39;], &#39;-o&#39;) plt.gca().set(xlabel=&quot;rank&quot;, ylabel=&quot;annual max (mm)&quot;) . [Text(0.5, 0, &#39;rank&#39;), Text(0, 0.5, &#39;annual max (mm)&#39;)] . fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,8)) h=max_annual[&#39;PRCP&#39;].values ax1.hist(h, bins=np.arange(0,250,20)) ax2.hist(h, bins=np.arange(0,250,20), cumulative=1) ax1.set(ylabel=&quot;number of years&quot;) ax2.set(xlabel=&quot;annual max (mm)&quot;, ylabel=&quot;cumulative&quot;) plt.savefig(&quot;hydrology_figures/hist_count_cumulative_bilbao.png&quot;) . fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,8)) h=max_annual[&#39;PRCP&#39;].values ax1.hist(h, bins=np.arange(0,250,20), density=True) ax2.hist(h, bins=np.arange(0,250,20), cumulative=1, density=True) ax1.set(ylabel=&quot;pdf&quot;) ax2.set(xlabel=&quot;annual max (mm)&quot;, ylabel=&quot;cdf&quot;, ) ax1.text(0.5, 0.5, &quot;probability density function&quot;, transform=ax1.transAxes, fontsize=16, bbox=dict(boxstyle=&#39;round&#39;, facecolor=&#39;wheat&#39;, alpha=1)) ax2.text(0.5, 0.5, &quot;cumulative density function&quot;, transform=ax2.transAxes, fontsize=16, bbox=dict(boxstyle=&#39;round&#39;, facecolor=&#39;wheat&#39;, alpha=1)) plt.savefig(&quot;hydrology_figures/pdf_cdf_bilbao.png&quot;) . max_annual.shape[0] . 75 . Ward, Environmental Hydrology, pages 46-47, Brutsaert, Hydrology, pages 514-516 . $P_m=$ plotting position, or probability of occurence for each event $n=$ total number of events $m=$ rank of each event . Return period: . $$ text{Return period} = frac{1}{1-P_m} $$oldest, and intuitively the simplest: . $$ P_m = frac{m}{n} $$another option: . $$ P_m = frac{m-1}{n} $$Hazen: . $$ P_m = frac{m- frac{1}{2}}{n} $$Weibull: . $$ P_m = frac{m}{n+1} $$ Return Period . Brutsaert, &quot;Hydrology, an introduction&quot;, page 513 . $F(x)$ is the CDF of the PDF $f(x)$. $F(x)$ indicates the probability that a certain event above $x$ has not occurred (or that an event below $x$ has occurred, same thing). . $1-F(x)$ is the probability that a certain event above $x$ has occurred. It&#39;s reciprocal is the return period: $$ T_r(x) = frac{1}{1-F(x)} $$ . This return period is the expected number of observations required until $x$ is exceeded once. In our case, we can ask the question: how many years will pass (on average) until we see a rainfall event greater that that of 26 August 1983? . Let&#39;s call $p=F(x)$ the probability that we measured once and that an event greater than $x$ has not occurred. What is the probability that a rainfall above $x$ will occur only on year number $k$? . it hasn&#39;t occurred on year 1 (probability p) | it hasn&#39;t occurred on year 2 (probability p) | it hasn&#39;t occurred on year 3 (probability p) | ... | it has occurred on year k (probability 1-p) | . $P {k text{ trials until }X&gt;x } = p^{k-1}(1-p)$ . Every time the number $k$ will be different. What will be $k$ on average? . $$ bar{k} = displaystyle sum_{k=1}^{ infty} k P(k) = displaystyle sum_{k=1}^{ infty} k p^{k-1}(1-p)$$ . Let&#39;s open that up: . $$ begin{align} bar{k} &amp;= 1-p + 2p(1-p) + 3p^2(1-p) + 4p^3(1-p)+ cdots bar{k} &amp;= 1-p + 2p - 2p^2 + 3p^2 - 3p^4 + 4p^3 - 4p^4+ cdots bar{k} &amp;= 1 + p + p^2 + p^3 + p^4 + cdots end{align} $$For $p&lt;1$, the series converges to $$ 1 + p + p^2 + p^3 + p^4 + cdots = frac{1}{1-p}, $$ therefore $$ bar{k} = frac{1}{1-p}. $$ . from scipy.stats import gamma import scipy from matplotlib import rc rc(&#39;text&#39;, usetex=True) fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,6))#, sharex=True) a = 4 xmax = 12 b = np.arange(0,13,1) x = np.linspace(0, gamma.ppf(0.999, a), 100) # x = np.linspace(gamma.ppf(0.001, a), # gamma.ppf(0.999, a), 100) ax1.plot(x, gamma.pdf(x, a), &#39;r-&#39;, lw=5, alpha=0.6, label=&#39;gamma pdf&#39;) r = gamma.rvs(a, size=10000) ax1.hist(r, bins=b, density=True, histtype=&#39;stepfilled&#39;, alpha=0.2) ax2.hist(r, bins=b, density=True, histtype=&#39;stepfilled&#39;, alpha=0.2, cumulative=-1) ax2.plot(x, 1-scipy.special.gammainc(a, x), &#39;r-&#39;, lw=5, alpha=0.6, label=&#39;gamma cdf&#39;) quantile = lambda p: gamma.ppf(1-p, a) p = 0.05 q = quantile(p) xfill = np.linspace(q, xmax, 100) ax1.fill_between(xfill, gamma.pdf(xfill, a), color=&#39;None&#39;, hatch=&quot;//&quot;,edgecolor=&#39;k&#39;) ax2.plot([0, q, q], [p, p ,0], color=&quot;black&quot;, ls=&quot;:&quot;) ax1.annotate(r&quot; noindent probability that next occurrence is textbf{ underline{higher}} than $x^*$:&quot; + &quot; {:.0f} %&quot;.format(100*p), xy=(q+0.8, gamma.pdf(q+0.8, a)*0.6), xycoords=&#39;data&#39;, xytext=(0.7, 0.8), textcoords=&#39;axes fraction&#39;, fontsize=16, horizontalalignment=&quot;center&quot;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.0)) ax2.annotate(r&quot; noindent textbf{ underline{exceedance}} probability&quot;, xy=(q, 1-scipy.special.gammainc(a, q)), xycoords=&#39;data&#39;, xytext=(0.5, 0.9), textcoords=&#39;axes fraction&#39;, fontsize=16, horizontalalignment=&quot;center&quot;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.0)) ax2.text(q+0.1, 0, r&quot;$x^*$&quot;, ha=&quot;left&quot;, va=&quot;bottom&quot;, fontsize=16) ax2.text(0.5, p, r&quot;{:.0f} %&quot;.format(100*p), ha=&quot;left&quot;, va=&quot;bottom&quot;, fontsize=16) ax1.set_title(r&quot;we&#39;ll wait textbf{on average}&quot; + &quot; ({:.0f} %)&quot;.format(100*p) + r&quot;$^{-1}=$&quot; + r&quot; ({:.2f})&quot;.format(p) + r&quot;$^{-1}=$&quot; + &quot; {:.0f} years &quot;.format(1/p) + r&quot; for a yearly maximum above $x^*$&quot;, fontsize=16) ax1.set(xlim=[0, xmax], ylabel=&quot;probability density&quot;, xticks=[q], xticklabels=[r&quot;$x^*$&quot;]) ax2.set(xlim=[0, xmax], xlabel=r&quot;$x$&quot;, ylabel=&quot;cumulative&quot;) plt.savefig(&quot;hydrology_figures/return_prob_005.png&quot;) . import imageio files = [&#39;hydrology_figures/return_prob_050.png&#39;, &#39;hydrology_figures/return_prob_033.png&#39;, &#39;hydrology_figures/return_prob_020.png&#39;, &#39;hydrology_figures/return_prob_010.png&#39;, &#39;hydrology_figures/return_prob_005.png&#39;,] images = [imageio.imread(file) for file in files] imageio.mimwrite(&#39;hydrology_figures/movie.gif&#39;, images, fps=1) . from scipy.stats import gamma import scipy from matplotlib import rc rc(&#39;text&#39;, usetex=True) fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,6))#, sharex=True) a = 4 xmax = 12 b = np.arange(0,13,1) x = np.linspace(0, gamma.ppf(0.999, a), 100) # x = np.linspace(gamma.ppf(0.001, a), # gamma.ppf(0.999, a), 100) ax1.plot(x, gamma.pdf(x, a), &#39;r-&#39;, lw=5, alpha=0.6, label=&#39;gamma pdf&#39;) r = gamma.rvs(a, size=10000) ax1.hist(r, bins=b, density=True, histtype=&#39;stepfilled&#39;, alpha=0.2) ax2.hist(r, bins=b, density=True, histtype=&#39;stepfilled&#39;, alpha=0.2, cumulative=1) ax2.plot(x, scipy.special.gammainc(a, x), &#39;r-&#39;, lw=5, alpha=0.6, label=&#39;gamma cdf&#39;) quantile = lambda p: gamma.ppf(1-p, a) p = 0.50 q = quantile(p) xfill = np.linspace(0, q, 100) ax1.fill_between(xfill, gamma.pdf(xfill, a), color=&#39;None&#39;, hatch=&quot;//&quot;,edgecolor=&#39;k&#39;) ax2.plot([0, q, q], [p, p ,0], color=&quot;black&quot;, ls=&quot;:&quot;) ax1.annotate(r&quot; noindent probability that next occurrence is textbf{ underline{lower}} than $x^*$:&quot; + &quot; {:.0f} %&quot;.format(100*p), xy=(q-0.8, gamma.pdf(q+0.8, a)*0.6), xycoords=&#39;data&#39;, xytext=(0.7, 0.8), textcoords=&#39;axes fraction&#39;, fontsize=16, horizontalalignment=&quot;center&quot;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.0)) ax2.annotate(r&quot; noindent textbf{ underline{non-exceedance}} probability&quot;, xy=(q, 1-scipy.special.gammainc(a, q)), xycoords=&#39;data&#39;, xytext=(0.3, 0.9), textcoords=&#39;axes fraction&#39;, fontsize=16, horizontalalignment=&quot;center&quot;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.0)) ax2.text(q+0.1, 0, r&quot;$x^*$&quot;, ha=&quot;left&quot;, va=&quot;bottom&quot;, fontsize=16) ax2.text(0.5, p, r&quot;{:.0f} %&quot;.format(100*p), ha=&quot;left&quot;, va=&quot;bottom&quot;, fontsize=16) # ax1.set_title(r&quot;we&#39;ll wait textbf{on average}&quot; + # &quot; ({:.0f} %)&quot;.format(100*p) + # r&quot;$^{-1}=$&quot; + # r&quot; ({:.2f})&quot;.format(p) + # r&quot;$^{-1}=$&quot; + # &quot; {:.0f} years &quot;.format(1/p) + r&quot; for a yearly maximum below $x^*$&quot;, fontsize=16) ax2.text(0.6,0.5,r&quot;$ displaystyle F(x)= int_0^x ! !f(x) textrm{d}x$&quot;, transform=ax2.transAxes, fontsize=20) ax1.set_ylabel(r&quot;$f(x)$&quot;, rotation=&quot;horizontal&quot;, labelpad=20) ax2.set_ylabel(r&quot;$F(x)$&quot;, rotation=&quot;horizontal&quot;, labelpad=20) ax1.set(xlim=[0, xmax], xticks=[q], xticklabels=[r&quot;$x^*$&quot;]) ax2.set(xlim=[0, xmax], xlabel=r&quot;$x$&quot;) plt.savefig(&quot;hydrology_figures/return_prob_050_reversed.png&quot;) . fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 10)) # resample daily data into yearly data (maximum yearly value) max_annual = df[&#39;PRCP&#39;].resample(&#39;A-JUL&#39;).max().to_frame() # sort yearly max from highest to lowest max_annual = max_annual.sort_values(by=[&#39;PRCP&#39;], ascending=True) max_annual[&#39;rank&#39;] = np.arange(1, len(max_annual) + 1) print(max_annual) # plot annual max vs. rank ax1.plot(max_annual[&#39;rank&#39;], max_annual[&#39;PRCP&#39;], &#39;-o&#39;) ax1.set(xlabel=&quot;rank&quot;, ylabel=&quot;annual max (mm)&quot;) n = len(max_annual[&#39;rank&#39;]) m = max_annual[&#39;rank&#39;] Pm = m / (n+1) Tr = 1 / (1 - Pm) ax2.plot(max_annual[&#39;rank&#39;], Tr) ax3.plot(Tr, max_annual[&#39;PRCP&#39;]) ax2.set(xlabel=&quot;rank&quot;, ylabel=&quot;return period (y)&quot;) ax3.set(xlabel=&quot;return period (y)&quot;, ylabel=&quot;yearly maximum (mm/24h)&quot;) . PRCP rank DATE 2011-07-31 27.0 1 2002-07-31 28.5 2 2021-07-31 35.8 3 2001-07-31 38.6 4 2004-07-31 41.1 5 ... ... ... 2010-07-31 108.1 71 1960-07-31 137.2 72 1964-07-31 143.5 73 1954-07-31 172.6 74 1984-07-31 252.6 75 [75 rows x 2 columns] . [Text(0.5, 0, &#39;return period (y)&#39;), Text(0, 0.5, &#39;yearly maximum (mm/24h)&#39;)] . fig, ax = plt.subplots(figsize=(10, 7)) # resample daily data into yearly data (maximum yearly value) max_annual = df[&#39;PRCP&#39;].resample(&#39;A-JUL&#39;).max().to_frame() # sort yearly max from highest to lowest max_annual = max_annual.sort_values(by=[&#39;PRCP&#39;], ascending=True) max_annual[&#39;rank&#39;] = np.arange(1, len(max_annual) + 1) print(max_annual) # plot annual max vs. rank # ax1.plot(max_annual[&#39;rank&#39;], max_annual[&#39;PRCP&#39;], &#39;-o&#39;) # ax1.set(xlabel=&quot;rank&quot;, # ylabel=&quot;annual max (mm)&quot;) n = len(max_annual[&#39;rank&#39;]) m = max_annual[&#39;rank&#39;] Pm = m / (n+1) Tr = 1 / (1 - Pm) # ax2.plot(max_annual[&#39;rank&#39;], Tr) ax.plot(Tr, max_annual[&#39;PRCP&#39;]) # ax2.set(xlabel=&quot;rank&quot;, # ylabel=&quot;return period (y)&quot;) ax.set(xlabel=&quot;return period (y)&quot;, ylabel=&quot;annual maximum (mm/24h)&quot;) plt.savefig(&quot;hydrology_figures/annual_max_vs_return_period.png&quot;) . PRCP rank DATE 2011-07-31 27.0 1 2002-07-31 28.5 2 2021-07-31 35.8 3 2001-07-31 38.6 4 2004-07-31 41.1 5 ... ... ... 2010-07-31 108.1 71 1960-07-31 137.2 72 1964-07-31 143.5 73 1954-07-31 172.6 74 1984-07-31 252.6 75 [75 rows x 2 columns] . Tr . DATE 2011-07-31 1.013333 2002-07-31 1.027027 2021-07-31 1.041096 2001-07-31 1.055556 2004-07-31 1.070423 ... 2010-07-31 15.200000 1960-07-31 19.000000 1964-07-31 25.333333 1954-07-31 38.000000 1984-07-31 76.000000 Name: rank, Length: 75, dtype: float64 . fig, ax = plt.subplots(figsize=(10, 6)) ax.hist(h, bins=np.arange(0,250,20), cumulative=1, density=True, label=&quot;1947--2021 statistics for Bilbao&quot;) ax.plot(max_annual[&#39;PRCP&#39;], Pm, lw=3, label=&quot;Weibull plotting position&quot;) ax.set(xlabel=&quot;annual maximum (mm/24h)&quot;, ylabel=&quot;cumulative (non-exceedance) probability&quot;, xlim=[0, 250], ylim=[0, 1.2], yticks=np.arange(0,1.1,0.2)) ax.legend(loc=&quot;upper left&quot;, frameon=False) plt.savefig(&quot;hydrology_figures/weibull_plotting_position.png&quot;) . fig, ax = plt.subplots(figsize=(10, 6)) # ax.hist(h, bins=np.arange(0,2500,200), cumulative=1, density=True, label=&quot;1947--2021 statistics for Bilbao&quot;) ax.plot(max_annual[&#39;PRCP&#39;], Pm, lw=3, label=&quot;Weibull plotting position&quot;) ax.set(xlabel=&quot;annual maximum (mm/24h)&quot;, ylabel=&quot;cumulative (non-exceedance) probability&quot;)#, # xlim=[0, 2500], # ylim=[0, 1.2], # yticks=np.arange(0,1.1,0.2)) ax.set_xscale(&quot;linear&quot;) ax.legend(loc=&quot;upper left&quot;, frameon=False) . &lt;matplotlib.legend.Legend at 0x7f87d147cad0&gt; . import imageio files = [&#39;hydrology_figures/return_prob_050.png&#39;, &#39;hydrology_figures/return_prob_033.png&#39;, &#39;hydrology_figures/return_prob_020.png&#39;, &#39;hydrology_figures/return_prob_010.png&#39;, &#39;hydrology_figures/return_prob_005.png&#39;,] images = [imageio.imread(file) for file in files] imageio.mimwrite(&#39;hydrology_figures/movie.gif&#39;, images, fps=1) . from matplotlib.ticker import ScalarFormatter import matplotlib.ticker as mtick fig, ax = plt.subplots(figsize=(10, 6)) depth = max_annual[&#39;PRCP&#39;].values exc_prob = (1-Pm).values ax.plot(exc_prob, depth, lw=3, label=&quot;Weibull plotting position&quot;) ax.set(ylabel=&quot;annual maximum (mm/24h)&quot;, xlabel=&quot;exceedance probability&quot;) ax.set_xscale(&quot;log&quot;) ax.set_yscale(&quot;log&quot;) exclude = 40 depth_tofit = depth[exclude:] exc_prob_tofit = exc_prob[exclude:] # ax.plot(exc_prob_tofit, depth_tofit, &#39;o&#39;) exc_prob_tofit_log = np.log(exc_prob_tofit) depth_tofit_log = np.log(depth_tofit) slope, intercept = np.polyfit(exc_prob_tofit_log, depth_tofit_log, 1) prob = [1e-3,1-1e-3] def equation(p): return np.exp(slope*np.log(p) + intercept) # ax.plot(prob, equation(prob), lw=3, color=&quot;tab:red&quot;, alpha=0.4) one_in_x_years = np.array([5, 10, 50, 100])#, 200, 1000]) for y in one_in_x_years: ax.plot([1/y]*2, [0, equation(1/y)], color=&quot;black&quot;, ls=&quot;--&quot;) ax.text(1/y, 30, f&quot;{y}-yr event&quot;, rotation=90, ha=&quot;right&quot;, fontsize=16) ax.set_xticks([0.01, 0.02, 0.1, 0.2, 1.0]) ax.set_yticks([10, 20, 50, 100, 200, 500]) ax.yaxis.set_major_formatter(ScalarFormatter()) ax.xaxis.set_major_formatter(mtick.FuncFormatter(lambda y, _: &#39;{:.1f}%&#39;.format(y*100))) # ax.annotate(f&quot;{equation(1/5):.0f} mm&quot;, # xy=(1/5, equation(1/5)), xycoords=&#39;data&#39;, # xytext=(0.8, 0.6), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/10):.0f} mm&quot;, # xy=(1/10, equation(1/10)), xycoords=&#39;data&#39;, # xytext=(0.7, 0.7), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/50):.0f} mm&quot;, # xy=(1/50, equation(1/50)), xycoords=&#39;data&#39;, # xytext=(0.6, 0.8), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/100):.0f} mm&quot;, # xy=(1/100, equation(1/100)), xycoords=&#39;data&#39;, # xytext=(0.5, 0.9), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/200):.0f} mm&quot;, # xy=(1/200, equation(1/200)), xycoords=&#39;data&#39;, # xytext=(0.35, 0.9), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/1000):.0f} mm&quot;, # xy=(1/1000, equation(1/1000)), xycoords=&#39;data&#39;, # xytext=(0.2, 0.95), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) plt.savefig(&quot;hydrology_figures/extrapolation_exceedance1.png&quot;) . from matplotlib.ticker import ScalarFormatter import matplotlib.ticker as mtick fig, ax = plt.subplots(figsize=(10, 6)) depth = max_annual[&#39;PRCP&#39;].values exc_prob = (1-Pm).values ax.plot(exc_prob, depth, lw=3, label=&quot;Weibull plotting position&quot;) ax.set(ylabel=&quot;annual maximum (mm/24h)&quot;, xlabel=&quot;exceedance probability&quot;) ax.set_xscale(&quot;log&quot;) ax.set_yscale(&quot;log&quot;) exclude = 40 depth_tofit = depth[exclude:] exc_prob_tofit = exc_prob[exclude:] ax.plot(exc_prob_tofit, depth_tofit, &#39;o&#39;) exc_prob_tofit_log = np.log(exc_prob_tofit) depth_tofit_log = np.log(depth_tofit) slope, intercept = np.polyfit(exc_prob_tofit_log, depth_tofit_log, 1) prob = [1e-3,1-1e-3] def equation(p): return np.exp(slope*np.log(p) + intercept) # ax.plot(prob, equation(prob), lw=3, color=&quot;tab:red&quot;, alpha=0.4) one_in_x_years = np.array([5, 10, 50, 100])#, 200, 1000]) for y in one_in_x_years: ax.plot([1/y]*2, [0, equation(1/y)], color=&quot;black&quot;, ls=&quot;--&quot;) ax.text(1/y, 30, f&quot;{y}-yr event&quot;, rotation=90, ha=&quot;right&quot;, fontsize=16) ax.set_xticks([0.01, 0.02, 0.1, 0.2, 1.0]) ax.set_yticks([10, 20, 50, 100, 200, 500]) ax.yaxis.set_major_formatter(ScalarFormatter()) ax.xaxis.set_major_formatter(mtick.FuncFormatter(lambda y, _: &#39;{:.1f}%&#39;.format(y*100))) # ax.annotate(f&quot;{equation(1/5):.0f} mm&quot;, # xy=(1/5, equation(1/5)), xycoords=&#39;data&#39;, # xytext=(0.8, 0.6), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/10):.0f} mm&quot;, # xy=(1/10, equation(1/10)), xycoords=&#39;data&#39;, # xytext=(0.7, 0.7), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/50):.0f} mm&quot;, # xy=(1/50, equation(1/50)), xycoords=&#39;data&#39;, # xytext=(0.6, 0.8), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/100):.0f} mm&quot;, # xy=(1/100, equation(1/100)), xycoords=&#39;data&#39;, # xytext=(0.5, 0.9), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/200):.0f} mm&quot;, # xy=(1/200, equation(1/200)), xycoords=&#39;data&#39;, # xytext=(0.35, 0.9), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/1000):.0f} mm&quot;, # xy=(1/1000, equation(1/1000)), xycoords=&#39;data&#39;, # xytext=(0.2, 0.95), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) plt.savefig(&quot;hydrology_figures/extrapolation_exceedance2.png&quot;) . from matplotlib.ticker import ScalarFormatter import matplotlib.ticker as mtick fig, ax = plt.subplots(figsize=(10, 6)) depth = max_annual[&#39;PRCP&#39;].values exc_prob = (1-Pm).values ax.plot(exc_prob, depth, lw=3, label=&quot;Weibull plotting position&quot;) ax.set(ylabel=&quot;annual maximum (mm/24h)&quot;, xlabel=&quot;exceedance probability&quot;) ax.set_xscale(&quot;log&quot;) ax.set_yscale(&quot;log&quot;) exclude = 40 depth_tofit = depth[exclude:] exc_prob_tofit = exc_prob[exclude:] ax.plot(exc_prob_tofit, depth_tofit, &#39;o&#39;) exc_prob_tofit_log = np.log(exc_prob_tofit) depth_tofit_log = np.log(depth_tofit) slope, intercept = np.polyfit(exc_prob_tofit_log, depth_tofit_log, 1) prob = [1e-2,1-1e-3] def equation(p): return np.exp(slope*np.log(p) + intercept) ax.plot(prob, equation(prob), lw=3, color=&quot;tab:red&quot;, alpha=0.4) one_in_x_years = np.array([5, 10, 50, 100])#, 200, 1000]) for y in one_in_x_years: ax.plot([1/y]*2, [0, equation(1/y)], color=&quot;black&quot;, ls=&quot;--&quot;) ax.text(1/y, 30, f&quot;{y}-yr event&quot;, rotation=90, ha=&quot;right&quot;, fontsize=16) ax.set_xticks([0.01, 0.02, 0.1, 0.2, 1.0]) ax.set_yticks([10, 20, 50, 100, 200, 500]) ax.yaxis.set_major_formatter(ScalarFormatter()) ax.xaxis.set_major_formatter(mtick.FuncFormatter(lambda y, _: &#39;{:.1f}%&#39;.format(y*100))) # ax.annotate(f&quot;{equation(1/5):.0f} mm&quot;, # xy=(1/5, equation(1/5)), xycoords=&#39;data&#39;, # xytext=(0.8, 0.6), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/10):.0f} mm&quot;, # xy=(1/10, equation(1/10)), xycoords=&#39;data&#39;, # xytext=(0.7, 0.7), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/50):.0f} mm&quot;, # xy=(1/50, equation(1/50)), xycoords=&#39;data&#39;, # xytext=(0.6, 0.8), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/100):.0f} mm&quot;, # xy=(1/100, equation(1/100)), xycoords=&#39;data&#39;, # xytext=(0.5, 0.9), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/200):.0f} mm&quot;, # xy=(1/200, equation(1/200)), xycoords=&#39;data&#39;, # xytext=(0.35, 0.9), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/1000):.0f} mm&quot;, # xy=(1/1000, equation(1/1000)), xycoords=&#39;data&#39;, # xytext=(0.2, 0.95), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) plt.savefig(&quot;hydrology_figures/extrapolation_exceedance3.png&quot;) . from matplotlib.ticker import ScalarFormatter import matplotlib.ticker as mtick fig, ax = plt.subplots(figsize=(10, 6)) depth = max_annual[&#39;PRCP&#39;].values exc_prob = (1-Pm).values ax.plot(exc_prob, depth, lw=3, label=&quot;Weibull plotting position&quot;) ax.set(ylabel=&quot;annual maximum (mm/24h)&quot;, xlabel=&quot;exceedance probability&quot;) ax.set_xscale(&quot;log&quot;) ax.set_yscale(&quot;log&quot;) exclude = 40 depth_tofit = depth[exclude:] exc_prob_tofit = exc_prob[exclude:] ax.plot(exc_prob_tofit, depth_tofit, &#39;o&#39;) exc_prob_tofit_log = np.log(exc_prob_tofit) depth_tofit_log = np.log(depth_tofit) slope, intercept = np.polyfit(exc_prob_tofit_log, depth_tofit_log, 1) prob = [1e-3,1-1e-3] def equation(p): return np.exp(slope*np.log(p) + intercept) ax.plot(prob, equation(prob), lw=3, color=&quot;tab:red&quot;, alpha=0.4) one_in_x_years = np.array([5, 10, 50, 100, 200, 1000]) for y in one_in_x_years: ax.plot([1/y]*2, [0, equation(1/y)], color=&quot;black&quot;, ls=&quot;--&quot;) ax.text(1/y, 30, f&quot;{y}-yr event&quot;, rotation=90, ha=&quot;right&quot;, fontsize=16) ax.set_xticks([0.001, 0.005, 0.01, 0.02, 0.1, 0.2, 1.0]) ax.set_yticks([10, 20, 50, 100, 200, 500]) ax.yaxis.set_major_formatter(ScalarFormatter()) ax.xaxis.set_major_formatter(mtick.FuncFormatter(lambda y, _: &#39;{:.1f}%&#39;.format(y*100))) ax.annotate(f&quot;{equation(1/5):.0f} mm&quot;, xy=(1/5, equation(1/5)), xycoords=&#39;data&#39;, xytext=(0.8, 0.6), textcoords=&#39;axes fraction&#39;, fontsize=16, horizontalalignment=&quot;center&quot;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) ax.annotate(f&quot;{equation(1/10):.0f} mm&quot;, xy=(1/10, equation(1/10)), xycoords=&#39;data&#39;, xytext=(0.7, 0.7), textcoords=&#39;axes fraction&#39;, fontsize=16, horizontalalignment=&quot;center&quot;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) ax.annotate(f&quot;{equation(1/50):.0f} mm&quot;, xy=(1/50, equation(1/50)), xycoords=&#39;data&#39;, xytext=(0.6, 0.8), textcoords=&#39;axes fraction&#39;, fontsize=16, horizontalalignment=&quot;center&quot;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) ax.annotate(f&quot;{equation(1/100):.0f} mm&quot;, xy=(1/100, equation(1/100)), xycoords=&#39;data&#39;, xytext=(0.5, 0.9), textcoords=&#39;axes fraction&#39;, fontsize=16, horizontalalignment=&quot;center&quot;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) ax.annotate(f&quot;{equation(1/200):.0f} mm&quot;, xy=(1/200, equation(1/200)), xycoords=&#39;data&#39;, xytext=(0.35, 0.9), textcoords=&#39;axes fraction&#39;, fontsize=16, horizontalalignment=&quot;center&quot;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) ax.annotate(f&quot;{equation(1/1000):.0f} mm&quot;, xy=(1/1000, equation(1/1000)), xycoords=&#39;data&#39;, xytext=(0.2, 0.95), textcoords=&#39;axes fraction&#39;, fontsize=16, horizontalalignment=&quot;center&quot;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) plt.savefig(&quot;hydrology_figures/extrapolation_exceedance4.png&quot;) . df7days = (df[&#39;PRCP&#39;].rolling(3) .sum() .dropna() ) annual_7days = (df7days.resample(&#39;A-JUL&#39;) .max() .to_frame() ) annual_7days = annual_7days.reset_index() annual_7days = (annual_7days.iloc[1:-1] .sort_values(by=[&#39;PRCP&#39;], ascending=True) ) annual_7days[&#39;rank&#39;] = np.arange(1, len(annual_7days) + 1) n = len(annual_7days[&#39;rank&#39;]) m = annual_7days[&#39;rank&#39;] Pm = m / (n+1) fig, ax = plt.subplots(figsize=(10, 6)) depth = annual_7days[&#39;PRCP&#39;].values exc_prob = (1-Pm).values ax.plot(exc_prob, depth, &#39;-o&#39;, lw=3, label=&quot;Weibull plotting position&quot;) ax.set(ylabel=&quot;annual maximum (mm/week)&quot;, xlabel=&quot;exceedance probability&quot;) ax.set_xscale(&quot;log&quot;) ax.set_yscale(&quot;log&quot;) exclude = 40 depth_tofit = depth[exclude:-1] exc_prob_tofit = exc_prob[exclude:-1] ax.plot(exc_prob_tofit, depth_tofit, &#39;o&#39;) exc_prob_tofit_log = np.log(exc_prob_tofit) depth_tofit_log = np.log(depth_tofit) slope, intercept = np.polyfit(exc_prob_tofit_log, depth_tofit_log, 1) prob = [1e-3,1-1e-3] def equation(p): return np.exp(slope*np.log(p) + intercept) ax.plot(prob, equation(prob), lw=3, color=&quot;tab:red&quot;, alpha=0.4) ax.set_xticks([0.001, 0.005, 0.01, 0.02, 0.1, 0.2, 1.0]) ax.set_yticks([50, 100, 200, 500]) ax.yaxis.set_major_formatter(mtick.FuncFormatter(lambda y, _: &#39;{:.0f}&#39;.format(y))) ax.xaxis.set_major_formatter(mtick.FuncFormatter(lambda x, _: &#39;{:.1f}%&#39;.format(x*100))) . df7days = (df[&#39;PRCP&#39;].rolling(3) .sum() .dropna() ) annual_7days = (df7days.resample(&#39;A-JUL&#39;) .max() .to_frame() ) annual_7days = annual_7days.reset_index() annual_7days = (annual_7days.iloc[1:-1] .sort_values(by=[&#39;PRCP&#39;], ascending=True) ) annual_7days[&#39;rank&#39;] = np.arange(1, len(annual_7days) + 1) n = len(annual_7days[&#39;rank&#39;]) m = annual_7days[&#39;rank&#39;] Pm = m / (n+1) fig, ax = plt.subplots(figsize=(10, 6)) depth = annual_7days[&#39;PRCP&#39;].values exc_prob = (1-Pm).values ret_time = 1 / exc_prob ax.plot(ret_time, depth, &#39;-o&#39;, lw=3, label=&quot;Weibull plotting position&quot;) ax.set(ylabel=&quot;annual maximum (mm/week)&quot;, xlabel=&quot;return time (year)&quot;) ax.set_xscale(&quot;log&quot;) ax.set_yscale(&quot;log&quot;) # exclude = 40 # depth_tofit = depth[exclude:-1] # exc_prob_tofit = exc_prob[exclude:-1] # ax.plot(exc_prob_tofit, depth_tofit, &#39;o&#39;) # exc_prob_tofit_log = np.log(exc_prob_tofit) # depth_tofit_log = np.log(depth_tofit) # slope, intercept = np.polyfit(exc_prob_tofit_log, depth_tofit_log, 1) # prob = [1e-3,1-1e-3] # def equation(p): # return np.exp(slope*np.log(p) + intercept) # ax.plot(prob, equation(prob), lw=3, color=&quot;tab:red&quot;, alpha=0.4) # ax.set_xticks([0.001, 0.005, 0.01, 0.02, 0.1, 0.2, 1.0]) # ax.set_yticks([500, 1000, 2000, 5000]) # ax.yaxis.set_major_formatter(mtick.FuncFormatter(lambda y, _: &#39;{:.0f}&#39;.format(y))) # ax.xaxis.set_major_formatter(mtick.FuncFormatter(lambda x, _: &#39;{:.1f} %&#39;.format(x*100))) .",
            "url": "https://yairmau.github.io/website/jupyter/2020/02/02/return-period-code.html",
            "relUrl": "/jupyter/2020/02/02/return-period-code.html",
            "date": " â€¢ Feb 2, 2020"
        }
        
    
  
    
        ,"post48": {
            "title": "Intra-annual variability of precipitation",
            "content": ". Let&#39;s shift the months according the the hydrological year: . . Seasonality Index, Walsh and Lawler (1981) . http://leddris.aegean.gr/ses-parameters/293-rainfall-seasonality.html#:~:text=Rainfall%20seasonality%20index%20is%20a,in%20relation%20to%20water%20availability . $R=$ mean annual precipitation $m_i$ precipitation mean for month $i$ . $$ SI = displaystyle frac{1}{R} sum_{n=1}^{n=12} left| m_i - frac{R}{12} right| $$ . $SI$ Precipitation Regime . &lt;0.19 | Precipitation spread throughout the year | . 0.20-0.39 | Precipitation spread throughout the year, but with a definite wetter season | . 0.40-0.59 | Rather seasonal with a short dry season | . 0.60-0.79 | Seasonal | . 0.80-0.99 | Marked seasonal with a long dry season | . 1.00-1.19 | Most precipitation in &lt;3 months | . # import packages import numpy as np import pandas as pd from calendar import month_abbr # load data month_numbers = np.arange(1,13) month_names = [month_abbr[i] for i in month_numbers] def monthly_mean(station_name, freq): # import daily data df = pd.read_csv(station_name + &#39;_&#39; + freq + &#39;.csv&#39;, sep=&quot;,&quot;) # make &#39;DATE&#39; the dataframe index df[&#39;DATE&#39;] = pd.to_datetime(df[&#39;DATE&#39;]) df = df.set_index(&#39;DATE&#39;) # print(df.index[0], df.index[-1]) if freq == &#39;daily&#39;: # resample data by month df_month = df[&#39;PRCP&#39;].resample(&#39;M&#39;).sum() # sum is labeled at the last day of the month df_month = df_month/10 # PRCP is given in tens of mm (see readme) if freq == &#39;monthly&#39;: df_month = df[&#39;PRCP&#39;] # calculate monthly mean monthly_mean = np.array([]) # empty array for m in month_numbers: # cycle over months (1, 2, 3, etc) this_month_all_indices = (df_month.index.month == m) # indices in df_month belonging to month m this_month_mean = df_month[this_month_all_indices].mean() # this is the monthly mean monthly_mean = np.append(monthly_mean, this_month_mean) # append # make new df and return it df_return = pd.DataFrame({&#39;monthly rainfall (mm)&#39;:monthly_mean, &#39;month names&#39;:month_names, &#39;month number&#39;:month_numbers }) return df_return # load monthly mean df_london = monthly_mean(&quot;LONDON HEATHROW&quot;, &#39;monthly&#39;) df_telaviv = monthly_mean(&quot;TEL AVIV READING&quot;, &#39;monthly&#39;) #collapse-hide def walsh_index(df): m = df[&quot;monthly rainfall (mm)&quot;] R = df[&quot;monthly rainfall (mm)&quot;].sum() SI = np.sum(np.abs(m-R/12)) / R return SI london_index = walsh_index(df_london) telaviv_index = walsh_index(df_telaviv) print(&quot;Seasonality index (Walsh and Lawler, 1981)&quot;) print(f&quot;London: {london_index:.2f}&quot;) print(f&quot;Tel Aviv: {telaviv_index:.2f}&quot;) . . Seasonality index (Walsh and Lawler, 1981) London: 0.13 Tel Aviv: 1.00 . .",
            "url": "https://yairmau.github.io/website/jupyter/2020/02/02/intra-annual-variability-of-precipitation-seasonality-lecture.html",
            "relUrl": "/jupyter/2020/02/02/intra-annual-variability-of-precipitation-seasonality-lecture.html",
            "date": " â€¢ Feb 2, 2020"
        }
        
    
  
    
        ,"post49": {
            "title": "Intra-annual variability of precipitation - code",
            "content": "import matplotlib.pyplot as plt import numpy as np import pandas as pd from calendar import month_abbr import seaborn as sns sns.set(style=&quot;ticks&quot;, font_scale=1.5) import urllib.request . import data . Go to NOAA&#39;s National Centers for Environmental Information (NCEI) Climate Data Online: Dataset Discovery . Find station codes in this map . def download_data(station_name, station_code): url_daily = &#39;https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access/&#39; url_monthly = &#39;https://www.ncei.noaa.gov/data/gsom/access/&#39; # download daily data urllib.request.urlretrieve(url_daily + station_code + &#39;.csv&#39;, station_name + &#39;_daily.csv&#39;) # download monthly data urllib.request.urlretrieve(url_monthly + station_code + &#39;.csv&#39;, station_name + &#39;_monthly.csv&#39;) . calculate monthly averages . month_numbers = np.arange(1,13) month_names = [month_abbr[i] for i in month_numbers] def monthly_mean(station_name, freq): # import daily data df = pd.read_csv(station_name + &#39;_&#39; + freq + &#39;.csv&#39;, sep=&quot;,&quot;) # make &#39;DATE&#39; the dataframe index df[&#39;DATE&#39;] = pd.to_datetime(df[&#39;DATE&#39;]) df = df.set_index(&#39;DATE&#39;) print(df.index[0], df.index[-1]) if freq == &#39;daily&#39;: # resample data by month df_month = df[&#39;PRCP&#39;].resample(&#39;M&#39;).sum() # sum is labeled at the last day of the month df_month = df_month/10 # PRCP is given in tens of mm (see readme) if freq == &#39;monthly&#39;: df_month = df[&#39;PRCP&#39;] # calculate monthly mean monthly_mean = np.array([]) # empty array for m in month_numbers: # cycle over months (1, 2, 3, etc) this_month_all_indices = (df_month.index.month == m) # indices in df_month belonging to month m this_month_mean = df_month[this_month_all_indices].mean() # this is the monthly mean monthly_mean = np.append(monthly_mean, this_month_mean) # append # make new df and return it df_return = pd.DataFrame({&#39;monthly rainfall (mm)&#39;:monthly_mean, &#39;month names&#39;:month_names, &#39;month number&#39;:month_numbers }) return df_return . # download_data(&#39;TEL AVIV READING&#39;, &#39;IS000002011&#39;) # download_data(&#39;SAO PAULO&#39;, &#39;BR00E3-0520&#39;) . df_london = monthly_mean(&quot;LONDON HEATHROW&quot;, &#39;monthly&#39;) df_telaviv = monthly_mean(&quot;TEL AVIV READING&quot;, &#39;monthly&#39;) df_saopaulo = monthly_mean(&quot;SAO PAULO&quot;, &#39;monthly&#39;) total_london = df_london[&#39;monthly rainfall (mm)&#39;].sum() total_telaviv = df_telaviv[&#39;monthly rainfall (mm)&#39;].sum() . 1973-01-01 00:00:00 2021-02-01 00:00:00 1939-11-01 00:00:00 1999-11-01 00:00:00 1940-04-01 00:00:00 2021-02-01 00:00:00 . fig, ax = plt.subplots(figsize=(10,7)) # bar plots ax.bar(df_london[&#39;month number&#39;], df_london[&#39;monthly rainfall (mm)&#39;], alpha=0.5, color=&quot;blue&quot;, label=f&quot;London ({total_london:.0f} mm per year)&quot;) ax.bar(df_telaviv[&#39;month number&#39;], df_telaviv[&#39;monthly rainfall (mm)&#39;], alpha=0.5, color=&quot;red&quot;, width=0.5, label=f&quot;Tel Aviv ({total_telaviv:.0f} mm per year)&quot;) # axes labels and figure title ax.set_xlabel(&#39;month&#39;) ax.set_ylabel(&#39;monthly rainfall average (mm)&#39;) ax.set_title(&#39;seasonality of two cities with similar yearly rainfall&#39;) # some ticks adjustments ax.set_xticks(month_numbers) plt.legend(loc=&#39;upper center&#39;) # save figure plt.savefig(&quot;hydrology_figures/monthly_tel_aviv_london_bars.png&quot;) . fig, ax = plt.subplots(figsize=(10,7)) # hydrological year starts in October, roll data by 9 months months = df_london[&#39;month number&#39;] months_to_roll = -9 # minus sign = roll data to the left london_rolled = np.roll(df_london[&#39;monthly rainfall (mm)&#39;], months_to_roll) telaviv_rolled = np.roll(df_telaviv[&#39;monthly rainfall (mm)&#39;], months_to_roll) months_rolled = np.roll(months, months_to_roll) months_rolled_str = [str(x) for x in months_rolled] # bar plots ax.bar(months, london_rolled, alpha=0.5, color=&quot;blue&quot;, label=f&quot;London ({total_london:.0f} mm per year)&quot;) ax.bar(months, telaviv_rolled, alpha=0.5, color=&quot;red&quot;, width=0.5, label=f&quot;Tel Aviv ({total_telaviv:.0f} mm per year)&quot;) # axes labels and figure title ax.set_xlabel(&#39;month&#39;) ax.set_ylabel(&#39;monthly rainfall average (mm)&#39;) ax.set_title(&#39;seasonality of two cities with similar yearly rainfall&#39;) # some ticks adjustments # ax.set_xticks(months) ax.set_xticklabels(months_rolled_str) plt.legend(loc=&#39;upper right&#39;) # save figure plt.savefig(&quot;hydrology_figures/monthly_tel_aviv_london_bars_hydrological_year.png&quot;) . fig = plt.figure(figsize=(10,10)) # radar chart ax = fig.add_subplot(111, polar=True) # make polar plot ax.set_theta_zero_location(&quot;N&quot;) # January on top (&quot;N&quot;orth) ax.set_theta_direction(-1) # clockwise direction ax.set_rlabel_position(90) # radial labels on the right ax.set_rticks([50,100]) # two radial ticks is enough ax.set_rlim(0,150) # limits of r axis angles=np.linspace(0, 2*np.pi, 12, endpoint=False) # divide circle into 12 slices angles=np.append(angles, angles[0]) # close loop, otherwise lines will be open ax.set_thetagrids(angles[:-1] * 180/np.pi, month_names) # relabel angles with month names # plot london data stats_london = np.array(df_london[&#39;monthly rainfall (mm)&#39;]) # get london data stats_london = np.append(stats_london, stats_london[0]) # close loop ax.plot(angles, stats_london, &quot;o-&quot;, color=&#39;blue&#39;, label=&quot;london&quot;) # plot line ax.fill(angles, stats_london, alpha=0.25, color=&#39;blue&#39;) # fill # plot tel aviv data stats_telaviv = np.array(df_telaviv[&#39;monthly rainfall (mm)&#39;]) # get tel aviv data stats_telaviv = np.append(stats_telaviv, stats_telaviv[0]) # close loop ax.plot(angles, stats_telaviv, &quot;o-&quot;, color=&#39;red&#39;, label=&quot;tel aviv&quot;) # plot line ax.fill(angles, stats_telaviv, alpha=0.25, color=&#39;red&#39;) # fill # plot sao paulo data # stats_saopaulo = np.array(df_saopaulo[&#39;monthly rainfall (mm)&#39;]) # get tel aviv data # stats_saopaulo = np.append(stats_saopaulo, stats_saopaulo[0]) # close loop # ax.plot(angles, stats_saopaulo, &quot;o-&quot;, color=&#39;green&#39;, label=&quot;sao paulo&quot;) # plot line # ax.fill(angles, stats_saopaulo, alpha=0.25, color=&#39;green&#39;) # fill ax.set_title(&quot;Monthly rainfall averages&quot;) ax.legend(loc=(-0.1,0.9)) # legend at x=-0.2 so it doesn&#39;t overlap with graph # save figure plt.savefig(&quot;hydrology_figures/radar_chart_tel_aviv_london.png&quot;) . station_name = &quot;LONDON HEATHROW&quot; freq = &#39;daily&#39; df_day = pd.read_csv(station_name + &#39;_&#39; + freq + &#39;.csv&#39;, sep=&quot;,&quot;) # make &#39;DATE&#39; the dataframe index df_day[&#39;DATE&#39;] = pd.to_datetime(df_day[&#39;DATE&#39;]) df_day = df_day.set_index(&#39;DATE&#39;) # resample data by month df_month1 = df_day[&#39;PRCP&#39;].resample(&#39;MS&#39;).sum() # sum is labeled at the last day of the month df_month1 = df_month1/10 # PRCP is given in tens of mm (see readme) freq = &#39;monthly&#39; df_mon = pd.read_csv(station_name + &#39;_&#39; + freq + &#39;.csv&#39;, sep=&quot;,&quot;) # make &#39;DATE&#39; the dataframe index df_mon[&#39;DATE&#39;] = pd.to_datetime(df_mon[&#39;DATE&#39;]) df_mon = df_mon.set_index(&#39;DATE&#39;) # resample data by month df_month2 = df_mon[&#39;PRCP&#39;] . %matplotlib notebook plt.plot(df_month1, label=&#39;from daily data&#39;) plt.plot(df_month2, label=&#39;from monthly data&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f93c9228cd0&gt; . %matplotlib notebook df = pd.read_csv(&quot;TEL AVIV READING_monthly.csv&quot;, sep=&quot;,&quot;) df[&#39;DATE&#39;] = pd.to_datetime(df[&#39;DATE&#39;]) df = df.set_index(&#39;DATE&#39;) df_by_year = df.groupby([df.index.year]) for year_number, df_year in df_by_year: year_precipitation = df_year[&#39;PRCP&#39;].values year_precipitation = np.roll(year_precipitation, 6) month_numbers = np.arange(1,len(year_precipitation)+1) month_numbers_roll = np.roll(month_numbers, 6) plt.plot(month_numbers, year_precipitation, color=&quot;gray&quot;, alpha=0.2) month_numbers = np.arange(1,12+1) month_numbers_roll = np.roll(month_numbers, 6) plt.xticks(month_numbers, month_numbers_roll) . ([&lt;matplotlib.axis.XTick at 0x7f83097830d0&gt;, &lt;matplotlib.axis.XTick at 0x7f82f83d3f10&gt;, &lt;matplotlib.axis.XTick at 0x7f82f83d3390&gt;, &lt;matplotlib.axis.XTick at 0x7f82e86780d0&gt;, &lt;matplotlib.axis.XTick at 0x7f835ad1a610&gt;, &lt;matplotlib.axis.XTick at 0x7f835acf4610&gt;, &lt;matplotlib.axis.XTick at 0x7f835acf4ed0&gt;, &lt;matplotlib.axis.XTick at 0x7f835acf42d0&gt;, &lt;matplotlib.axis.XTick at 0x7f835acf4510&gt;, &lt;matplotlib.axis.XTick at 0x7f835ad0f590&gt;, &lt;matplotlib.axis.XTick at 0x7f835ad0fa90&gt;, &lt;matplotlib.axis.XTick at 0x7f83096f7090&gt;], [Text(1, 0, &#39;7&#39;), Text(2, 0, &#39;8&#39;), Text(3, 0, &#39;9&#39;), Text(4, 0, &#39;10&#39;), Text(5, 0, &#39;11&#39;), Text(6, 0, &#39;12&#39;), Text(7, 0, &#39;1&#39;), Text(8, 0, &#39;2&#39;), Text(9, 0, &#39;3&#39;), Text(10, 0, &#39;4&#39;), Text(11, 0, &#39;5&#39;), Text(12, 0, &#39;6&#39;)]) . np.roll(np.arange(12),6) . array([ 6, 7, 8, 9, 10, 11, 0, 1, 2, 3, 4, 5]) . Seasonality Index, Walsh and Lawler (1981) . http://leddris.aegean.gr/ses-parameters/293-rainfall-seasonality.html#:~:text=Rainfall%20seasonality%20index%20is%20a,in%20relation%20to%20water%20availability . SI Precipitation Regime . &lt;0.19 | Precipitation spread throughout the year | . 0.20-0.39 | Precipitation spread throughout the year, but with a definite wetter season | . 0.40-0.59 | Rather seasonal with a short dry season | . 0.60-0.79 | Seasonal | . 0.80-0.99 | Marked seasonal with a long dry season | . 1.00-1.19 | Most precipitation in &lt;3 months | . def walsh_index(df): X = df[&quot;monthly rainfall (mm)&quot;] Ri = df[&quot;monthly rainfall (mm)&quot;].sum() SI = np.sum(np.abs(X-Ri/12)) / Ri return SI london_index = walsh_index(df_london) telaviv_index = walsh_index(df_telaviv) print(&quot;london seasonality: t&quot;, london_index) print(&quot;tel aviv seasonality: t&quot;, telaviv_index) print(&quot;ratio: t&quot;, telaviv_index/london_index) . london seasonality: 0.114678755577802 tel aviv seasonality: 1.0004295512696868 ratio: 8.723756603645398 . fig, ax = plt.subplots(figsize=(10,7)) plt.rcParams[&#39;hatch.linewidth&#39;] = 3 # hydrological year starts in October, roll data by 9 months months = df_london[&#39;month number&#39;] months_to_roll = -9 # minus sign = roll data to the left # london_rolled = np.roll(df_london[&#39;monthly rainfall (mm)&#39;], months_to_roll) telaviv_rolled = np.roll(df_telaviv[&#39;monthly rainfall (mm)&#39;], months_to_roll) months_rolled = np.roll(months, months_to_roll) months_rolled_str = [str(x) for x in months_rolled] # bar plots # ax.bar(months, london_rolled, # alpha=0.5, color=&quot;blue&quot;, # label=f&quot;London ({total_london:.0f} mm per year)&quot;) xlim = [1, 13] ax.plot(xlim, [total_telaviv/12]*2, color=&quot;tab:blue&quot;, linewidth=3) ax.set_xlim(xlim) shaded = telaviv_rolled - total_telaviv/12 ax.bar(months, shaded, alpha=0.9, color=&quot;None&quot;, width=1, hatch=&quot;//&quot;, edgecolor=&#39;k&#39;, align=&#39;edge&#39;, bottom=total_telaviv/12, label=f&quot;absolute difference&quot;) ax.bar(months, telaviv_rolled, alpha=0.5, color=&quot;red&quot;, width=1, align=&#39;edge&#39;, label=f&quot;total rainfall&quot;, zorder=0) ax.text(5.3, 86.5, r&quot;SI$=1.00=$&quot;, fontsize=24) ax.text(xlim[-1], total_telaviv/12, &quot; mean&quot;, va=&quot;center&quot;) ax.plot([8.2, 12.8], [89.5]*2, color=&quot;black&quot;, lw=2) # # axes labels and figure title ax.set_xlabel(&#39;month&#39;) ax.set_ylabel(&#39;monthly rainfall average (mm)&#39;) ax.set_title(&#39;Walsh and Lawler (1981) Seasonality Index; Tel Aviv&#39;) ax.set_xticks(np.arange(1.5,12.6,1)) ax.set_xticklabels(months_rolled_str) # ax.set_title(&#39;seasonality of two cities with similar yearly rainfall&#39;) # # some ticks adjustments # # # ax.set_xticks(months) # ax.set_xticklabels(months_rolled_str) plt.legend(loc=&#39;upper right&#39;, frameon=False, bbox_to_anchor=(1, 0.7), fontsize=18) # save figure plt.savefig(&quot;hydrology_figures/si_walsh_telaviv.png&quot;) . fig, ax = plt.subplots(figsize=(10,7)) plt.rcParams[&#39;hatch.linewidth&#39;] = 3 # hydrological year starts in October, roll data by 9 months months = df_london[&#39;month number&#39;] months_to_roll = -9 # minus sign = roll data to the left london_rolled = np.roll(df_london[&#39;monthly rainfall (mm)&#39;], months_to_roll) # telaviv_rolled = np.roll(df_telaviv[&#39;monthly rainfall (mm)&#39;], months_to_roll) months_rolled = np.roll(months, months_to_roll) months_rolled_str = [str(x) for x in months_rolled] xlim = [1, 13] ax.plot(xlim, [total_london/12]*2, color=&quot;tab:blue&quot;, linewidth=3) ax.set_xlim(xlim) shaded = london_rolled - total_london/12 ax.bar(months, shaded, alpha=0.9, color=&quot;None&quot;, width=1, hatch=&quot;//&quot;, edgecolor=&#39;k&#39;, align=&#39;edge&#39;, bottom=total_london/12, label=f&quot;absolute difference&quot;) ax.bar(months, london_rolled, alpha=0.5, color=&quot;red&quot;, width=1, align=&#39;edge&#39;, label=f&quot;total rainfall&quot;, zorder=0) ax.text(5.3, 73.5, r&quot;SI$=0.13=$&quot;, fontsize=24) ax.text(xlim[-1], total_london/12, &quot; mean&quot;, va=&quot;center&quot;) ax.plot([8.2, 12.8], [75]*2, color=&quot;black&quot;, lw=2) # # axes labels and figure title ax.set_xlabel(&#39;month&#39;) ax.set_ylabel(&#39;monthly rainfall average (mm)&#39;) ax.set_title(&#39;Walsh and Lawler (1981) Seasonality Index; London&#39;) ax.set_xticks(np.arange(1.5,12.6,1)) ax.set_xticklabels(months_rolled_str) ax.set_ylim([0, 83]) # ax.set_title(&#39;seasonality of two cities with similar yearly rainfall&#39;) # # some ticks adjustments # # # ax.set_xticks(months) # ax.set_xticklabels(months_rolled_str) plt.legend(loc=&#39;upper right&#39;, frameon=False, bbox_to_anchor=(1, 1.005), fontsize=18) # save figure plt.savefig(&quot;hydrology_figures/si_walsh_london.png&quot;) .",
            "url": "https://yairmau.github.io/website/jupyter/2020/02/02/intra-annual-variability-of-precipitation-code.html",
            "relUrl": "/jupyter/2020/02/02/intra-annual-variability-of-precipitation-code.html",
            "date": " â€¢ Feb 2, 2020"
        }
        
    
  
    
        ,"post50": {
            "title": "Interannual variability of precipitation",
            "content": ". hydrological year . A time period of 12 months for which precipitation totals are measured. The hydrological year is designated by the calendar year in which it ends. Let&#39;s define the hydrological year for Tel Aviv from 1 October to 30 September. . ×”×× ××§×œ×™× ×”×’×©× ×©×œ× ×• ××©×ª× ×” . . coefficient of variation . $ langle{P} rangle=$ average precipitation $ sigma=$ standard deviation . $$CV = frac{ sigma}{ langle{P} rangle}$$ . Assuming that the inter-annual distribution is a gaussian: 67% of the time, rainfall will vary +/- 30% from its long term average in Tel Aviv. . Precipitation averages are usually calculated for time intervals of 30 years. . . . import altair as alt import pandas as pd df = pd.read_csv(&quot;TEL_AVIV_READING_monthly.csv&quot;, sep=&quot;,&quot;) # make &#39;DATE&#39; the dataframe index df[&#39;DATE&#39;] = pd.to_datetime(df[&#39;DATE&#39;]) df = df.set_index(&#39;DATE&#39;) df_year_all = df[&#39;PRCP&#39;].resample(&#39;A-SEP&#39;).sum().to_frame() # annual frequency, anchored end of September df_year_all.columns = [&#39;rain (mm)&#39;] # rename &#39;PRCP&#39; column to &#39;rain (mm)&#39; df_year = df_year_all.iloc[:-1] # exclude last row # Altair only recognizes column data; it ignores index values. # You can plot the index data by first resetting the index # I know that I&#39;ve just made &#39;DATE&#39; the index, but I want to have this here nonetheless so I can refer to this in the future source = df_year.reset_index() brush = alt.selection(type=&#39;interval&#39;, encodings=[&#39;x&#39;]) # T: temporal, a time or date value # Q: quantitative, a continuous real-valued quantity # https://altair-viz.github.io/user_guide/encoding.html#encoding-data-types bars = alt.Chart().mark_bar().encode( x=alt.X(&#39;DATE:T&#39;, axis=alt.Axis(title=&#39;date&#39;)), y=alt.Y(&#39;rain (mm):Q&#39;, axis=alt.Axis(title=&#39;annual precipitation (mm) and average&#39;)), opacity=alt.condition(brush, alt.OpacityValue(1), alt.OpacityValue(0.2)), ).add_selection( brush ).properties( title=&#39;Select year range and drag for rolling average of annual precipitation in Tel Aviv&#39; ).properties( width=600, height=400 ) line = alt.Chart().mark_rule(color=&#39;orange&#39;).encode( y=&#39;mean(rain (mm)):Q&#39;, size=alt.SizeValue(3) ).transform_filter( brush ) alt.layer(bars, line, data=source) . . .",
            "url": "https://yairmau.github.io/website/jupyter/2020/02/02/interannual-variability-of-precipitation-lecture.html",
            "relUrl": "/jupyter/2020/02/02/interannual-variability-of-precipitation-lecture.html",
            "date": " â€¢ Feb 2, 2020"
        }
        
    
  
    
        ,"post51": {
            "title": "Interannual variability of precipitation - code",
            "content": "import matplotlib.pyplot as plt import matplotlib import numpy as np import pandas as pd from pandas.plotting import register_matplotlib_converters register_matplotlib_converters() # datetime converter for a matplotlib from calendar import month_abbr import seaborn as sns sns.set(style=&quot;ticks&quot;, font_scale=1.5) import urllib.request . def download_data(station_name, station_code): url_daily = &#39;https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access/&#39; url_monthly = &#39;https://www.ncei.noaa.gov/data/gsom/access/&#39; # download daily data urllib.request.urlretrieve(url_daily + station_code + &#39;.csv&#39;, station_name + &#39;_daily.csv&#39;) # download monthly data urllib.request.urlretrieve(url_monthly + station_code + &#39;.csv&#39;, station_name + &#39;_monthly.csv&#39;) . df = pd.read_csv(&quot;TEL AVIV READING_monthly.csv&quot;, sep=&quot;,&quot;) # make &#39;DATE&#39; the dataframe index df[&#39;DATE&#39;] = pd.to_datetime(df[&#39;DATE&#39;]) df = df.set_index(&#39;DATE&#39;) df . STATION LATITUDE LONGITUDE ELEVATION NAME CDSD CDSD_ATTRIBUTES CLDD CLDD_ATTRIBUTES DP01 ... HTDD HTDD_ATTRIBUTES PRCP PRCP_ATTRIBUTES TAVG TAVG_ATTRIBUTES TMAX TMAX_ATTRIBUTES TMIN TMIN_ATTRIBUTES . DATE . 1939-11-01 IS000002011 | 32.1 | 34.7831 | 3.0 | TEL AVIV READING, IS | NaN | NaN | NaN | NaN | 7.0 | ... | NaN | NaN | 106.5 | ,,,E | NaN | NaN | NaN | NaN | NaN | NaN | . 1939-12-01 IS000002011 | 32.1 | 34.7831 | 3.0 | TEL AVIV READING, IS | NaN | NaN | NaN | NaN | 7.0 | ... | NaN | NaN | 100.1 | ,,,E | NaN | NaN | NaN | NaN | NaN | NaN | . 1940-01-01 IS000002011 | 32.1 | 34.7831 | 3.0 | TEL AVIV READING, IS | NaN | NaN | NaN | NaN | 11.0 | ... | NaN | NaN | 181.5 | ,,,E | NaN | NaN | NaN | NaN | NaN | NaN | . 1940-02-01 IS000002011 | 32.1 | 34.7831 | 3.0 | TEL AVIV READING, IS | NaN | NaN | NaN | NaN | 6.0 | ... | NaN | NaN | 57.7 | ,,,E | NaN | NaN | NaN | NaN | NaN | NaN | . 1940-03-01 IS000002011 | 32.1 | 34.7831 | 3.0 | TEL AVIV READING, IS | NaN | NaN | NaN | NaN | 7.0 | ... | NaN | NaN | 27.2 | ,,,E | NaN | NaN | NaN | NaN | NaN | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1999-05-01 IS000002011 | 32.1 | 34.7831 | 3.0 | TEL AVIV READING, IS | 165.9 | NaN | 125.0 | ,I | NaN | ... | 0.0 | ,I | NaN | NaN | 22.37 | ,I | 26.15 | ,,,I | 18.58 | ,,,I | . 1999-06-01 IS000002011 | 32.1 | 34.7831 | 3.0 | TEL AVIV READING, IS | 367.1 | NaN | 201.2 | 1,I | NaN | ... | 0.0 | 1,I | NaN | NaN | 25.29 | 1,I | 28.93 | 1,,,I | 21.65 | ,,,I | . 1999-07-01 IS000002011 | 32.1 | 34.7831 | 3.0 | TEL AVIV READING, IS | 612.0 | NaN | 244.9 | 4,I | NaN | ... | 0.0 | 4,I | NaN | NaN | 27.44 | 4,I | 31.09 | 4,,,I | 23.80 | ,,,I | . 1999-08-01 IS000002011 | 32.1 | 34.7831 | 3.0 | TEL AVIV READING, IS | 921.4 | NaN | 309.4 | ,I | NaN | ... | 0.0 | ,I | NaN | NaN | 28.31 | ,I | 31.54 | ,,,I | 25.09 | ,,,I | . 1999-11-01 IS000002011 | 32.1 | 34.7831 | 3.0 | TEL AVIV READING, IS | NaN | NaN | 62.1 | ,I | NaN | ... | 13.3 | ,I | NaN | NaN | 19.96 | ,I | 24.41 | ,,,I | 15.51 | ,,,I | . 719 rows Ã— 43 columns . fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,7)) # plot precipitation ax1.fill_between(df.index, df[&#39;PRCP&#39;], 0, color=&#39;tab:blue&#39;) df_1990_1992 = df.loc[&#39;1990-07-01&#39;:&#39;1992-07-01&#39;] ax2.bar(df_1990_1992.index, df_1990_1992[&#39;PRCP&#39;], width=30) # adjust labels, ticks, title, etc ax1.set_title(&quot;Monthly precipitation in Tel Aviv&quot;) ax2.tick_params(axis=&#39;x&#39;, rotation=45) ax2.set_xlabel(&quot;date&quot;) # dirty trick to get common y label between the two panels: # make a large invisible axes, give it a ylabel ax0 = fig.add_subplot(111, frame_on=False) ax0.tick_params(labelcolor=&quot;none&quot;, bottom=False, left=False) ax0.set_ylabel(&quot;monthly precipitation (mm)&quot;, labelpad=20) # write yearly rainfall rain_1990_1991 = df.loc[&#39;1990-07-01&#39;:&#39;1991-07-01&#39;,&#39;PRCP&#39;].sum() rain_1991_1992 = df.loc[&#39;1991-07-01&#39;:&#39;1992-07-01&#39;,&#39;PRCP&#39;].sum() ax2.text(&#39;1991-01-01&#39;, 300, &quot;{:.0f} mm&quot;.format(rain_1990_1991)) ax2.text(&#39;1992-01-01&#39;, 300, &quot;{:.0f} mm&quot;.format(rain_1991_1992)) # save figure plt.savefig(&quot;monthly_tel_aviv_1940-1999.png&quot;) . hydrological year . A time period of 12 months for which precipitation totals are measured. The water year is designated by the calendar year in which it ends. Let&#39;s define the hydrological year for Tel Aviv from 1 October to 30 September. . # https://pandas.pydata.org/pandas-docs/version/0.12.0/timeseries.html#offset-aliases # also, annual resampling can be anchored to the end of specific months: # https://pandas.pydata.org/pandas-docs/version/0.12.0/timeseries.html#anchored-offsets df_year_all = df[&#39;PRCP&#39;].resample(&#39;A-SEP&#39;).sum().to_frame() # annual frequency, anchored end of September df_year_all.columns = [&#39;rain (mm)&#39;] # rename &#39;PRCP&#39; column to &#39;rain (mm)&#39; df_year_all . rain (mm) . DATE . 1940-09-30 474.9 | . 1941-09-30 447.8 | . 1942-09-30 372.9 | . 1943-09-30 618.2 | . 1944-09-30 440.5 | . ... ... | . 1996-09-30 488.2 | . 1997-09-30 619.1 | . 1998-09-30 489.6 | . 1999-09-30 226.5 | . 2000-09-30 0.0 | . 61 rows Ã— 1 columns . df_year = df_year_all.iloc[:-1] # exclude last row df_year.tail() # show &quot;tail&quot; of the dataframe to see that year 2000 was excluded . rain (mm) . DATE . 1995-09-30 804.7 | . 1996-09-30 488.2 | . 1997-09-30 619.1 | . 1998-09-30 489.6 | . 1999-09-30 226.5 | . fig, ax = plt.subplots(figsize=(10,7)) # plot YEARLY precipitation ax.bar(df_year.index, df_year[&#39;rain (mm)&#39;], width=365, align=&#39;edge&#39;, color=&quot;tab:blue&quot;) # plot mean rain_mean = df_year[&#39;rain (mm)&#39;].mean() ax.plot(df_year*0 + rain_mean, linewidth=3, color=&quot;tab:orange&quot;) # adjust labels, ticks, title, etc ax.set_title(&quot;Annual precipitation in Tel Aviv, 1940â€“1999&quot;) ax.set_xlabel(&quot;date&quot;) ax.set_ylabel(&quot;annual precipitation (mm)&quot;) ax.set_xlim([df_year.index[0], df_year.index[-1]]) # write mean on the right ax.text(df_year.index[-1], rain_mean, &quot; mean n {:.0f} mm&quot;.format(rain_mean), horizontalalignment=&quot;left&quot;, verticalalignment=&quot;center&quot;) # save figure plt.savefig(&quot;annual_tel_aviv_with_mean.png&quot;) . fig, ax = plt.subplots(figsize=(10,7)) # calculate mean and standard deviation rain_mean = df_year[&#39;rain (mm)&#39;].mean() rain_std = df_year[&#39;rain (mm)&#39;].std() # plot histogram b = np.arange(0, 1101, 100) # bins from 0 to 55, width = 5 ax.hist(df_year, bins=b) # plot vertical lines with mean, std, etc ylim = np.array(ax.get_ylim()) ylim[1] = ylim[1]*1.1 ax.plot([rain_mean]*2, ylim, linewidth=3, color=&quot;tab:orange&quot;) ax.plot([rain_mean+rain_std]*2, ylim, linewidth=3, linestyle=&quot;--&quot;, color=&quot;tab:olive&quot;) ax.plot([rain_mean-rain_std]*2, ylim, linewidth=3, linestyle=&quot;--&quot;, color=&quot;tab:olive&quot;) ax.set_ylim(ylim) # write mean, std, etc ax.text(rain_mean, ylim[1]*0.99, &quot;mean&quot;, horizontalalignment=&quot;center&quot;, verticalalignment=&quot;top&quot;, bbox=dict(facecolor=&#39;white&#39;, edgecolor=&#39;none&#39;, pad=0.0)) ax.text(rain_mean+rain_std, ylim[1]*0.99, &quot;mean+std&quot;, horizontalalignment=&quot;center&quot;, verticalalignment=&quot;top&quot;, bbox=dict(facecolor=&#39;white&#39;, edgecolor=&#39;none&#39;, pad=0.0)) ax.text(rain_mean-rain_std, ylim[1]*0.99, &quot;mean-std&quot;, horizontalalignment=&quot;center&quot;, verticalalignment=&quot;top&quot;, bbox=dict(facecolor=&#39;white&#39;, edgecolor=&#39;none&#39;, pad=0.0)) # adjust labels, ticks, title, limits, etc ax.set_title(&quot;Histogram of annual precipitation in Tel Aviv, 1940â€“1999&quot;) ax.set_xlabel(&quot;annual rainfall (mm)&quot;) ax.set_ylabel(&quot;number of years&quot;) # save figure plt.savefig(&quot;histogram_tel_aviv_with_mean_and_std.png&quot;) . coefficient of variation . $ langle{P} rangle=$ average precipitation $ sigma=$ standard deviation . $$CV = frac{ sigma}{ langle{P} rangle}$$ . Assuming that the inter-annual distribution is a gaussian: 67% of the time, rainfall will vary +/- 30% from its long term average in Tel Aviv. . CV = rain_std / rain_mean print(f&quot;CV = {CV:.2f}&quot;) # rain_mean . CV = 0.30 . fig, ax = plt.subplots(figsize=(10,7)) # windows of length 30 years windows = [[1940,1969], [1970,1999]] for window in windows: start_date = f&quot;{window[0]:d}-09-30&quot; end_date = f&quot;{window[1]:d}-09-30&quot; window_mean = df_year[&#39;rain (mm)&#39;][start_date:end_date].mean() ax.plot(df_year[start_date:end_date]*0+window_mean, color=&quot;purple&quot;, linewidth=3) ax.text(start_date, window_mean+0.5, f&quot;{window[0]} to {window[1]}: {window_mean:.0f} mm&quot;) # plot mean rain_mean = df_year[&#39;rain (mm)&#39;].mean() ax.plot(df_year*0 + rain_mean, linewidth=3, color=&quot;tab:orange&quot;, alpha=0.5) ax.text(df_year.index[-1], rain_mean, &quot; mean&quot;.format(rain_mean), horizontalalignment=&quot;left&quot;, verticalalignment=&quot;center&quot;) # adjust labels, ticks, title, limits, etc ax.set_title(&quot;Annual precipitation averages in Tel Aviv, 1940â€“1999&quot;) ax.set_xlabel(&quot;date&quot;) ax.set_ylabel(&quot;annual precipitation (mm)&quot;) ax.set_xlim([df_year.index[0], df_year.index[-1]]) ax.set_ylim([500, 560]) # save figure plt.savefig(&quot;mean_tel_aviv_2_windows.png&quot;) . fig, ax = plt.subplots(figsize=(10,7)) # windows of length 30 years windows = [[x,x+29] for x in [1940,1950,1960,1970]] for window in windows: start_date = f&quot;{window[0]:d}-09-30&quot; end_date = f&quot;{window[1]:d}-09-30&quot; window_mean = df_year[&#39;rain (mm)&#39;][start_date:end_date].mean() ax.plot(df_year[start_date:end_date]*0+window_mean, color=&quot;purple&quot;, linewidth=3) ax.text(start_date, window_mean+0.5, f&quot;{window[0]} to {window[1]}: {window_mean:.0f} mm&quot;) # plot mean ax.plot(df_year*0 + rain_mean, linewidth=3, color=&quot;tab:orange&quot;, alpha=0.5) ax.text(df_year.index[-1], rain_mean, &quot; mean&quot;.format(rain_mean), horizontalalignment=&quot;left&quot;, verticalalignment=&quot;center&quot;) # adjust labels, ticks, title, limits, etc ax.set_title(&quot;Annual precipitation averages in Tel Aviv&quot;) ax.set_xlabel(&quot;date&quot;) ax.set_ylabel(&quot;annual precipitation (mm)&quot;) ax.set_xlim([df_year.index[0], df_year.index[-1]]) ax.set_ylim([500, 560]) # save figure plt.savefig(&quot;mean_tel_aviv_4_windows.png&quot;) . import altair as alt from vega_datasets import data # Altair only recognizes column data; it ignores index values. You can plot the index data by first resetting the index source = df_year.reset_index() brush = alt.selection(type=&#39;interval&#39;, encodings=[&#39;x&#39;]) # T: temporal, a time or date value # Q: quantitative, a continuous real-valued quantity # https://altair-viz.github.io/user_guide/encoding.html#encoding-data-types bars = alt.Chart().mark_bar().encode( x=alt.X(&#39;DATE:T&#39;, axis=alt.Axis(title=&#39;date&#39;)), y=alt.Y(&#39;rain (mm):Q&#39;, axis=alt.Axis(title=&#39;annual precipitation (mm) and average&#39;)), opacity=alt.condition(brush, alt.OpacityValue(1), alt.OpacityValue(0.2)), ).add_selection( brush ).properties( title=&#39;Select year range and drag for rolling average of annual precipitation in Tel Aviv&#39; ).properties( width=600, height=400 ) line = alt.Chart().mark_rule(color=&#39;orange&#39;).encode( y=&#39;mean(rain (mm)):Q&#39;, size=alt.SizeValue(3) ).transform_filter( brush ) alt.layer(bars, line, data=source) . fig, ax = plt.subplots(figsize=(10,7)) rolling_mean = df_year.rolling(30, center=True).mean() ax.plot(rolling_mean, linewidth=3, color=&quot;tab:red&quot;, zorder=5) ax.set_title(&quot;30-year rolling average, Tel Aviv&quot;) ax.set_xlabel(&quot;date&quot;) ax.set_ylabel(&quot;annual precipitation (mm)&quot;) # windows of length 30 years windows = [[x,x+29] for x in [1940,1950,1960,1970]] for window in windows: start_date = f&quot;{window[0]:d}-09-30&quot; end_date = f&quot;{window[1]:d}-09-30&quot; window_mean = df_year[&#39;rain (mm)&#39;][start_date:end_date].mean() ax.plot(df_year[start_date:end_date]*0+window_mean, color=&quot;purple&quot;, linewidth=3, alpha=0.5) ax.text(start_date, window_mean+0.5, f&quot;{window[0]} to {window[1]}: {window_mean:.0f} mm&quot;, alpha=0.5) ax.set_ylim([480, 560]) # plot mean ax.plot(df_year*0 + rain_mean, linewidth=3, color=&quot;tab:orange&quot;, alpha=0.5) ax.text(df_year.index[-1], rain_mean, &quot; mean&quot;.format(rain_mean), horizontalalignment=&quot;left&quot;, verticalalignment=&quot;center&quot;) ax.set_xlim([df_year.index[0], df_year.index[-1]]) # save figure plt.savefig(&quot;rolling_average_tel_aviv.png&quot;) . rolling_mean . rain (mm) . DATE . 1940-09-30 NaN | . 1941-09-30 NaN | . 1942-09-30 NaN | . 1943-09-30 NaN | . 1944-09-30 NaN | . 1945-09-30 NaN | . 1946-09-30 NaN | . 1947-09-30 NaN | . 1948-09-30 NaN | . 1949-09-30 NaN | . 1950-09-30 NaN | . 1951-09-30 NaN | . 1952-09-30 NaN | . 1953-09-30 NaN | . 1954-09-30 NaN | . 1955-09-30 543.683333 | . 1956-09-30 541.690000 | . 1957-09-30 539.496667 | . 1958-09-30 547.746667 | . 1959-09-30 541.823333 | . 1960-09-30 550.390000 | . 1961-09-30 542.760000 | . 1962-09-30 536.510000 | . 1963-09-30 545.810000 | . 1964-09-30 545.803333 | . 1965-09-30 537.756667 | . 1966-09-30 530.206667 | . 1967-09-30 535.093333 | . 1968-09-30 527.663333 | . 1969-09-30 528.926667 | . 1970-09-30 516.710000 | . 1971-09-30 510.670000 | . 1972-09-30 495.516667 | . 1973-09-30 495.690000 | . 1974-09-30 502.350000 | . 1975-09-30 506.033333 | . 1976-09-30 517.593333 | . 1977-09-30 513.926667 | . 1978-09-30 527.450000 | . 1979-09-30 533.643333 | . 1980-09-30 524.583333 | . 1981-09-30 526.223333 | . 1982-09-30 531.463333 | . 1983-09-30 529.980000 | . 1984-09-30 532.553333 | . 1985-09-30 517.790000 | . 1986-09-30 NaN | . 1987-09-30 NaN | . 1988-09-30 NaN | . 1989-09-30 NaN | . 1990-09-30 NaN | . 1991-09-30 NaN | . 1992-09-30 NaN | . 1993-09-30 NaN | . 1994-09-30 NaN | . 1995-09-30 NaN | . 1996-09-30 NaN | . 1997-09-30 NaN | . 1998-09-30 NaN | . 1999-09-30 NaN | .",
            "url": "https://yairmau.github.io/website/jupyter/hydrology/2020/02/02/interannual-variability-of-precipitation-code.html",
            "relUrl": "/jupyter/hydrology/2020/02/02/interannual-variability-of-precipitation-code.html",
            "date": " â€¢ Feb 2, 2020"
        }
        
    
  
    
        ,"post52": {
            "title": "Assignment 2 - Evapotranspiration",
            "content": "&#128210; instructions . This is where learning happens, not during a lecture. You&#39;ll learn a ton of things by doing them yourself. Much success! ğŸ˜„ . Create a Jupyter Notebook called assignment-02-IDNUMBER, where IDNUMBER is your 9-digit ID. This is the file only file we will check. . &#128204; locations and data . Choose two stations with different climates. . Go to NOAA&#39;s Climate Reference Network Data website. The sub-hourly (5-min) data contains information on . air temperature, | precipitation, | global solar radiation, | surface infrared temperature, | relative humidity, | soil moisture and temperature, | wetness, and | 1.5 meter wind speed. There is no data on air pressure, so one needs to use the stations coordinates (lat, lon) to find its height above sea level, and from that infer the air pressure. You can use Google Earth or any other means to find the station&#39;s height. | . In the Data Access link, choose a year and a station you would like to analyze. If you are not sure where the stations are, find them using the 2-letter state abbreviation and the station name. . Download the following files: . One full year of data for each station. Make sure important data we need to calculate Penman&#39;s ET estimation is available. | The headers file | The documentation file | Make sure you understand what are the units provided for each measurement (see documentation). . &#128736; tasks . Produce potential ET estimates using Thornthwaite&#39;s equation and Penman&#39;s equation. Produce plots of ET as a function of time for each station, comparing the two methods you used. Also, using Penman&#39;s ET estimates, compare the two stations and discuss about their differences/similarities. . You might find interesting things in the data, such as periods of unusually high/low temperatures, radiation, etc. Discuss how these factors might have affected the ET estimates that you calculated. . You will have two weeks to deliver your assignment. You should not hand in a dry document with only figures and code, I&#39;m expecting text before and after each code/graph cell, explaining what you did, why you did it, and how it fits the story you are telling. Don&#39;t forget to put labels on your plot axes, title, legend, etc. . Your Jupyter Notebook should be fully functional: if we press Kernel &gt; Restart &amp; Run All, all the code must work without any errors. . &#127749; presentation . All the assignment must be in one single Jupyter Notebook. Use markdown cells to discuss the analysis and results, and in code cells show all the code you used to produce the figures and data analysis. Leave only the code necessary for your analysis, delete unnecessary lines your wrote while analyzing your data. Don&#39;t forget to comment your code, just like we did during exercise sessions. . You can write in English or in Hebrew, but the text in the figures must be in English. If you choose to write the discussion in Hebrew, be aware that Jupyter Notebooks don&#39;t have native right-to-left language support: . × ×™×ª×Ÿ ×œ×›×ª×•×‘ ×‘×¢×‘×¨×™×ª, ×œ××¨×•×ª ×©×–×” ×œ× × ×¨××” ×›×´×› ×˜×•×‘... . You can use some HTML code to achieve best results in Hebrew. Type the following . &lt;p dir=&quot;rtl&quot; style=&quot;text-align: right;&quot;&gt; ×¢×›×©×™×• ×”×¨×‘×” ×™×•×ª×¨ ×˜×•×‘! &lt;/p&gt; . to get . ×¢×›×©×™×• ×”×¨×‘×” ×™×•×ª×¨ ×˜×•×‘! . If you have many paragraphs in hebrew, do the following: . ×¤×¡×§×” ××¡×¤×¨ 1. . ×¤×¡×§×” ××¡×¤×¨ 2. . ×× ×™×© ×œ×›× ×›××” ×¤×¡×§××•×ª, ×›×œ ××—×ª ××”×Ÿ ×ª×”×™×” ×‘×ª×•×š &quot;dir&quot; ××©×œ×” . In my opinion it is too complicated to write in Hebrew in Jupyter Notebooks, just write in English, your grade will not be affected by typos nor less-than-perfect English proficiency. . &#128175; evaluation . Your assignment will be evaluated according to the following criteria: . 40% Presentation. How the graphs look, labels, general organization, markdown, clean code. | 30% Discussion. This is where you explain what you did, what you found out, etc. | 15% Depth of analysis. You can analyze/explore the data with different levels of complexity, this is where we take that into consideration. | 10% Replicability: Your code runs flawlessly. | 5%: Code commenting. Explain in your code what you are doing, this is good for everyone, especially for yourself! | . &#128666; importing the data . Below you can find an example of how to import the data file provided by NOAA&#39;s Climate Reference Network Data website. You might have to make some adjustments to it. . data_file = &quot;CRNS0101-05-2020-CO_Boulder_14_W.txt&quot; df = pd.read_csv(data_file, header=None, # no headers needed, we&#39;ll do that later delim_whitespace=True, # blank spaces separate between columns na_values=[&quot;-99.000&quot;, &quot;-9999.0&quot;] # substitute these values for missing (NaN) values ) headers = pd.read_csv(&quot;HEADERS_sub_hourly.txt&quot;, # load headers file header=1, # skip the first [0] line delim_whitespace=True ) df.columns = headers.columns # rename df columns with headers columns # LST = local standard time df[&quot;LST_TIME&quot;] = [f&quot;{x:04d}&quot; for x in df[&quot;LST_TIME&quot;]] # time needs padding of zeros, then convert to string df[&#39;LST_DATE&#39;] = df[&#39;LST_DATE&#39;].astype(str) # convert date into string df[&#39;datetime&#39;] = df[&#39;LST_DATE&#39;] + &#39; &#39; + df[&#39;LST_TIME&#39;] # combine date+time into datetime df[&#39;datetime&#39;] = pd.to_datetime(df[&#39;datetime&#39;]) # interpret datetime df = df.set_index(&#39;datetime&#39;) # make datetime the index df .",
            "url": "https://yairmau.github.io/website/jupyter/2020/02/02/assignment-02-ET.html",
            "relUrl": "/jupyter/2020/02/02/assignment-02-ET.html",
            "date": " â€¢ Feb 2, 2020"
        }
        
    
  
    
        ,"post53": {
            "title": "Introduction - lecture",
            "content": "Sources . Sources used: . (USGS, 2019) (USGS, 2019) (Margulis, 2017) . How much water is there? Where? . . Source: usgs.gov . Source: usgs.gov The water cycle . . Global water distribution . Water source Volume (km$^3$) % of freshwater % of total water . Oceans, Seas, &amp; Bays | 1,338,000,000 | â€“ | 96.54 | . Ice caps, Glaciers,&amp; Permanent Snow | 24,064,000 | 68.7 | 1.74 | . Groundwater | 23,400,000 | â€“ | 1.69 | . $ quad$Fresh | 10,530,000 | 30.1 | 0.76 | . $ quad$Saline | 12,870,000 | â€“ | 0.93 | . Soil Moisture | 16,500 | 0.05 | 0.001 | . Ground Ice&amp; Permafrost | 300,000 | 0.86 | 0.022 | . Lakes | 176,400 | â€“ | 0.013 | . $ quad$Fresh | 91,000 | 0.26 | 0.007 | . $ quad$Saline | 85,400 | â€“ | 0.006 | . Atmosphere | 12,900 | 0.04 | 0.001 | . Swamp Water | 11,470 | 0.03 | 0.0008 | . Rivers | 2,120 | 0.006 | 0.0002 | . Biological Water | 1,120 | 0.003 | 0.0001 | . * (Percents are rounded, so will not add to 100) https://www.usgs.gov/special-topic/water-science-school/science/fundamentals-water-cycle . Energy drives the hydrologic cycle . A key aspect of the hydrologic cycle is the fact that it is driven by energy inputs (primarily from the sun). At the global scale, the system is essentially closed with respect to water; negligible water is entering or leaving the system. In other words, there is no external forcing in terms of a water flux. Systems with no external forcing will generally eventually come to an equilibrium state. So what makes the hydrologic cycle so dynamic? The solar radiative energy input, which is external to the system, drives the hydrologic cycle. Averaged over the globe, 342 W m$^{-2}$ of solar radiative energy is being continuously input to the system at the top of the atmosphere. This energy input must be dissipated, and this is done, to a large extent, via the hydrologic cycle. Due to this fact, the study of hydrology is not isolated to the study of water storage and movement, but also must often include study of energy storage and movements. . Margulis, 2017, â€œIntroduction to Hydrologyâ€ . Components of the water cycle . Water storage in oceans . Evaporation / Sublimation . Evaporation $ longrightarrow$ cooling . . Source: hk-phy.org . Source: courses.lumenlearning.com . Source: engineeringinsider.org . Source: slideplayer.com . Source: slideplayer.com . Source: slideplayer.com Evapotranspiration . . Source: eschooltoday.com/water-cycle Water storage in the atmosphere . Cumulonimbus cloud over Africa . . Source: Wikimedia Picture of cumulonimbus taken from the International Space Station, over western Africa near the Senegal-Mali border. . If all of the water in the atmosphere rained down at once, it would only cover the globe to a depth of 2.5 centimeters. . amountÂ ofÂ waterÂ inÂ theÂ atmosphereV=12â€‰900â€‰km3surfaceÂ ofÂ EarthS=4Ï€R2;R=6371â€‰kmV=SÃ—hheighth=VSâ‰ƒ2.5â€‰cm begin{align} text{amount of water in the atmosphere} &amp; qquad V = 12 , 900 , text{km}^3 text{surface of Earth} &amp; qquad S = 4 pi R^2; quad R=6371 , text{km} &amp; qquad V = S times h text{height} &amp; qquad h = frac{V}{S} simeq 2.5 , text{cm} end{align}amountÂ ofÂ waterÂ inÂ theÂ atmospheresurfaceÂ ofÂ Earthheightâ€‹V=12900km3S=4Ï€R2;R=6371kmV=SÃ—hh=SVâ€‹â‰ƒ2.5cmâ€‹â€‹ . Try to calculate this yourself, and click on the button below to check how to do it. . Toggle Code â€ƒ . # amount of water in the atmosphere V = 12900 # km^3 # Earth&#39;s radius R = 6371 # km # surface of Earth = 4 pi RË†2 S = 4 * 3.141592 * R**2 # Volume: V = S * h, therefore # height h = V / S # in km h_cm = h * 1e5 # in cm print(f&quot;The height would be ~ {h_cm:.1f} cm&quot;) . Condensation . Precipitation . . Source: usgs.gov Â  Intensity (cm/h) Median diameter (mm) Velocity of fall (m/s) Drops s$^{-1}$ m$^{-2}$ . Fog | 0.013 | 0.01 | 0.003 | 67,425,000 | . Mist | 0.005 | 0.1 | 0.21 | 27,000 | . Drizzle | 0.025 | 0.96 | 4.1 | 151 | . Light rain | 0.10 | 1.24 | 4.8 | 280 | . Moderate rain | 0.38 | 1.60 | 5.7 | 495 | . Heavy rain | 1.52 | 2.05 | 6.7 | 495 | . Excessive rain | 4.06 | 2.40 | 7.3 | 818 | . Cloudburst | 10.2 | 2.85 | 7.9 | 1,220 | . Source: usgs.gov . Water storage in ice and snow . . Source: usgs.gov . Source: usgs.gov Snowmelt runoff to streams . Surface runoff . . Source: phys.org . Source: melabes.co.il Streamflow . The Mississippi river basin is very large . . Source: images.app.goo.gl The Amazon river basin is Huge . . Source: amazonwaters.org . Lakes and rivers . . Source: dreamstime.com Lake Malawi . . Source: images.app.goo.gl . Source: telegraph.co.uk Infiltration . . Source: sumagroulx.com Groundwater storage . . Source: usgs.gov . Source: modernfarmer.com . Source: Wikimedia Center Pivot irrigation in Nebraska taps the Ogallala Aquifer. . . Source: nebraskaeducationonlocation.org Groundwater flow and discharge . . Source: usgs.gov . Source: wellwater.oregonstate.edu . Source: researchgate.net Spring . Ein Gedi . . Source: haaretz.com . Source: usgs.gov References . USGS, 2019. How Much Water is There on Earth? | USGS, 2019. Water Science School. | Margulis, S., 2017. Introduction to Hydrology. eBook. |",
            "url": "https://yairmau.github.io/website/markdown/2020/02/01/introduction-lecture.html",
            "relUrl": "/markdown/2020/02/01/introduction-lecture.html",
            "date": " â€¢ Feb 1, 2020"
        }
        
    
  
    
        ,"post54": {
            "title": "Assignment - FINAL",
            "content": "ğŸ“’ Instructions . This is where learning happens, not during a lecture. Youâ€™ll learn a ton of things by doing them yourself. Much success! ğŸ˜„ . Create two Jupyter Notebooks called . assignment-FINAL-CODE-IDNUMBER, and | assignment-FINAL-REPORT-IDNUMBER, | where IDNUMBER is your 9-digit ID. These are the only files we will check. . ğŸ“Œ Locations and data . Choose one location in the US. . Download relevant data from NOAAâ€™s Global Summary of the Month, NOAAâ€™s Climate Reference Network Data, and from the USGSâ€™s National Water Information System. . Try to find locations with many years of data, the more the better. Take some time to choose your station, plan well. Choose a location you have not worked with in past assignments. . ğŸ›  Tasks . In this final project, we will integrate the various topics we learned throughout the semester. You will tell a story about the location you chose, and describe the changes it experienced in the past many decades. You can focus on any kind of changes that would influence the hydrological fluxes we learned about. Here are a few examples of changes that you might work on: . severe droughts in part of the studied period, or an increasing trend in drought severity. | same as above for rainfall/floods, high temperatures, low temperatures, etc. | significant changes in land use, such as urbanization, deforestation, agricultural practices, etc. | . The list above is not comprehensive, you can choose other factors. Consult with me in case of doubt. . Try to find on the media and in scientific papers evidence for the change you are focusing on. Cite these sources: at least one peer-reviewed scientific paper, and at least 3 other sources, such as a government website, official weather sites, books, reputable news websites, etc. . Can you see the same when analyzing data for the location you chose? Do your findings corroborate the expectation you had when you started this project? If they donâ€™t, can you explain why? Did you reach interesting or surprising conclusions in your analysis? . Analyze your locationâ€™s history with respect to the following: . Precipitation: seasonality, inter-annual variability, extreme precipitation events and return periods. | Potential evapotranspiration: Calculate PET using Penmanâ€™s equation for at least three different years of interest (not necessarily contiguous years). Calculate Thornthwaiteâ€™s PET for the whole length of the available data (comment about the suitability of Thornthwaiteâ€™s PET to the location you chose). | Analyze streamflow statistics in a similar manner as for precipitation: extreme discharge events and return periods. | Use Budykoâ€™s framework to calculate where the location you chose falls on the $(ET/P,PET/P)$ space for at least three different years of interest. | . Try to connect the dots: how do your different findings fit together? Discuss what you are trying to show, tell your story with the help of the data and your analyses. If you find things that go contrary to your expectations, can you raise hypotheses of why you see what you see? . You will have one month to hand in your project. Much success! ğŸ˜ . ğŸŒ… Presentation . All the assignment must be in one single Jupyter Notebook. Use markdown cells to discuss the analysis and results, and in code cells show all the code you used to produce the figures and data analysis. Leave only the code necessary for your analysis, delete unnecessary lines your wrote while analyzing your data. Donâ€™t forget to comment your code, just like we did during exercise sessions. . The assignment will be written in English. I am well aware that English is probably not most studentâ€™s mother tongue, therefore your grade will not be affected by typos nor less-than-perfect English proficiency. . ğŸ’¯ Evaluation . Your assignment will be evaluated according to the following criteria: . 40% Presentation. How the graphs look, labels, general organization, markdown, clean code. | 30% Discussion. This is where you explain what you did, what you found out, etc. | 15% Depth of analysis. You can analyze/explore the data with different levels of complexity, this is where we take that into consideration. | 10% Replicability: Your code runs flawlessly. | 5%: Code commenting. Explain in your code what you are doing, this is good for everyone, especially for yourself! | . ğŸšš Importing the data . You can use the code from previous assignments and from the exercise lectures. .",
            "url": "https://yairmau.github.io/website/markdown/2020/02/01/assignment_final.html",
            "relUrl": "/markdown/2020/02/01/assignment_final.html",
            "date": " â€¢ Feb 1, 2020"
        }
        
    
  
    
        ,"post55": {
            "title": "Assignment 3",
            "content": "ğŸ“’ Instructions . This is where learning happens, not during a lecture. Youâ€™ll learn a ton of things by doing them yourself. Much success! ğŸ˜„ . Create a Jupyter Notebook called assignment-03-IDNUMBER, where IDNUMBER is your 9-digit ID. This is the file only file we will check. . ğŸ“Œ Locations and data . Choose one location in the US. . Import streamflow data from USGSâ€™s National Water Information System. Choose on the map any measuring station you see fit. Make sure there is available discharge data (usually given in cubic feet per second) in small time intervals, e.g., every 15 minutes. Some of the sites provide precipitation data as well, then point 2 below is unnecessary. | On the map you can show â€œAtmospheric Sitesâ€ as well. You can download precipitation data from there. | . | Go to NOAAâ€™s Climate Reference Network Data website. The sub-hourly (5-min) data contains information on many variables, we are interested in precipitation. | Attention! Some os the USGS stations provide precipitation data. If you find one such station, step 2 above is unnecessary. If you only find discharge data in the USGS website, then make sure you choose two stations in very close proximity (USGS and NOAA). Because there are only a few high-resolution NOAA stations, you might want to start from there and then find discharge data for a stream near the NOAA station. . Bottom line: you are looking for precipitation and stream discharge data, for stations in close proximity, with a high temporal resolution (5 min, 15 min, etc). . ğŸ›  Tasks . Choose a rain event of a few hours in your data set. Find the rate of effective water input (p) and the event flow rate (q). Analyze the data in a similar was as done during class (various graphs explaining what you see). Find also the characteristic times of the event (centroid lag $T_{LC}$, and centroid lag-to-peak $T_{LPC}$). . Try to find information on the climate, geography, soil, and land use of the watershed. Begin the assignment by explaining about the watershed you chose and characterizing it. When presenting the data and your analyses, discuss what you see based on the concepts learned in class (infiltration, runoff generation, and the factors that affect them). Does the information you found match what you see? What makes sense, and what doesnâ€™t? . Discussion is important! . You will have two weeks to deliver your assignment. You should not hand in a dry document with only figures and code, Iâ€™m expecting text before and after each code/graph cell, explaining what you did, why you did it, and how it fits the story you are telling. Donâ€™t forget to put labels on your plot axes, title, legend, etc. . Your Jupyter Notebook should be fully functional: if we press Kernel &gt; Restart &amp; Run All, all the code must work without any errors. . ğŸŒ… Presentation . All the assignment must be in one single Jupyter Notebook. Use markdown cells to discuss the analysis and results, and in code cells show all the code you used to produce the figures and data analysis. Leave only the code necessary for your analysis, delete unnecessary lines your wrote while analyzing your data. Donâ€™t forget to comment your code, just like we did during exercise sessions. . The assignment will be written in English. I am well aware that English is probably not most studentâ€™s mother tongue, therefore your grade will not be affected by typos nor less-than-perfect English proficiency. . ğŸ’¯ Evaluation . Your assignment will be evaluated according to the following criteria: . 40% Presentation. How the graphs look, labels, general organization, markdown, clean code. | 30% Discussion. This is where you explain what you did, what you found out, etc. | 15% Depth of analysis. You can analyze/explore the data with different levels of complexity, this is where we take that into consideration. | 10% Replicability: Your code runs flawlessly. | 5%: Code commenting. Explain in your code what you are doing, this is good for everyone, especially for yourself! | . ğŸšš Importing the data . You can use the code from previous assignments and from the exercise lectures. .",
            "url": "https://yairmau.github.io/website/markdown/2020/02/01/assignment3.html",
            "relUrl": "/markdown/2020/02/01/assignment3.html",
            "date": " â€¢ Feb 1, 2020"
        }
        
    
  
    
        ,"post56": {
            "title": "Assignment 2",
            "content": "ğŸ“’ Instructions . This is where learning happens, not during a lecture. Youâ€™ll learn a ton of things by doing them yourself. Much success! ğŸ˜„ . Create a Jupyter Notebook called assignment-02-IDNUMBER, where IDNUMBER is your 9-digit ID. This is the file only file we will check. . ğŸ“Œ Locations and data . Choose two stations with different climates. . Go to NOAAâ€™s Climate Reference Network Data website. The sub-hourly (5-min) data contains information on . air temperature, | precipitation, | global solar radiation, | surface infrared temperature, | relative humidity, | soil moisture and temperature, | wetness, and | 1.5 meter wind speed. | . There is no data on air pressure, so one needs to use the stations coordinates (lat, lon) to find its height above sea level, and from that infer the air pressure. You can use Google Earth or any other means to find the stationâ€™s height. . In the Data Access link, choose a year and a station you would like to analyze. If you are not sure where the stations are, find them using the 2-letter state abbreviation and the station name. . Download the following files: . One full year of data for each station. Make sure important data we need to calculate Penmanâ€™s ET estimation is available. | The headers file | The documentation file | Make sure you understand what are the units provided for each measurement (see documentation). . ğŸ›  Tasks . Produce potential ET estimates using Thornthwaiteâ€™s equation and Penmanâ€™s equation. Produce plots of ET as a function of time for each station, comparing the two methods you used. Also, using Penmanâ€™s ET estimates, compare the two stations and discuss about their differences/similarities. . You might find interesting things in the data, such as periods of unusually high/low temperatures, radiation, etc. Discuss how these factors might have affected the ET estimates that you calculated. . You will have two weeks to deliver your assignment. You should not hand in a dry document with only figures and code, Iâ€™m expecting text before and after each code/graph cell, explaining what you did, why you did it, and how it fits the story you are telling. Donâ€™t forget to put labels on your plot axes, title, legend, etc. . Your Jupyter Notebook should be fully functional: if we press Kernel &gt; Restart &amp; Run All, all the code must work without any errors. . ğŸŒ… Presentation . All the assignment must be in one single Jupyter Notebook. Use markdown cells to discuss the analysis and results, and in code cells show all the code you used to produce the figures and data analysis. Leave only the code necessary for your analysis, delete unnecessary lines your wrote while analyzing your data. Donâ€™t forget to comment your code, just like we did during exercise sessions. . The assignment will be written in English. I am well aware that English is probably not most studentâ€™s mother tongue, therefore your grade will not be affected by typos nor less-than-perfect English proficiency. . ğŸ’¯ Evaluation . Your assignment will be evaluated according to the following criteria: . 40% Presentation. How the graphs look, labels, general organization, markdown, clean code. | 30% Discussion. This is where you explain what you did, what you found out, etc. | 15% Depth of analysis. You can analyze/explore the data with different levels of complexity, this is where we take that into consideration. | 10% Replicability: Your code runs flawlessly. | 5%: Code commenting. Explain in your code what you are doing, this is good for everyone, especially for yourself! | . ğŸšš Importing the data . Below you can find an example of how to import the data file provided by NOAAâ€™s Climate Reference Network Data website. You might have to make some adjustments to it. . data_file = &quot;CRNS0101-05-2020-CO_Boulder_14_W.txt&quot; df = pd.read_csv(data_file, header=None, # no headers needed, we&#39;ll do that later delim_whitespace=True, # blank spaces separate between columns na_values=[&quot;-99.000&quot;, &quot;-9999.0&quot;] # substitute these values for missing (NaN) values ) headers = pd.read_csv(&quot;HEADERS_sub_hourly.txt&quot;, # load headers file header=1, # skip the first [0] line delim_whitespace=True ) df.columns = headers.columns # rename df columns with headers columns # LST = local standard time df[&quot;LST_TIME&quot;] = [f&quot;{x:04d}&quot; for x in df[&quot;LST_TIME&quot;]] # time needs padding of zeros, then convert to string df[&#39;LST_DATE&#39;] = df[&#39;LST_DATE&#39;].astype(str) # convert date into string df[&#39;datetime&#39;] = df[&#39;LST_DATE&#39;] + &#39; &#39; + df[&#39;LST_TIME&#39;] # combine date+time into datetime df[&#39;datetime&#39;] = pd.to_datetime(df[&#39;datetime&#39;]) # interpret datetime df = df.set_index(&#39;datetime&#39;) # make datetime the index df .",
            "url": "https://yairmau.github.io/website/markdown/2020/02/01/assignment2.html",
            "relUrl": "/markdown/2020/02/01/assignment2.html",
            "date": " â€¢ Feb 1, 2020"
        }
        
    
  
    
        ,"post57": {
            "title": "Assignment 1",
            "content": "ğŸ“’ Instructions . This is where learning happens, not during a lecture. Youâ€™ll learn a ton of things by doing them yourself. Much success! ğŸ˜„ . Create a Jupyter Notebook called assignment-01-IDNUMBER, where IDNUMBER is your 9-digit ID. This is the file only file we will check. . ğŸ“Œ Locations . Choose two meteorologic stations from NOAAâ€™s Global Summary of the Month. Criteria: . at least 60 years of data for each station. | choose stations with different characteristics, regarding mean annual precipitation, seasonality, extreme events, etc. | ğŸ›  Tasks . Analyze the data and make graphs showing the differences and similarities between the two locations you chose. The analyses and graphs should be similar to those we saw during our lectures and exercise sessions. Of course, if you find something interesting we did not do in class, you are more than welcome to show it. Discuss about: . mean annual precipitation and inter-annual variability. | intra-annual variability (seasonality). | extreme rainfall events and return times. | You will have two weeks to deliver your assignment. You should not hand in a dry document with only figures and code, Iâ€™m expecting text before and after each code/graph cell, explaining what you did, why you did it, and how it fits the story you are telling. Donâ€™t forget to put labels on your plot axes, title, legend, etc. . Your Jupyter Notebook should be fully functional: if we press Kernel &gt; Restart &amp; Run All, all the code must work without any errors. . ğŸŒ… Presentation . All the assignment must be in one single Jupyter Notebook. Use markdown cells to discuss the analysis and results, and in code cells show all the code you used to produce the figures and data analysis. Leave only the code necessary for your analysis, delete unnecessary lines your wrote while analyzing your data. Donâ€™t forget to comment your code, just like we did during exercise sessions. . The assignment will be written in English. I am well aware that English is probably not most studentâ€™s mother tongue, therefore your grade will not be affected by typos nor less-than-perfect English proficiency. . ğŸ’¯ Evaluation . Your assignment will be evaluated according to the following criteria: . 40% Presentation. How the graphs look, labels, general organization, markdown, clean code. | 30% Discussion. This is where you explain what you did, what you found out, etc. | 15% Depth of analysis. You can analyze/explore the data with different levels of complexity, this is where we take that into consideration. | 10% Replicability: Your code runs flawlessly. | 5%: Code commenting. Explain in your code what you are doing, this is good for everyone, especially for yourself! | .",
            "url": "https://yairmau.github.io/website/markdown/2020/02/01/assignment1.html",
            "relUrl": "/markdown/2020/02/01/assignment1.html",
            "date": " â€¢ Feb 1, 2020"
        }
        
    
  
    
        ,"post58": {
            "title": "let's have fun plotting some data ğŸ˜€",
            "content": "download the data . Go to the Faculty of Agriculture&#39;s weather station. | Click on ××©×™×›×ª × ×ª×•× ×™× and download data for 1 September to 28 February, with a 24h interval. Call it data-sep2020-feb2021. You can download an example file here. | Open the .csv file with Excel, see how it looks like | import packages . We need to import this data into python. First we import useful packages. Type (don&#39;t copy and paste) the following lines in the code cell below: . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns.set(style=&quot;ticks&quot;, font_scale=1.5) # the following lets us use dates as xlim from pandas.plotting import register_matplotlib_converters register_matplotlib_converters() . import data with pandas . Import data from csv and put it in a pandas dataframe (a table). Make line 5 the header (column names) . df = pd.read_csv(&quot;data-sep2020-feb2021.csv&quot;, header=[4], encoding = &quot;ISO-8859-1&quot;) df . rename columns . rename the columns to: date, tmax, tmin, wind, rain24h, rain_cumulative . df.columns = [&#39;date&#39;, &#39;tmax&#39;, &#39;tmin&#39;, &#39;wind&#39;, &#39;rain24h&#39;, &#39;rain_cumulative&#39;] df . a first plot! . plot the minimum temperature: . plt.plot(df[&#39;tmin&#39;]) . how to deal with dates . We want the dates to appear on the horizontal axis. Interpret &#39;date&#39; column as a pandas datetime, see how it looks different from before before: 01/09/20 after: 2020-09-01 . df[&#39;date&#39;] = pd.to_datetime(df[&#39;date&#39;], dayfirst=True) df . date as dataframe index . Make &#39;date&#39; the dataframe&#39;s index . df = df.set_index(&#39;date&#39;) df . plot again, now with dates . Plot minimum temperature, now we have dates on the horizontal axis . plt.plot(df[&#39;tmin&#39;]) . we&#39;re getting there! the graph could look better . Let&#39;s make the graph look better: labels, title, slanted dates, etc . %matplotlib notebook # creates figure (the canvas) and the axis (rectangle where the plot sits) fig, ax = plt.subplots(1, figsize=(10,7)) # two line plots ax.plot(df[&#39;tmin&#39;], color=&quot;red&quot;, label=&quot;Temp (min)&quot;) ax.plot(df[&#39;tmax&#39;], color=&quot;blue&quot;, label=&quot;Temp (max)&quot;) # axes labels and figure title ax.set_xlabel(&#39;date&#39;) ax.set_ylabel(&#39;temperature (Â°C)&#39;) ax.set_title(&#39;maximum and minimum temperatures&#39;) # some ticks adjustments ax.set_yticks([10,15,20,25]) # we can choose where to put ticks ax.grid(axis=&#39;y&#39;) # makes horizontal lines plt.gcf().autofmt_xdate() # makes slated dates # legend ax.legend(loc=&#39;upper right&#39;) # save png figure plt.savefig(&quot;temp_max_min.png&quot;) . make the following figure . Use the following function to plot bars for daily rainfall . ax.bar(x_array, y_array) . Can you write yourself some lines of code that calculate the cumulative rainfall from the daily rainfall? . . double click this markdown cell to reveal the code I used to produce the figure. Don&#39;t do this right away, try to go as far as you can! | . make another figure . In order to choose just a part of the time series, you can use the following: . start_date = &#39;2021-01-01&#39; end_date = &#39;2021-01-31&#39; january = df[start_date:end_date] . . double click this markdown cell to reveal the code I used to produce the figure. Don&#39;t do this right away, try to go as far as you can! | . one last figure for today . Use the following code to create histograms with user-defined bins: . b = np.arange(0, 56, 5) # bins from 0 to 55, width = 5 ax.hist(df[&#39;wind&#39;], bins=b, density=True) . Play with the bins, see what happens. What does density=True do? . . double click this markdown cell to reveal the code I used to produce the figure. Don&#39;t do this right away, try to go as far as you can! | . homework . Go back to the weather station website, download one year of data from 01.01.2020 to 31.12.2020 (24h data). Make the following graph: . daily tmax and tmin | smoothed data for tmax and tmin | . In order to smooth the data with a 30 day window, use the following function: df[&#39;tmin&#39;].rolling(30, center=True).mean() This means that you will take the mean of 30 days, and put the result in the center of this 30-day window. . Play with this function, see what you can do with it. What happens when you change the size of the window? Why is the smoothed data shorter than the original data? See the documentation for rolling to find more options. . . double click this markdown cell to reveal the code I used to produce the figure. Don&#39;t do this right away, try to go as far as you can! | .",
            "url": "https://yairmau.github.io/website/jupyter/2020/02/01/python-intro.html",
            "relUrl": "/jupyter/2020/02/01/python-intro.html",
            "date": " â€¢ Feb 1, 2020"
        }
        
    
  
    
        ,"post59": {
            "title": "Hydrology --- introduction",
            "content": "How much water is there? Where? . . . The water cycle . Global water distribution . Water source Volume (km$^3$) % of freshwater % of total water . Oceans, Seas, &amp; Bays | 1,338,000,000 | -- | 96.54 | . Ice caps, Glaciers,&amp; Permanent Snow | 24,064,000 | 68.7 | 1.74 | . Groundwater | 23,400,000 | -- | 1.69 | . $ quad$Fresh | 10,530,000 | 30.1 | 0.76 | . $ quad$Saline | 12,870,000 | -- | 0.93 | . Soil Moisture | 16,500 | 0.05 | 0.001 | . Ground Ice&amp; Permafrost | 300,000 | 0.86 | 0.022 | . Lakes | 176,400 | -- | 0.013 | . $ quad$Fresh | 91,000 | 0.26 | 0.007 | . $ quad$Saline | 85,400 | -- | 0.006 | . Atmosphere | 12,900 | 0.04 | 0.001 | . Swamp Water | 11,470 | 0.03 | 0.0008 | . Rivers | 2,120 | 0.006 | 0.0002 | . Biological Water | 1,120 | 0.003 | 0.0001 | . * (Percents are rounded, so will not add to 100) https://www.usgs.gov/special-topic/water-science-school/science/fundamentals-water-cycle . Energy drives the hydrologic cycle . A key aspect of the hydrologic cycle is the fact that it is driven by energy inputs (primarily from the sun). At the global scale, the system is essentially closed with respect to water; negligible water is entering or leaving the system. In other words, there is no external forcing in terms of a water flux. Systems with no external forcing will generally eventually come to an equilibrium state. So what makes the hydrologic cycle so dynamic? The solar radiative energy input, which is external to the system, drives the hydrologic cycle. Averaged over the globe, 342 W m$^{-2}$ of solar radiative energy is being continuously input to the system at the top of the atmosphere. This energy input must be dissipated, and this is done, to a large extent, via the hydrologic cycle. Due to this fact, the study of hydrology is not isolated to the study of water storage and movement, but also must often include study of energy storage and movements. . Margulis, 2017, &quot;Introduction to Hydrology&quot; . Components of the water cycle . Water storage in oceans . Evaporation / Sublimation . Evaporation $ longrightarrow$ cooling . . . . . Evapotranspiration . Water storage in the atmosphere . Cumulonimbus cloud over Africa . Picture of cumulonimbus taken from the International Space Station, over western Africa near the Senegal-Mali border. . If all of the water in the atmosphere rained down at once, it would only cover the globe to a depth of 2.5 centimeters. $$ begin{align} text{amount of water in the atmosphere} &amp; qquad V = 12 , 900 , text{km}^3 text{surface of Earth} &amp; qquad S = 4 pi R^2; quad R=6371 , text{km} &amp; qquad V = S times h text{height} &amp; qquad h = frac{V}{S} simeq 2.5 , text{cm} end{align} $$ . Try to calculate this yourself, and click on the button below to check how to do it. . # amount of water in the atmosphere V = 12900 # km^3 # Earth&#39;s radius R = 6371 # km # surface of Earth = 4 pi RË†2 S = 4 * 3.141592 * R**2 # Volume: V = S * h, therefore # height h = V / S # in km h_cm = h * 1e5 # in cm print(f&quot;The height would be ~ {h_cm:.1f} cm&quot;) . . The height would be ~ 2.5 cm . Condensation . Precipitation . Intensity (cm/h) Median diameter (mm) Velocity of fall (m/s) Drops s$^{-1}$ m$^{-2}$ . Fog | 0.013 | 0.01 | 0.003 | 67,425,000 | . Mist | 0.005 | 0.1 | 0.21 | 27,000 | . Drizzle | 0.025 | 0.96 | 4.1 | 151 | . Light rain | 0.10 | 1.24 | 4.8 | 280 | . Moderate rain | 0.38 | 1.60 | 5.7 | 495 | . Heavy rain | 1.52 | 2.05 | 6.7 | 495 | . Excessive rain | 4.06 | 2.40 | 7.3 | 818 | . Cloudburst | 10.2 | 2.85 | 7.9 | 1,220 | . Source: https://www.usgs.gov/special-topic/water-science-school/science/precipitation-and-water-cycle . Water storage in ice and snow . . Snowmelt runoff to streams . Surface runoff . . Streamflow . The Mississippi river basin is very large . The Amazon river basin is Huge . . Lakes and rivers . Lake Malawi . . Infiltration . Groundwater storage . . . Center Pivot irrigation in Nebraska taps the Ogallala Aquifer. . Groundwater flow and discharge . . . Spring . Ein Gedi . Thousand Springs, Idaho .",
            "url": "https://yairmau.github.io/website/jupyter/2020/02/01/introduction.html",
            "relUrl": "/jupyter/2020/02/01/introduction.html",
            "date": " â€¢ Feb 1, 2020"
        }
        
    
  
    
        ,"post60": {
            "title": "Hydrology --- introduction",
            "content": "How much water is there? Where? . . . The water cycle . Global water distribution . Water source Volume (km$^3$) % of freshwater % of total water . Oceans, Seas, &amp; Bays | 1,338,000,000 | -- | 96.54 | . Ice caps, Glaciers,&amp; Permanent Snow | 24,064,000 | 68.7 | 1.74 | . Groundwater | 23,400,000 | -- | 1.69 | . $ quad$Fresh | 10,530,000 | 30.1 | 0.76 | . $ quad$Saline | 12,870,000 | -- | 0.93 | . Soil Moisture | 16,500 | 0.05 | 0.001 | . Ground Ice&amp; Permafrost | 300,000 | 0.86 | 0.022 | . Lakes | 176,400 | -- | 0.013 | . $ quad$Fresh | 91,000 | 0.26 | 0.007 | . $ quad$Saline | 85,400 | -- | 0.006 | . Atmosphere | 12,900 | 0.04 | 0.001 | . Swamp Water | 11,470 | 0.03 | 0.0008 | . Rivers | 2,120 | 0.006 | 0.0002 | . Biological Water | 1,120 | 0.003 | 0.0001 | . * (Percents are rounded, so will not add to 100) https://www.usgs.gov/special-topic/water-science-school/science/fundamentals-water-cycle . Energy drives the hydrologic cycle . A key aspect of the hydrologic cycle is the fact that it is driven by energy inputs (primarily from the sun). At the global scale, the system is essentially closed with respect to water; negligible water is entering or leaving the system. In other words, there is no external forcing in terms of a water flux. Systems with no external forcing will generally eventually come to an equilibrium state. So what makes the hydrologic cycle so dynamic? The solar radiative energy input, which is external to the system, drives the hydrologic cycle. Averaged over the globe, 342 W m$^{-2}$ of solar radiative energy is being continuously input to the system at the top of the atmosphere. This energy input must be dissipated, and this is done, to a large extent, via the hydrologic cycle. Due to this fact, the study of hydrology is not isolated to the study of water storage and movement, but also must often include study of energy storage and movements. . Margulis, 2017, &quot;Introduction to Hydrology&quot; . Components of the water cycle . Water storage in oceans . Evaporation / Sublimation . Evaporation $ longrightarrow$ cooling . . . . . Evapotranspiration . Water storage in the atmosphere . Cumulonimbus cloud over Africa . Picture of cumulonimbus taken from the International Space Station, over western Africa near the Senegal-Mali border. . If all of the water in the atmosphere rained down at once, it would only cover the globe to a depth of 2.5 centimeters. $$ begin{align} text{amount of water in the atmosphere} &amp; qquad V = 12 , 900 , text{km}^3 text{surface of Earth} &amp; qquad S = 4 pi R^2; quad R=6371 , text{km} &amp; qquad V = S times h text{height} &amp; qquad h = frac{V}{S} simeq 2.5 , text{cm} end{align} $$ . Try to calculate this yourself, and click on the button below to check how to do it. . # amount of water in the atmosphere V = 12900 # km^3 # Earth&#39;s radius R = 6371 # km # surface of Earth = 4 pi RË†2 S = 4 * 3.141592 * R**2 # Volume: V = S * h, therefore # height h = V / S # in km h_cm = h * 1e5 # in cm print(f&quot;The height would be ~ {h_cm:.1f} cm&quot;) . . The height would be ~ 2.5 cm . Condensation . Precipitation . Intensity (cm/h) Median diameter (mm) Velocity of fall (m/s) Drops s$^{-1}$ m$^{-2}$ . Fog | 0.013 | 0.01 | 0.003 | 67,425,000 | . Mist | 0.005 | 0.1 | 0.21 | 27,000 | . Drizzle | 0.025 | 0.96 | 4.1 | 151 | . Light rain | 0.10 | 1.24 | 4.8 | 280 | . Moderate rain | 0.38 | 1.60 | 5.7 | 495 | . Heavy rain | 1.52 | 2.05 | 6.7 | 495 | . Excessive rain | 4.06 | 2.40 | 7.3 | 818 | . Cloudburst | 10.2 | 2.85 | 7.9 | 1,220 | . Source: https://www.usgs.gov/special-topic/water-science-school/science/precipitation-and-water-cycle . Water storage in ice and snow . . Snowmelt runoff to streams . Surface runoff . . Streamflow . The Mississippi river basin is very large . The Amazon river basin is Huge . . Lakes and rivers . Lake Malawi . . Infiltration . Groundwater storage . . . Center Pivot irrigation in Nebraska taps the Ogallala Aquifer. . Groundwater flow and discharge . . . Spring . Ein Gedi . Thousand Springs, Idaho .",
            "url": "https://yairmau.github.io/website/jupyter/2020/02/01/introduction-lecture.html",
            "relUrl": "/jupyter/2020/02/01/introduction-lecture.html",
            "date": " â€¢ Feb 1, 2020"
        }
        
    
  
    
        ,"post61": {
            "title": "Introduction - some first exercises in python",
            "content": "let&#39;s have fun plotting some data &#128512; . download the data . Go to the Faculty of Agriculture&#39;s weather station. | Click on ××©×™×›×ª × ×ª×•× ×™× and download data for 1 September to 28 February, with a 24h interval. Call it data-sep2020-feb2021 | Open the .csv file with Excel, see how it looks like | If you can&#39;t download the data, just click here. | import packages . We need to import this data into python. First we import useful packages. Type (don&#39;t copy and paste) the following lines in the code cell below. . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns.set(style=&quot;ticks&quot;, font_scale=1.5) . . import data with pandas . Import data from csv and put it in a pandas dataframe (a table). Make line 5 the header (column names) . df = pd.read_csv(&quot;data-sep2020-feb2021.csv&quot;, header=[4]) df . . Unnamed: 0 ï¿½C ï¿½C.1 km/h mm mm.1 . 0 01/09/20 | 32.8 | 25.3 | 29.7 | 0.0 | 0.0 | . 1 02/09/20 | 33.0 | 24.0 | 28.8 | 0.0 | 0.0 | . 2 03/09/20 | 34.2 | 23.8 | 31.6 | 0.0 | 0.0 | . 3 04/09/20 | 36.3 | 27.3 | 24.2 | 0.0 | 0.0 | . 4 05/09/20 | 34.2 | 26.3 | 22.4 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | . 176 24/02/21 | 20.6 | 9.9 | 28.8 | 0.0 | 481.7 | . 177 25/02/21 | 19.4 | 9.3 | 23.3 | 0.0 | 481.7 | . 178 26/02/21 | 21.3 | 8.0 | 24.2 | 0.1 | 481.8 | . 179 27/02/21 | 23.4 | 9.2 | 30.6 | 0.0 | 481.8 | . 180 28/02/21 | 19.7 | 9.2 | 22.4 | 0.0 | 481.8 | . 181 rows Ã— 6 columns . rename columns . rename the columns to: date, tmax, tmin, wind, rain24h, rain_cumulative . df.columns = [&#39;date&#39;, &#39;tmax&#39;, &#39;tmin&#39;, &#39;wind&#39;, &#39;rain24h&#39;, &#39;rain_cumulative&#39;] df . . date tmax tmin wind rain24h rain_cumulative . 0 01/09/20 | 32.8 | 25.3 | 29.7 | 0.0 | 0.0 | . 1 02/09/20 | 33.0 | 24.0 | 28.8 | 0.0 | 0.0 | . 2 03/09/20 | 34.2 | 23.8 | 31.6 | 0.0 | 0.0 | . 3 04/09/20 | 36.3 | 27.3 | 24.2 | 0.0 | 0.0 | . 4 05/09/20 | 34.2 | 26.3 | 22.4 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | . 176 24/02/21 | 20.6 | 9.9 | 28.8 | 0.0 | 481.7 | . 177 25/02/21 | 19.4 | 9.3 | 23.3 | 0.0 | 481.7 | . 178 26/02/21 | 21.3 | 8.0 | 24.2 | 0.1 | 481.8 | . 179 27/02/21 | 23.4 | 9.2 | 30.6 | 0.0 | 481.8 | . 180 28/02/21 | 19.7 | 9.2 | 22.4 | 0.0 | 481.8 | . 181 rows Ã— 6 columns . a first plot! . plot the minimum temperature: . plt.plot(df[&#39;tmin&#39;]) . . [&lt;matplotlib.lines.Line2D at 0x7fd02954bbd0&gt;] . how to deal with dates . We want the dates to appear on the horizontal axis. Interpret &#39;date&#39; column as a pandas datetime, see how it looks different from before before: 01/09/20 after: 2020-09-01 . df[&#39;date&#39;] = pd.to_datetime(df[&#39;date&#39;], dayfirst=True) df . . date tmax tmin wind rain24h rain_cumulative . 0 2020-09-01 | 32.8 | 25.3 | 29.7 | 0.0 | 0.0 | . 1 2020-09-02 | 33.0 | 24.0 | 28.8 | 0.0 | 0.0 | . 2 2020-09-03 | 34.2 | 23.8 | 31.6 | 0.0 | 0.0 | . 3 2020-09-04 | 36.3 | 27.3 | 24.2 | 0.0 | 0.0 | . 4 2020-09-05 | 34.2 | 26.3 | 22.4 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | . 176 2021-02-24 | 20.6 | 9.9 | 28.8 | 0.0 | 481.7 | . 177 2021-02-25 | 19.4 | 9.3 | 23.3 | 0.0 | 481.7 | . 178 2021-02-26 | 21.3 | 8.0 | 24.2 | 0.1 | 481.8 | . 179 2021-02-27 | 23.4 | 9.2 | 30.6 | 0.0 | 481.8 | . 180 2021-02-28 | 19.7 | 9.2 | 22.4 | 0.0 | 481.8 | . 181 rows Ã— 6 columns . date as dataframe index . Make &#39;date&#39; the dataframe&#39;s index (leftmost column, but not really a column!) . df = df.set_index(&#39;date&#39;) df . . tmax tmin wind rain24h rain_cumulative . date . 2020-09-01 32.8 | 25.3 | 29.7 | 0.0 | 0.0 | . 2020-09-02 33.0 | 24.0 | 28.8 | 0.0 | 0.0 | . 2020-09-03 34.2 | 23.8 | 31.6 | 0.0 | 0.0 | . 2020-09-04 36.3 | 27.3 | 24.2 | 0.0 | 0.0 | . 2020-09-05 34.2 | 26.3 | 22.4 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | . 2021-02-24 20.6 | 9.9 | 28.8 | 0.0 | 481.7 | . 2021-02-25 19.4 | 9.3 | 23.3 | 0.0 | 481.7 | . 2021-02-26 21.3 | 8.0 | 24.2 | 0.1 | 481.8 | . 2021-02-27 23.4 | 9.2 | 30.6 | 0.0 | 481.8 | . 2021-02-28 19.7 | 9.2 | 22.4 | 0.0 | 481.8 | . 181 rows Ã— 5 columns . plot again, now with dates . Plot minimum temperature, now we have dates on the horizontal axis . plt.plot(df[&#39;tmin&#39;]) . . [&lt;matplotlib.lines.Line2D at 0x7fd0795aa190&gt;] . we&#39;re getting there! the graph could look better . Let&#39;s make the graph look better: labels, title, slanted dates, etc . # creates figure (the canvas) and the axis (rectangle where the plot sits) fig, ax = plt.subplots(1, figsize=(10,7)) # two line plots ax.plot(df[&#39;tmin&#39;], color=&quot;red&quot;, label=&quot;Temp (min)&quot;) ax.plot(df[&#39;tmax&#39;], color=&quot;blue&quot;, label=&quot;Temp (max)&quot;) # axes labels and figure title ax.set_xlabel(&#39;date&#39;) ax.set_ylabel(&#39;temperature (Â°C)&#39;) ax.set_title(&#39;maximum and minimum temperatures&#39;) # some ticks adjustments ax.set_yticks([10,15,20,25]) # we can choose where to put ticks ax.grid(axis=&#39;y&#39;) # makes horizontal lines plt.gcf().autofmt_xdate() # makes slated dates # legend ax.legend(loc=&#39;upper right&#39;) # save png figure plt.savefig(&quot;temp_max_min.png&quot;) . . make the following figure . Use the following function to plot bars for daily rainfall . ax.bar(x_array, y_array) . Can you write yourself some lines of code that calculate the cumulative rainfall from the daily rainfall? . # creates figure (the canvas) and the axis (rectangle where the plot sits) fig, ax = plt.subplots(1, figsize=(10,7)) # line and bar plots ax.bar(df.index, df[&#39;rain24h&#39;], color=&quot;blue&quot;, label=&quot;daily rainfall&quot;) # there are many ways of calculating the cumulative rain # method 1, use a for loop: # rain = df[&#39;rain24h&#39;].to_numpy() # cumulative = rain * 0 # for i in range(len(rain)): # cumulative[i] = np.sum(rain[:i]) # df[&#39;cumulative1&#39;] = cumulative # method 2, use list comprehension: # rain = df[&#39;rain24h&#39;].to_numpy() # cumulative = [np.sum(rain[:i]) for i in range(len(rain))] # df[&#39;cumulative2&#39;] = cumulative # method 3, use existing functions: df[&#39;cumulative3&#39;] = np.cumsum(df[&#39;rain24h&#39;]) ax.plot(df[&#39;cumulative3&#39;], color=&quot;red&quot;, label=&quot;cumulative rainfall&quot;) # compare our cumulative rainfall with the downloaded data # ax.plot(df[&#39;rain_cumulative&#39;], &#39;x&#39;) # axes labels and figure title ax.set_xlabel(&#39;date&#39;) ax.set_ylabel(&#39;rainfall (mm)&#39;) ax.set_title(&#39;daily and cumulative rainfall&#39;) ax.set_xlim([&#39;2020-11-01&#39;,&#39;2021-02-28&#39;]) # some ticks adjustments plt.gcf().autofmt_xdate() # makes slated dates # legend ax.legend(loc=&#39;upper left&#39;) # save png figure plt.savefig(&quot;cumulative_rainfall.png&quot;) . . make another figure . In order to choose just a part of the time series, you can use the following: . start_date = &#39;2021-01-01&#39; end_date = &#39;2021-01-31&#39; january = df[start_date:end_date] . # creates figure (the canvas) and the axis (rectangle where the plot sits) fig, ax = plt.subplots(1, figsize=(10,7)) # define date range start_date = &#39;2021-01-01&#39; end_date = &#39;2021-01-31&#39; january = df[start_date:end_date][&#39;tmax&#39;] # plots ax.plot(january, color=&quot;red&quot;, label=&quot;daily max&quot;) ax.plot(january*0 + january.mean(), color=&quot;purple&quot;, linestyle=&quot;--&quot;, label=&quot;average daily max&quot;) # axes labels and figure title ax.set_xlabel(&#39;date&#39;) ax.set_ylabel(&#39;temperature (Â°C)&#39;) ax.set_title(&#39;average daily maximum temperature for January 2021&#39;) # some ticks adjustments plt.gcf().autofmt_xdate() # makes slated dates # legend ax.legend(loc=&#39;lower left&#39;) # save png figure plt.savefig(&quot;average_max_temp.png&quot;) . . one last figure for today . Use the following code to create histograms with user-defined bins: . b = np.arange(0, 56, 5) # bins from 0 to 55, width = 5 ax.hist(df[&#39;wind&#39;], bins=b, density=True) . Play with the bins, see what happens. What does density=True do? . # creates figure (the canvas) and the axis (rectangle where the plot sits) fig, ax = plt.subplots(1, figsize=(10,7)) # histogram b = np.arange(0, 56, 5) # bins from 0 to 55, width = 5 ax.hist(df[&#39;wind&#39;], bins=b, density=True) # axes labels and figure title ax.set_xlabel(&#39;max wind speed (km/h)&#39;) ax.set_ylabel(&#39;frequency&#39;) ax.set_title(&#39;frequency of maximum wind speed&#39;) # save png figure plt.savefig(&quot;wind-histogram.png&quot;) . . homework . Go back to the weather station website, download one year of data from 01.01.2020 to 31.12.2020 (24h data). If you can&#39;t download the data, just click here. Make the following graph: . daily tmax and tmin | smoothed data for tmax and tmin | . In order to smooth the data with a 30 day window, use the following function: df[&#39;tmin&#39;].rolling(30, center=True).mean() This means that you will take the mean of 30 days, and put the result in the center of this 30-day window. . Play with this function, see what you can do with it. What happens when you change the size of the window? Why is the smoothed data shorter than the original data? See the documentation for rolling to find more options. . fig, ax = plt.subplots(figsize=(10,7)) df2 = pd.read_csv(&quot;1year.csv&quot;, header=[4]) df2[&#39;date&#39;] = pd.to_datetime(df2[&#39;date&#39;], dayfirst=True) df2 = df2.set_index(&#39;date&#39;) plt.plot(df2[&#39;tmax&#39;], label=&#39;tmax&#39;, color=&quot;tab:red&quot;) plt.plot(df2[&#39;tmin&#39;], label=&#39;tmin&#39;, color=&quot;tab:blue&quot;) tmin_smooth = df2[&#39;tmin&#39;].rolling(30, center=True).mean() tmax_smooth = df2[&#39;tmax&#39;].rolling(30, center=True).mean() plt.plot(tmax_smooth, label=&#39;tmax smoothed&#39;, color=&quot;tab:pink&quot;, linestyle=&quot;--&quot;, linewidth=3) plt.plot(tmin_smooth, label=&#39;tmin smoothed&#39;, color=&quot;tab:cyan&quot;, linestyle=&quot;--&quot;, linewidth=3) plt.legend() plt.savefig(&quot;t_smoothed.png&quot;) . .",
            "url": "https://yairmau.github.io/website/jupyter/2020/02/01/introduction-exercises.html",
            "relUrl": "/jupyter/2020/02/01/introduction-exercises.html",
            "date": " â€¢ Feb 1, 2020"
        }
        
    
  
    
        ,"post62": {
            "title": "Final Assignment",
            "content": "&#128210; instructions . This is where learning happens, not during a lecture. You&#39;ll learn a ton of things by doing them yourself. Much success! ğŸ˜„ . Create two Jupyter Notebooks called . assignment-FINAL-CODE-IDNUMBER, and | assignment-FINAL-REPORT-IDNUMBER, where IDNUMBER is your 9-digit ID. These are the only files we will check. | &#128204; locations and data . Choose one location in the US. . Download relevant data from NOAA&#39;s Global Summary of the Month, NOAA&#39;s Climate Reference Network Data, and from the USGS&#39;s National Water Information System. . Try to find locations with many years of data, the more the better. Take some time to choose your station, plan well. Choose a location you have not worked with in past assignments. . &#128736; tasks . In this final project, we will integrate the various topics we learned throughout the semester. You will tell a story about the location you chose, and describe the changes it experienced in the past many decades. You can focus on any kind of changes that would influence the hydrological fluxes we learned about. Here are a few examples of changes that you might work on: . severe droughts in part of the studied period, or an increasing trend in drought severity. | same as above for rainfall/floods, high temperatures, low temperatures, etc. | significant changes in land use, such as urbanization, deforestation, agricultural practices, etc. | . The list above is not comprehensive, you can choose other factors. Consult with me in case of doubt. . Try to find on the media and in scientific papers evidence for the change you are focusing on. Cite these sources: at least one peer-reviewed scientific paper, and at least 3 other sources, such as a government website, official weather sites, books, reputable news websites, etc. . Can you see the same when analyzing data for the location you chose? Do your findings corroborate the expectation you had when you started this project? If they don&#39;t, can you explain why? Did you reach interesting or surprising conclusions in your analysis? . Analyze your location&#39;s history with respect to the following: . Precipitation: seasonality, inter-annual variability, extreme precipitation events and return periods. | Potential evapotranspiration: Calculate PET using Penman&#39;s equation for at least three different years of interest (not necessarily contiguous years). Calculate Thornthwaite&#39;s PET for the whole length of the available data (comment about the suitability of Thornthwaite&#39;s PET to the location you chose). | Analyze streamflow statistics in a similar manner as for precipitation: extreme discharge events and return periods. | Use Budyko&#39;s framework to calculate where the location you chose falls on the $(ET/P,PET/P)$ space for at least three different years of interest. | . Try to connect the dots: how do your different findings fit together? Discuss what you are trying to show, tell your story with the help of the data and your analyses. If you find things that go contrary to your expectations, can you raise hypotheses of why you see what you see? . You will have one month to hand in your project. Much success! ğŸ˜ . &#127749; presentation . All the assignment must be in two Jupyter Notebooks. . The notebook called CODE will contain all the code for the analyses you made. It must be fully functional, i.e., we must be able to Run All and not get any errors. Explain what you are doing in each step. Comment your code. Use markdown cells to split the notebook into subsections, one for each analysis (e.g.: ## Precipitation Analysis, ### Inter-annual variability, etc). . The notebook called Report will contain graphs and relevant data from the CODE notebook. It is here where you will introduce the location you chose, what you are trying to see. Here you will write all the results and discussion, as supported by the graphs and results you produced. Divide this notebook into sections: Introduction, Results and Discussion, Conclusion. Subdivide the sections into subsections when needed. In this file there should be no code at all. . You can write in English or in Hebrew, but the text in the figures must be in English. If you choose to write the discussion in Hebrew, be aware that Jupyter Notebooks don&#39;t have native right-to-left language support: . × ×™×ª×Ÿ ×œ×›×ª×•×‘ ×‘×¢×‘×¨×™×ª, ×œ××¨×•×ª ×©×–×” ×œ× × ×¨××” ×›×´×› ×˜×•×‘... . You can use some HTML code to achieve best results in Hebrew. Type the following . &lt;p dir=&quot;rtl&quot; style=&quot;text-align: right;&quot;&gt; ×¢×›×©×™×• ×”×¨×‘×” ×™×•×ª×¨ ×˜×•×‘! &lt;/p&gt; . to get . ×¢×›×©×™×• ×”×¨×‘×” ×™×•×ª×¨ ×˜×•×‘! . If you have many paragraphs in hebrew, do the following: . ×¤×¡×§×” ××¡×¤×¨ 1. . ×¤×¡×§×” ××¡×¤×¨ 2. . ×× ×™×© ×œ×›× ×›××” ×¤×¡×§××•×ª, ×›×œ ××—×ª ××”×Ÿ ×ª×”×™×” ×‘×ª×•×š &quot;dir&quot; ××©×œ×” . In my opinion it is too complicated to write in Hebrew in Jupyter Notebooks, just write in English, your grade will not be affected by typos nor less-than-perfect English proficiency. . &#128175; evaluation . Your assignment will be evaluated according to the following criteria: . 40% Presentation. How the graphs look, labels, general organization, markdown, clean code. | 30% Discussion. This is where you explain what you did, what you found out, etc. | 15% Depth of analysis. You can analyze/explore the data with different levels of complexity, this is where we take that into consideration. | 10% Replicability: Your code runs flawlessly. | 5%: Code commenting. Explain in your code what you are doing, this is good for everyone, especially for yourself! | . &#128666; importing the data . You can use the code from previous assignments and from the exercise lectures. .",
            "url": "https://yairmau.github.io/website/jupyter/2020/02/01/assignment-FINAL.html",
            "relUrl": "/jupyter/2020/02/01/assignment-FINAL.html",
            "date": " â€¢ Feb 1, 2020"
        }
        
    
  
    
        ,"post63": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a â€œlevel 1 headingâ€ in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Hereâ€™s a footnote 1. Hereâ€™s a horizontal rule: . . Lists . Hereâ€™s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes â€¦andâ€¦ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.Â &#8617; . |",
            "url": "https://yairmau.github.io/website/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " â€¢ Jan 14, 2020"
        }
        
    
  
    
        ,"post64": {
            "title": "This website's logo",
            "content": ". Introduction . This will only work if you have Mathematica installed in your machine. Python can interface with Wolfram Mathematica, taking advantage of its awesome power. . The curve inside the square is a parabola: $$ y = ax^2 + bx + c $$ . This parabola passes through the points $(0.3, 0)$ and $(1, 1)$, and it&#39;s derivative at $(0.3, 0)$ is zero. We use Mathematica to figure out what are the parameters $a,b,c$. Finally, we transpose the line when plotting, i.e., $x$ is in the vertical axis, and $y$ is in the horizontal axis. . The code . %matplotlib inline import matplotlib.pyplot as plt import numpy as np from wolframclient.language import wl from wolframclient.evaluation import WolframLanguageSession from wolframclient.language import wl, wlexpr w = 20 plt.rc(&#39;axes&#39;, linewidth=w) fig=plt.figure(1, (5, 5)) fig.subplots_adjust(left=0.0, right=1.0, top=1.0, bottom=0.0, hspace=0, wspace=0) ax = plt.Axes(fig, [0., 0., 1., 1.]) fig.add_axes(ax) session = WolframLanguageSession() session.evaluate(wlexpr(&#39;y[x_] := a x^2 + b x + c&#39;)) session.evaluate(wlexpr(&#39;p1 = {0.3, 0}&#39;)) session.evaluate(wlexpr(&#39;p2 = {1, 1}&#39;)) session.evaluate(wlexpr(&#39;sol1 = Solve[ {y[p1[[1]]] == p1[[2]], y[p2[[1]]] == p2[[2]]}, {a, b, c}][[1]]&#39;)) session.evaluate(wlexpr(&#39;sol2 = Solve[(D[(y[x] /. sol1), x] /. x -&gt; 0.3) == 0, a][[1]]&#39;)) par = list(session.evaluate(wlexpr(&#39;{a, b, c} /. sol1 /. sol2&#39;))) x0 = 0.3 x=np.linspace(x0, 1, 1001) a, b, c = par# [2.08163, -1.24898, 0.167347] y = lambda x: a*x**2 + b*x + c c1 = &#39;white&#39; # bottom right c2 = &#39;white&#39; # top left # c1 = &#39;#6c7053&#39; # bottom right # c2 = &#39;#6e0014&#39; # top left # ax.fill_between(y(x), x, y2=0, facecolor=c1, # edgecolor=&#39;black&#39;) # bottom right # ax.fill_between(y(x), x, y2=1, facecolor=c2, # edgecolor=&quot;black&quot;, linewidth=10) # top left ax.plot(y(x), x, color=&quot;black&quot;, lw=w) ax.set_xlim([0,1]) ax.set_ylim([0,1]) ax.set_xticks([]) ax.set_yticks([]) fig.savefig(&quot;./python_figures/site-logo.png&quot;, resolution=600, transparent=True, bbox_inches=&quot;tight&quot;) fig.savefig(&quot;./python_figures/site-logo.svg&quot;) plt.show() . Equations may not give solutions for all &#34;solve&#34; variables. Equations may not give solutions for all &#34;solve&#34; variables. . session = WolframLanguageSession() session.evaluate(wlexpr(&#39;y[x_] := a x^2 + b x + c&#39;)) session.evaluate(wlexpr(&#39;p1 = {0.3, -0.02}&#39;)) session.evaluate(wlexpr(&#39;p2 = {1, 1}&#39;)) session.evaluate(wlexpr(&#39;sol1 = Solve[ {y[p1[[1]]] == p1[[2]], y[p2[[1]]] == p2[[2]]}, {a, b, c}][[1]]&#39;)) session.evaluate(wlexpr(&#39;sol2 = Solve[(D[(y[x] /. sol1), x] /. x -&gt; 0.3) == 0, a][[1]]&#39;)) par = list(session.evaluate(wlexpr(&#39;{a, b, c} /. sol1 /. sol2&#39;))) . Equations may not give solutions for all &#34;solve&#34; variables. Equations may not give solutions for all &#34;solve&#34; variables. . par . [2.0816326530612246, -1.2489795918367346, 0.16734693877551027] .",
            "url": "https://yairmau.github.io/website/jupyter/2020/01/01/websitelogo.html",
            "relUrl": "/jupyter/2020/01/01/websitelogo.html",
            "date": " â€¢ Jan 1, 2020"
        }
        
    
  
    
        ,"post65": {
            "title": "The time-dependent Ginzburg-Landau equation",
            "content": ". Introduction . Simulation of the Time-Dependent Ginzburg-Landau Equation $$ frac{ text{d}u}{ text{d}t}= u - u^3 +D nabla^2 u,$$ in 1 and 2 spatial dimensions. This is the simplest example of numerical integration through Finite Differences: . Euler method to advance time | Five-point stencil to compute the laplacian, periodic boundary conditions are assumed. See an example of the output here: https://www.youtube.com/watch?v=JgE9Px7zsQE | . Code . import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.axes_grid1 import make_axes_locatable import time as tm . n = 128 # for 1d simulation write N=(n,) N=(n,n) # diffusion coefficient D = 1.0 # spatial dimensions L = 100.0 dx = L / n x = np.arange(0,L,dx) # time t = 0.0 total_time = 3.0 # beware of the Von Neumann stability analysis # https://en.wikipedia.org/wiki/Von_Neumann_stability_analysis dt = 0.2 * 0.5 * dx**2 / D . define functions . def periodic_lap_1d(u,dx=1.0): return (+1*np.roll(u,+1) +1*np.roll(u,-1) -2*u) / dx**2 def periodic_lap_2d(u,dx=1.0): return (+1*np.roll(u,+1,axis=0) +1*np.roll(u,-1,axis=0) +1*np.roll(u,+1,axis=1) +1*np.roll(u,-1,axis=1) -4*u) / dx**2 f = lambda u: u - u**3 . initialize and start plotting . plt.ion() fig = plt.figure(1,figsize=(7,6)) plt.clf() ax = fig.add_subplot(111) # random initial condition u = 2*np.random.random(N)-1.0 if len(N) == 1: lap = periodic_lap_1d p, = ax.plot(x,u) ax.axis([x[0],x[-1],-1.1,1.1]) if len(N) == 2: lap = periodic_lap_2d p = ax.imshow(u,cmap=&quot;RdGy&quot;, vmin=-1.0, vmax=1.0,extent=[0,L,0,L]) # create an axes on the right side of ax. The width of cax will be 5% # of ax and the padding between cax and ax will be fixed at 0.15 inch. divider = make_axes_locatable(ax) colorbar_ax = divider.append_axes(&quot;right&quot;, size=&quot;5%&quot;, pad=0.15) cbar = fig.colorbar(p, cax=colorbar_ax, ticks=[-1,-0.5,0,0.5,1]) ax.set_title(&quot;time={:5.1f}&quot;.format(0.0)) . Text(0.5, 1.0, &#39;time= 0.0&#39;) . start simulation . while t&lt;total_time: t += dt u = u + dt * (f(u) + D * lap(u,dx) ) # we don&#39;t need to plot again, just to update the data of the plot if len(N) == 1: p.set_data(x,u) if len(N) == 2: p.set_data(u) ax.set_title(&quot;time={:5.1f}&quot;.format(t)) fig.canvas.draw() .",
            "url": "https://yairmau.github.io/website/jupyter/2020/01/01/tdgle.html",
            "relUrl": "/jupyter/2020/01/01/tdgle.html",
            "date": " â€¢ Jan 1, 2020"
        }
        
    
  
    
        ,"post66": {
            "title": "Fancy subplot grid",
            "content": ". Introduction . With GridSpec you can create any combination of panels . The code . import numpy as np import matplotlib import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec from matplotlib.ticker import FuncFormatter . # this is a latex constant, don&#39;t change it. pts_per_inch = 72.27 # write &quot; the textwidth&quot; (or &quot; showthe columnwidth&quot; for a 2 collumn text) text_width_in_pts = 246.0 # inside a figure environment in latex, the result will be on the # dvi/pdf next to the figure. See url above. text_width_in_inches = text_width_in_pts / pts_per_inch # make rectangles with a nice proportion golden_ratio = 0.618 # figure.png or figure.eps will be intentionally larger, because it is prettier inverse_latex_scale = 2 # when compiling latex code, use # includegraphics[scale=(1/inverse_latex_scale)]{figure} # we want the figure to occupy 2/3 (for example) of the text width fig_proportion = (3.0 / 3.0) csize = inverse_latex_scale * fig_proportion * text_width_in_inches # always 1.0 on the first argument fig_size = (1.0 * csize, 1.1 * csize) # find out the fontsize of your latex text, and put it here text_size = inverse_latex_scale * 12 tick_size = inverse_latex_scale * 8 # learn how to configure: # http://matplotlib.sourceforge.net/users/customizing.html params = {&#39;backend&#39;: &#39;ps&#39;, &#39;axes.labelsize&#39;: text_size, &#39;legend.fontsize&#39;: tick_size, &#39;legend.handlelength&#39;: 2.5, &#39;legend.borderaxespad&#39;: 0, &#39;xtick.labelsize&#39;: tick_size, &#39;ytick.labelsize&#39;: tick_size, &#39;font.family&#39;: &#39;serif&#39;, &#39;font.size&#39;: text_size, # Times, Palatino, New Century Schoolbook, # Bookman, Computer Modern Roman &#39;font.serif&#39;: [&#39;Times&#39;], &#39;ps.usedistiller&#39;: &#39;xpdf&#39;, &#39;text.usetex&#39;: True, &#39;figure.figsize&#39;: fig_size, # include here any neede package for latex &#39;text.latex.preamble&#39;: [r&#39; usepackage{amsmath}&#39;, ], } plt.rcParams.update(params) plt.ioff() plt.clf() # figsize accepts only inches. fig = plt.figure(1, figsize=fig_size) gs = gridspec.GridSpec(3, 2, width_ratios=[1,0.5], height_ratios=[1,0.7,0.3]) gs.update(left=0.16, right=0.86,top=0.92, bottom=0.08, hspace=0.05, wspace=0.05) . subplot a . ax0 = plt.subplot(gs[0, :]) heaviside = lambda x: 0.5 * (np.sign(x) + 1) x = np.arange(0, 10.01, 0.01) ax0.plot(x, heaviside(x - 2), color=&#39;purple&#39;, lw=3) ax0.text(2.5, 1.1, r&quot;$ longleftarrow$ heaviside&quot;) # y ticks as a percentage ax0.set_yticks(np.arange(-0.5, 2.0, 0.5)) def to_percent(y, position): # Ignore the passed in position. This has the effect of scaling the default # tick locations. s = &quot;{:+.0f}&quot;.format(y * 100) # str(100 * y) # The percent symbol needs escaping in latex if matplotlib.rcParams[&#39;text.usetex&#39;] is True: return s + r&#39;$ %$&#39; else: return s + &#39;%&#39; # Create the formatter using the function to_percent. This multiplies all the # default labels by 100, making them all percentages formatter = FuncFormatter(to_percent) # Set the formatter ax0.yaxis.set_major_formatter(formatter) ax0.set_ylabel(&quot;heaviside, percentage&quot;) # x ticks on top ax0.axis([x.min(), x.max(), -0.5, 1.5]) ax0.xaxis.tick_top() ax0.set_xlabel(r&quot;x labels on top&quot;) ax0.xaxis.set_label_position(&quot;top&quot;) # transAxes makes position relative to axes ax0.text(0.97, 0.97, r&quot; textbf{a}&quot;, transform=ax0.transAxes, horizontalalignment=&#39;right&#39;, verticalalignment=&#39;top&#39;) # copy window with same x axis (y will be different) ax0b = ax0.twinx() ax0b.plot(x, np.tanh(x - 5), color=&quot;green&quot;, linewidth=3) ax0b.axis([x.min(), x.max(), -1.1, 2.5]) ax0b.text(5.5, 0, r&quot;tanh $ longrightarrow$&quot;) ax0b.set_ylabel(r&#39;tanh, offset label&#39;) ax0b.yaxis.set_label_coords(1.1, 0.70) . subplot b . ax10 = plt.subplot(gs[1, 0]) x = np.arange(-5, 5, 0.01) y = np.exp(-x) ax10.plot(x, y, color=&quot;orange&quot;, lw=3) ax10.set_yscale(&#39;log&#39;, basey=2) ax10.set_yticks(2.0 ** np.arange(-7, 7, 3)) ax10.text(1.0, 1, r&quot;$y=e^{-x}$&quot;) ax10.set_xticks(np.arange(-5, 6, 2)) ax10.set_xticklabels(np.arange(-5, 6, 2), y=0.15) ax10.get_yaxis().set_tick_params(direction=&#39;out&#39;) ax10.set_ylabel(&quot;log scale base 2&quot;, labelpad=15) ax10.text(0.97, 0.97, r&quot; textbf{b}&quot;, transform=ax10.transAxes, horizontalalignment=&#39;right&#39;, verticalalignment=&#39;top&#39;) . Text(0.97, 0.97, &#39; textbf{b}&#39;) . subplot c . ax11 = plt.subplot(gs[1, 1]) x = np.arange(1.0, np.e ** 4, 0.01) y = x ** (-0.8) ax11.plot(x, y, color=&quot;cyan&quot;, lw=3) ax11.text(2, 1, r&quot;$y=x^{-0.8}$&quot;, fontsize=tick_size) ax11.loglog(x, y, basex=np.e, basey=np.e) xt = np.exp(np.arange(1, 4, 1)) yt = np.pi ** (np.arange(-3, 2, 1)) ax11.set_xticks(xt) ax11.set_xticklabels(xt, y=0.15) ax11.set_yticks(yt) def ticks_e(y, pos): # base e return r&#39;$e^{:.0f}$&#39;.format(np.log(y)) def ticks_pi(y, pos): # base pi, why not? return r&#39;$ pi^{%+.0f}$&#39;%(np.log(y)/np.log(np.pi)) ax11.xaxis.set_major_formatter(FuncFormatter(ticks_e)) ax11.yaxis.set_major_formatter(FuncFormatter(ticks_pi)) ax11.yaxis.tick_right() ax11.yaxis.set_label_position(&quot;right&quot;) ax11.set_ylabel(&quot;right side&quot;, labelpad=10) ax11.text(0.97, 0.97, r&quot; textbf{c}&quot;, transform=ax11.transAxes, horizontalalignment=&#39;right&#39;, verticalalignment=&#39;top&#39;) . Text(0.97, 0.97, &#39; textbf{c}&#39;) . subplot d . ax20 = plt.subplot(gs[2, 0]) ax20.axis([0, 1, 0, 1]) ax20.set_xticks(np.arange(0, 1.1, 0.2)) ax20.set_xticklabels([&quot;January&quot;, &quot;February&quot;, &quot;March&quot;, &quot;April&quot;, &quot;May&quot;, &quot;June&quot;], rotation=30, horizontalalignment=&quot;right&quot;) ax20.set_yticks([]) ax20.text(0.97, 0.97, r&quot; textbf{d}&quot;, transform=ax20.transAxes, horizontalalignment=&#39;right&#39;, verticalalignment=&#39;top&#39;) . Text(0.97, 0.97, &#39; textbf{d}&#39;) . subplot e . ax21 = plt.subplot(gs[2, 1]) ax21.set_xticks([]) ax21.set_yticks([]) ax21.axis([0, 1, 0, 1]) ax21.text(0.97, 0.97, r&quot; textbf{e}&quot;, transform=ax21.transAxes, horizontalalignment=&#39;right&#39;, verticalalignment=&#39;top&#39;) . Text(0.97, 0.97, &#39; textbf{e}&#39;) . %matplotlib notebook fig.savefig(&quot;./python_figures/subplot-grid.png&quot;, dpi=300) fig .",
            "url": "https://yairmau.github.io/website/jupyter/2020/01/01/subplotgrid.html",
            "relUrl": "/jupyter/2020/01/01/subplotgrid.html",
            "date": " â€¢ Jan 1, 2020"
        }
        
    
  
    
        ,"post67": {
            "title": "Streamplot",
            "content": ". Introduction . Streamplot of a two-dimensional linear system, with eigenvectors and nullclines. Python shows LaTeX equations beautifully. Main features: meshgrid, streamplot, contour, legend, LaTeX . The code . %matplotlib notebook import matplotlib import matplotlib.pyplot as plt import numpy as np . make graph look pretty . # this is a latex constant, don&#39;t change it. pts_per_inch = 72.27 # write &quot; the textwidth&quot; (or &quot; showthe columnwidth&quot; for a 2 collumn text) text_width_in_pts = 300.0 # inside a figure environment in latex, the result will be on the # dvi/pdf next to the figure. See url above. text_width_in_inches = text_width_in_pts / pts_per_inch # make rectangles with a nice proportion golden_ratio = 0.618 # figure.png or figure.eps will be intentionally larger, because it is prettier inverse_latex_scale = 2 # when compiling latex code, use # includegraphics[scale=(1/inverse_latex_scale)]{figure} # we want the figure to occupy 2/3 (for example) of the text width fig_proportion = (3.0 / 3.0) csize = inverse_latex_scale * fig_proportion * text_width_in_inches # always 1.0 on the first argument fig_size = (1.0 * csize, 0.8 * csize) # find out the fontsize of your latex text, and put it here text_size = inverse_latex_scale * 12 tick_size = inverse_latex_scale * 8 # learn how to configure: # http://matplotlib.sourceforge.net/users/customizing.html params = {&#39;backend&#39;: &#39;ps&#39;, &#39;axes.labelsize&#39;: text_size, &#39;legend.fontsize&#39;: tick_size, &#39;legend.handlelength&#39;: 2.5, &#39;legend.borderaxespad&#39;: 0, &#39;xtick.labelsize&#39;: tick_size, &#39;ytick.labelsize&#39;: tick_size, &#39;font.family&#39;: &#39;serif&#39;, &#39;font.size&#39;: text_size, # Times, Palatino, New Century Schoolbook, # Bookman, Computer Modern Roman # &#39;font.serif&#39;: [&#39;Times&#39;], &#39;ps.usedistiller&#39;: &#39;xpdf&#39;, &#39;text.usetex&#39;: True, &#39;figure.figsize&#39;: fig_size, # include here any neede package for latex &#39;text.latex.preamble&#39;: [r&#39; usepackage{amsmath}&#39;, ], } plt.rcParams.update(params) plt.ioff() # figsize accepts only inches. fig = plt.figure(1, figsize=fig_size) fig.subplots_adjust(left=0.10, right=0.97, top=0.82, bottom=0.10, hspace=0.02, wspace=0.02) ax = fig.add_subplot(111) . define parameters, system of equations, and equation for eigenvectors . p = {&#39;a&#39;: -1.0, &#39;b&#39;: +0.2, &#39;c&#39;: +1.2, &#39;d&#39;: -1.5} # the equations def system_equations(x,y): return [p[&#39;a&#39;] * x + p[&#39;b&#39;] * y, p[&#39;c&#39;] * x + p[&#39;d&#39;] * y, ] # eigenvectors eigen_vec = 100 * np.array([ [(p[&#39;a&#39;] - p[&#39;d&#39;] - np.sqrt((p[&#39;a&#39;] - p[&#39;d&#39;]) ** 2 + 4.0 * p[&#39;b&#39;] * p[&#39;c&#39;])) / (2.0 * p[&#39;c&#39;]), 1.0], [(p[&#39;a&#39;] - p[&#39;d&#39;] + np.sqrt((p[&#39;a&#39;] - p[&#39;d&#39;]) ** 2 + 4.0 * p[&#39;b&#39;] * p[&#39;c&#39;])) / (2.0 * p[&#39;c&#39;]), 1.0], ]) . there are two equivalent ways to build a mesh, choose the one that makes more sense to you... . min_x, max_x = [-1, 1] min_y, max_y = [-4, 4] divJ = 50j div = 50 # 1st way # Y, X = np.mgrid[min_y:max_y:div,min_x:max_x:div] # 2nd way X, Y = np.meshgrid(np.linspace(min_x, max_x, div), np.linspace(min_y, max_y, div)) # streamplot density = 2 * [0.80] minlength = 0.2 arrow_color = 3 * [0.5] ax.streamplot(X, Y, system_equations(X, Y)[0], system_equations(X, Y)[1], density=density, color=arrow_color, arrowsize=2, linewidth=2, minlength=minlength) . &lt;matplotlib.streamplot.StreamplotSet at 0x7ff7c5afdd50&gt; . nullclines . null_0 = ax.contour(X, Y, system_equations(X, Y)[0], levels=[0], colors=&#39;black&#39;, linewidths=3) null_1 = ax.contour(X, Y,system_equations(X, Y)[1], levels=[0], colors=&#39;blue&#39;, linewidths=3) n0 = null_0.collections[0] n1 = null_1.collections[0] . eigenvectors . eigen_0, = ax.plot([eigen_vec[0, 0],-eigen_vec[0, 0]], [eigen_vec[0, 1],-eigen_vec[0, 1]], color=&#39;red&#39;, lw=2, ls=&quot;--&quot;) eigen_1, = ax.plot([eigen_vec[1, 0],-eigen_vec[1, 0]], [eigen_vec[1, 1],-eigen_vec[1, 1]], color=&#39;orange&#39;, lw=2, ls=&quot;--&quot;) dash = (15, 10, 15, 10) eigen_0.set_dashes(dash) eigen_1.set_dashes(dash) . some labels, legend, and text . ax.set_ylabel(r&quot;$y$&quot;, rotation=&#39;horizontal&#39;) ax.set_xlabel(r&quot;$x$&quot;, labelpad=5) ax.legend([n0, n1, eigen_0, eigen_1], [r&#39;$dx/dt=0$&#39;, r&#39;$dy/dt=0$&#39;, &quot;eigenvector 1&quot;, &quot;eigenvector 2&quot;], loc=&quot;lower right&quot;, frameon=True, fancybox=False, shadow=False, ncol=2, borderpad=0.5, labelspacing=0.5, handlelength=3, handletextpad=0.1, borderaxespad=0.3, columnspacing=2) ax.text(-1.0, 4.3, (r&quot;$ frac{d}{dt} begin{pmatrix}x y end{pmatrix}=$&quot; r&quot;$ begin{pmatrix}a&amp;b c&amp;d end{pmatrix} cdot$&quot; r&quot;$ begin{pmatrix}x y end{pmatrix}$&quot;)) ax.text(0.1, 5.0, r&quot;$a={:.1f} qquad b={:.1f}$ &quot;.format(p[&#39;a&#39;], p[&#39;b&#39;])) ax.text(0.1, 4.3, r&quot;$c={:.1f} qquad d={:.1f}$ &quot;.format(p[&#39;c&#39;], p[&#39;d&#39;])) ax.axis([min_x, max_x, min_y, max_y]) fig.savefig(&quot;python_figures/streamplot.png&quot;, resolution=300) plt.draw() fig .",
            "url": "https://yairmau.github.io/website/jupyter/2020/01/01/streamplot.html",
            "relUrl": "/jupyter/2020/01/01/streamplot.html",
            "date": " â€¢ Jan 1, 2020"
        }
        
    
  
    
        ,"post68": {
            "title": "Least Squares",
            "content": ". Introduction . This code produces the figure above. It&#39;s main tool is the curve_fit method, that allows us to fit any function to data, and get optimal parameter values. . The code . %matplotlib notebook import matplotlib.pyplot as plt import numpy as np import matplotlib.gridspec as gridspec import scipy.special from scipy.optimize import curve_fit import matplotlib.patches as patches . Make graph look pretty . %%capture out %matplotlib notebook # http://wiki.scipy.org/Cookbook/Matplotlib/LaTeX_Examples # this is a latex constant, don&#39;t change it. pts_per_inch = 72.27 # write &quot; the textwidth&quot; (or &quot; showthe columnwidth&quot; for a 2 collumn text) text_width_in_pts = 300.0 # inside a figure environment in latex, the result will be on the # dvi/pdf next to the figure. See url above. text_width_in_inches = text_width_in_pts / pts_per_inch # make rectangles with a nice proportion golden_ratio = 0.618 # figure.png or figure.eps will be intentionally larger, because it is prettier inverse_latex_scale = 2 # when compiling latex code, use # includegraphics[scale=(1/inverse_latex_scale)]{figure} # we want the figure to occupy 2/3 (for example) of the text width fig_proportion = (3.0 / 3.0) csize = inverse_latex_scale * fig_proportion * text_width_in_inches # always 1.0 on the first argument fig_size = (1.0 * csize, 0.85 * csize) # find out the fontsize of your latex text, and put it here text_size = inverse_latex_scale * 12 label_size = inverse_latex_scale * 10 tick_size = inverse_latex_scale * 8 # learn how to configure: # http://matplotlib.sourceforge.net/users/customizing.html params = {#&#39;backend&#39;: &#39;ps&#39;, &#39;axes.labelsize&#39;: 16, &#39;legend.fontsize&#39;: tick_size, &#39;legend.handlelength&#39;: 2.5, &#39;legend.borderaxespad&#39;: 0, &#39;axes.labelsize&#39;: label_size, &#39;xtick.labelsize&#39;: tick_size, &#39;ytick.labelsize&#39;: tick_size, &#39;font.family&#39;: &#39;serif&#39;, &#39;font.size&#39;: text_size, # &#39;font.serif&#39;: [&#39;Computer Modern Roman&#39;], &#39;ps.usedistiller&#39;: &#39;xpdf&#39;, &#39;text.usetex&#39;: True, &#39;figure.figsize&#39;: fig_size, # include here any neede package for latex &#39;text.latex.preamble&#39;: [r&#39; usepackage{amsmath}&#39;], } plt.rcParams.update(params) plt.clf() fig = plt.figure(1, figsize=fig_size) # figsize accepts only inches. fig.subplots_adjust(left=0.04, right=0.98, top=0.93, bottom=0.15, hspace=0.05, wspace=0.02) plt.ioff() . Configure axes and some function definitions . x = np.arange(0, 12, 0.4) ax1 = fig.add_subplot(211, aspect=&#39;equal&#39;) ax2 = fig.add_subplot(212, aspect=&#39;equal&#39;) ax1.set_xlim((x.min(), x.max())) ax2.set_xlim((x.min(), x.max())) ax1.set_ylim(-1, 3.5) ax2.set_ylim(-1, 3.5) ax1.set_xticklabels([]) ax1.set_yticks(np.arange(-1, 4)) ax2.set_yticks(np.arange(-1, 4)) def func(x, par0, par1, par2): return par0 + np.cos(par1 * x + par2) def add_rec(ax, c, v, col): ax.add_patch( patches.Rectangle( c, # (x,y) np.abs(v), # width v, # height alpha=0.4, color=col ) ) . Now let&#39;s plot some stuff . %matplotlib notebook # the parameter values par = (1, 2, 1) # generating data with noise y = func(x, *par) + (np.random.random(len(x)) - 0.5) ax1.plot(x, y, marker=&#39;o&#39;, ls=&#39;None&#39;, markerfacecolor=&quot;blue&quot;, markeredgecolor=&quot;black&quot;) ax2.plot(x, y, marker=&#39;o&#39;, ls=&#39;None&#39;, markerfacecolor=&quot;red&quot;, markeredgecolor=&quot;black&quot;) # best fit popt, pcov = curve_fit(func, x, y, p0=(1.5, 1.5, 2.5)) # p0 = initial guess p0, p1, p2 = popt # The total sum of squares (proportional to the variance of the data) SStot = ((y - y.mean()) ** 2).sum() # The sum of squares of residuals SSres = ((y - func(x, p0, p1, p2)) ** 2).sum() Rsquared = 1 - SSres / SStot # plot best fit h = np.linspace(x.min(), x.max(), 1001) fit, = ax1.plot(h, func(h, p0, p1, p2), color=&#39;black&#39;, linewidth=2) ax1.legend([fit], [&quot;Best fit&quot;], loc=&quot;upper right&quot;, frameon=False, handlelength=4) # plot mean mean, = ax2.plot(h, h * 0 + np.mean(y), ls=&#39;--&#39;, color=&#39;black&#39;, linewidth=2) ax2.legend([mean], [&quot;Mean&quot;], loc=&quot;upper right&quot;, frameon=False, handlelength=4) # plot blue and red squares for ind in np.arange(len(x)): x0 = x[ind] y0 = y[ind] # print(x0,y0) v1 = y0 - func(x0, p0, p1, p2) v2 = y0 - y.mean() add_rec(ax1, (x0, y0), -v1, &quot;blue&quot;) add_rec(ax2, (x0, y0), -v2, &quot;red&quot;) ax2.text(0.5, 2.7, r&quot;Total sum of squares: {:.1f}&quot;.format(SStot)) ax1.text(0.5, 2.7, r&quot;Sum of squares of residuals: {:.1f}&quot;.format(SSres)) ax2.set_xlabel( r&quot;R-squared = $1 - displaystyle frac{ text{blue area}}{ text{red area}}$ = &quot; + &quot;{:.2f}&quot;.format(Rsquared)) ax1.set_xlabel( r&quot;Data: $f(x) = p_0 + cos(p_1 x + p_2)+ $ noise &quot;) ax1.xaxis.set_label_position(&quot;top&quot;) fig.savefig(&quot;./python_figures/least-squares.png&quot;,dpi=300) fig .",
            "url": "https://yairmau.github.io/website/jupyter/2020/01/01/leastsquares.html",
            "relUrl": "/jupyter/2020/01/01/leastsquares.html",
            "date": " â€¢ Jan 1, 2020"
        }
        
    
  
    
        ,"post69": {
            "title": "A hysteresis mechanism",
            "content": ". Introduction . Hysteresis mechanism created by bistability of states. . Energy function: $$f = u^4 - 2u^2 + hu$$ . The code . # i.e., if you want to see the animation in real time. import matplotlib matplotlib.use(&#39;Agg&#39;) . import matplotlib.pyplot as plt import numpy as np import os import sympy from scipy.integrate import ode # learn how to configure: http://matplotlib.sourceforge.net/users/customizing.html params = {#&#39;backend&#39;: &#39;GTKAgg&#39;, &#39;legend.handlelength&#39;: 2.5, &#39;legend.borderaxespad&#39;: 0, &#39;font.family&#39;:&#39;serif&#39;, &#39;font.size&#39;: 18, &#39;font.serif&#39;:[&#39;Times&#39;], # Times, Palatino, New Century Schoolbook, Bookman, Computer Modern Roman &#39;ps.usedistiller&#39;: &#39;xpdf&#39;, &#39;text.usetex&#39;: True, } plt.rcParams.update(params) fig=plt.figure(1,figsize=(9.6,5.4),dpi=100) # 1920x1080 # figsize accepts only inches. if you rather think in cm, change the code yourself. fig.clf() fig.subplots_adjust(left=0.07, right=0.93,top=0.90, bottom=0.12,hspace=0.02,wspace=0.10) Hlim=2.5 # parameter range from -Hlim to Hlim ax1=fig.add_subplot(121) ax1.set_xticks([]) ax1.set_yticks([]) ax1.set_xlabel(r&#39;System response&#39;,labelpad=12) ax1.set_ylabel(&#39;Energy&#39;,labelpad=12) ax1.axis([-Hlim,Hlim,-5,5]) ax2=fig.add_subplot(122) ax2.set_xticks([]) ax2.set_yticks([]) ax2.set_xlabel(r&#39;Parameter&#39;,labelpad=12) ax2.set_ylabel(r&#39;System response&#39;,labelpad=12) ax2.yaxis.set_label_position(&quot;right&quot;) ax2.axis([-Hlim*1.2,Hlim*1.2,-2,2]) frame_names = [] frame_index = 0 make_movie=True plt.ion() . f = lambda u,h: u**4-2*u**2+h*u fprime = lambda u,h: sympy.diff(f(u,h),u) Hinit=Hlim ulim=2.5 # system response axis, from -ulim to ulim u = np.linspace(-ulim,ulim,101) x = sympy.Symbol(&#39;x&#39;) def res(h): &quot;&quot;&quot;System response is one of the real roots of the energy function derivative &quot;&quot;&quot; # derivative roots, complex resp = sympy.solvers.solve(fprime(x,h),x) # numerical evaluation resp = map(sympy.N,resp) # let&#39;s check which roots are real isreal = len(resp)*[False] for i in range(len(resp)): # negligible imaginary component if np.abs(sympy.functions.im(resp[i]))&lt;1e-15: resp[i]=sympy.functions.re(resp[i]) isreal[i]=True resp = np.array(resp) # return only real roots return resp[np.array(isreal)] # let&#39;s plot stuff, and make a nice movie #### left plot, ax1 #### line_func, = ax1.plot(u,f(u,Hinit),lw=2,color=&#39;black&#39;) # ball color ball_color = &quot;blue&quot; # minimum = the smallest root, the leftmost root mini = np.min(res(Hinit)) # calculated for initial parameter value boost = 0.22 # so that ball sits on top of the curve # plot ball ball_u, = ax1.plot([mini],[f(mini,Hinit)+boost],&#39;o&#39;, markersize=12, markerfacecolor=ball_color) #### right plot, ax2 #### # build empty hysteresis array, we will add values # as simulation progresses deetype = np.dtype([(&#39;h&#39;, &#39;float64&#39;), (&#39;u&#39;, &#39;float64&#39;)]) hysteresis = np.array([(Hinit,mini)],dtype=deetype) line_hyst, = ax2.plot(hysteresis[&#39;h&#39;],hysteresis[&#39;u&#39;], lw=2,color=&#39;black&#39;) ballH, = ax2.plot([hysteresis[&#39;h&#39;][-1]],[hysteresis[&#39;u&#39;][-1]],&#39;o&#39;, markersize=12, markerfacecolor=ball_color) plt.show() . Total_time = 15 # seconds fps = 24 # frames per second # divided by 2 because we ramp down then up param_vec = np.linspace(Hlim,-Hlim,Total_time*fps/2) # ramp down for H in param_vec: line_func.set_data(u,f(u,H)) # update line on the left mini = np.min(res(H)) # calculate new minimum ball_u.set_data([mini],[f(mini,H)+boost]) # update ball on the left new_line = np.array([(H,mini)],dtype=deetype) # create new line # append new line to hysteresis array hysteresis = np.concatenate((hysteresis,new_line)) line_hyst.set_data(hysteresis[&#39;h&#39;],hysteresis[&#39;u&#39;]) # update line ballH.set_data([hysteresis[&#39;h&#39;][-1]],[hysteresis[&#39;u&#39;][-1]]) # update ball on the right fig.canvas.draw() if make_movie: fname = &quot;_tmp{:05d}.png&quot;.format(frame_index) frame_names.append(fname) fig.savefig(fname,dpi=200) frame_index+=1 # ramp up for H in param_vec[::-1]: # just reverse parameter array line_func.set_data(u,f(u,H)) maxi = np.max(res(H)) # everything is the same, but now with maximum ball_u.set_data([maxi],[f(maxi,H)+boost]) new_line = np.array([(H,maxi)],dtype=deetype) hysteresis = np.concatenate((hysteresis,new_line)) line_hyst.set_data(hysteresis[&#39;h&#39;],hysteresis[&#39;u&#39;]) ballH.set_data([hysteresis[&#39;h&#39;][-1]],[hysteresis[&#39;u&#39;][-1]]) fig.canvas.draw() if make_movie: fname = &quot;_tmp{:05d}.png&quot;.format(frame_index) frame_names.append(fname) fig.savefig(fname,dpi=200) frame_index+=1 if make_movie: frames = &quot;_tmp%5d.png&quot; movie_command = &quot;ffmpeg -y -r {:} -i {:} ball.mp4&quot;.format(fps,frames) os.system(movie_command) for fname in frame_names: # pass os.remove(fname) .",
            "url": "https://yairmau.github.io/website/jupyter/2020/01/01/hysteresis.html",
            "relUrl": "/jupyter/2020/01/01/hysteresis.html",
            "date": " â€¢ Jan 1, 2020"
        }
        
    
  
    
        ,"post70": {
            "title": "Fun with histograms",
            "content": ". Introduction . This code produces the figure above. I tried to showcase a few things one can do with 1d and 2d histograms. . The code . import matplotlib.pyplot as plt import numpy as np import matplotlib.gridspec as gridspec import scipy.special from scipy.optimize import curve_fit . make graph look pretty . # this is a latex constant, don&#39;t change it. pts_per_inch = 72.27 # write &quot; the textwidth&quot; (or &quot; showthe columnwidth&quot; for a 2 collumn text) text_width_in_pts = 450.0 # inside a figure environment in latex, the result will be on the # dvi/pdf next to the figure. See url above. text_width_in_inches = text_width_in_pts / pts_per_inch # make rectangles with a nice proportion golden_ratio = 0.618 # figure.png or figure.eps will be intentionally larger, because it is prettier inverse_latex_scale = 2 # when compiling latex code, use # includegraphics[scale=(1/inverse_latex_scale)]{figure} # we want the figure to occupy 2/3 (for example) of the text width fig_proportion = (3.0 / 3.0) csize = inverse_latex_scale * fig_proportion * text_width_in_inches # always 1.0 on the first argument fig_size = (1.0 * csize, 0.5 * csize) # find out the fontsize of your latex text, and put it here text_size = inverse_latex_scale * 12 label_size = inverse_latex_scale * 10 tick_size = inverse_latex_scale * 8 # learn how to configure: # http://matplotlib.sourceforge.net/users/customizing.html params = {&#39;backend&#39;: &#39;ps&#39;, &#39;axes.labelsize&#39;: 16, &#39;legend.fontsize&#39;: tick_size, &#39;legend.handlelength&#39;: 2.5, &#39;legend.borderaxespad&#39;: 0, &#39;axes.labelsize&#39;: label_size, &#39;xtick.labelsize&#39;: tick_size, &#39;ytick.labelsize&#39;: tick_size, &#39;font.family&#39;: &#39;serif&#39;, &#39;font.size&#39;: text_size, # &#39;font.serif&#39;: [&#39;Computer Modern Roman&#39;], &#39;ps.usedistiller&#39;: &#39;xpdf&#39;, &#39;text.usetex&#39;: True, &#39;figure.figsize&#39;: fig_size, } plt.rcParams.update(params) plt.ioff() fig = plt.figure(1, figsize=fig_size) # figsize accepts only inches. . Panels on the left of the figure . gs = gridspec.GridSpec(2, 2, width_ratios=[1, 0.2], height_ratios=[0.2, 1]) gs.update(left=0.05, right=0.50, top=0.95, bottom=0.10, hspace=0.02, wspace=0.02) sigma = 1.0 # standard deviation (spread) mu = 0.0 # mean (center) of the distribution x = np.random.normal(loc=mu, scale=sigma, size=5000) k = 2.0 # shape theta = 1.0 # scale y = np.random.gamma(shape=k, scale=theta, size=5000) # bottom left panel ax10 = plt.subplot(gs[1, 0]) counts, xedges, yedges, image = ax10.hist2d(x, y, bins=40, cmap=&quot;YlOrRd&quot;, density=True) dx = xedges[1] - xedges[0] dy = yedges[1] - yedges[0] xvec = xedges[:-1] + dx / 2 yvec = yedges[:-1] + dy / 2 ax10.set_xlabel(r&quot;$x$&quot;) ax10.set_ylabel(r&quot;$y$&quot;, rotation=&quot;horizontal&quot;) ax10.text(-2, 8, r&quot;$p(x,y)$&quot;) ax10.set_xlim([xedges.min(), xedges.max()]) ax10.set_ylim([yedges.min(), yedges.max()]) # top left panel ax00 = plt.subplot(gs[0, 0]) gaussian = (1.0 / np.sqrt(2.0 * np.pi * sigma ** 2)) * np.exp(-((xvec - mu) ** 2) / (2.0 * sigma ** 2)) xdist = counts.sum(axis=1) * dy ax00.bar(xvec, xdist, width=dx, fill=False, edgecolor=&#39;black&#39;, alpha=0.8) ax00.plot(xvec, gaussian, color=&#39;black&#39;) ax00.set_xlim([xedges.min(), xedges.max()]) ax00.set_xticklabels([]) ax00.set_yticks([]) ax00.set_xlabel(&quot;Normal distribution&quot;, fontsize=16) ax00.xaxis.set_label_position(&quot;top&quot;) ax00.set_ylabel(r&quot;$p(x)$&quot;, rotation=&quot;horizontal&quot;, labelpad=20) # bottom right panel ax11 = plt.subplot(gs[1, 1]) gamma_dist = yvec ** (k - 1.0) * np.exp(-yvec / theta) / (theta ** k * scipy.special.gamma(k)) ydist = counts.sum(axis=0) * dx ax11.barh(yvec, ydist, height=dy, fill=False, edgecolor=&#39;black&#39;, alpha=0.8) ax11.plot(gamma_dist, yvec, color=&#39;black&#39;) ax11.set_ylim([yedges.min(), yedges.max()]) ax11.set_xticks([]) ax11.set_yticklabels([]) ax11.set_ylabel(&quot;Gamma distribution&quot;, fontsize=16) ax11.yaxis.set_label_position(&quot;right&quot;) ax11.set_xlabel(r&quot;$p(y)$&quot;) ax11.xaxis.set_label_position(&quot;top&quot;) . Panels on the right of the figure . gs2 = gridspec.GridSpec(2, 1, width_ratios=[1], height_ratios=[1, 1]) gs2.update(left=0.60, right=0.98, top=0.95, bottom=0.10, hspace=0.02, wspace=0.05) x = np.random.normal(loc=0, scale=1, size=1000) y = np.random.gamma(shape=2, size=1000) bx10 = plt.subplot(gs2[1, 0]) bx00 = plt.subplot(gs2[0, 0]) N = 100 a = np.random.gamma(shape=5, size=N) my_bins = np.arange(0,15,1.5) n1, bins1, patches1 = bx00.hist(a, bins=my_bins, density=True, histtype=&#39;stepfilled&#39;, alpha=0.2, hatch=&#39;/&#39;) bx00.set_xlim([0, 15]) bx00.set_ylim([0, 0.28]) bx00.set_xticklabels([]) bx00.set_xlabel(r&quot; texttt{plt.hist}&quot;) bx00.xaxis.set_label_position(&quot;top&quot;) # the following way is equivalent to plt.hist, but it gives # the user more flexibility when plotting and analysing the results n2, bins2 = np.histogram(a, bins=my_bins, density=True) wid = bins2[1] - bins2[0] red, = bx10.plot(bins2[:-1]+wid/2, n2, marker=&#39;o&#39;, color=&#39;red&#39;) bx10.bar(bins2[:-1], n2, width=wid, fill=False, edgecolor=&#39;black&#39;, linewidth=3, alpha=0.8, align=&quot;edge&quot;) bx10.set_xlim([0, 15]) bx10.set_ylim([0, 0.28]) bx10.set_xlabel(r&quot; texttt{np.histogram}; quad texttt{plt.bar}&quot;) . Text(0.5, 0, &#39; texttt{np.histogram}; quad texttt{plt.bar}&#39;) . best fit . xdata = my_bins[:-1] + wid/2 ydata = n2 def func(x, p1, p2): return x ** (p1 - 1.0) * np.exp(-x / p2) / (p2 ** p1 * scipy.special.gamma(p1)) popt, pcov = curve_fit(func, xdata, ydata, p0=(1.5, 1.5)) # p0 = initial guess p1, p2 = popt SStot = ((ydata - ydata.mean()) ** 2).sum() SSres = ((ydata - func(xdata, p1, p2)) ** 1).sum() Rsquared = 1 - SSres / SStot h = np.linspace(0,15,101) bx00.plot(h, func(h, p1, p2), color=&#39;blue&#39;, linewidth=2) # dummy plot, just so we can have a legend on the bottom panel blue, = ax10.plot([100],[100], color=&#39;blue&#39;, linewidth=2, label=&quot;Best fit&quot;) bx10.legend([red,blue],[r&#39;Data&#39;,r&#39;Best fit, $r^2=${:.2f}&#39;.format(Rsquared)], loc=&#39;upper right&#39;, frameon=False, handlelength=4, markerfirst=False, numpoints=3) . /Users/yairmau/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in double_scalars after removing the cwd from sys.path. . &lt;matplotlib.legend.Legend at 0x7fde860d7ed0&gt; . fig.savefig(&quot;./python_figures/histograms.png&quot;,dpi=300) fig .",
            "url": "https://yairmau.github.io/website/jupyter/2020/01/01/histograms.html",
            "relUrl": "/jupyter/2020/01/01/histograms.html",
            "date": " â€¢ Jan 1, 2020"
        }
        
    
  
    
        ,"post71": {
            "title": "Hilbert Curve",
            "content": ". Introduction . The Hilbert curve is calculated iteratively. Check out this: Hilbert_curve: Representation_as_Lindenmayer_system . The code . import matplotlib matplotlib.use(&#39;AGG&#39;) . import numpy as np import matplotlib.pyplot as plt import os . def apply_rules(s): &quot;&quot;&quot; Hilbert Curve as a Lindenmayer system (L-system) https://en.wikipedia.org/wiki/Hilbert_curve#Representation_as_Lindenmayer_system&quot;&quot;&quot; s=s.replace(&quot;a&quot;,&quot;-Bf+AfA+fB-&quot;) # capital letters &quot;A&quot; and &quot;B&quot; so that the second operation s=s.replace(&quot;b&quot;,&quot;+Af-BfB-fA+&quot;) # doesn&#39;t apply to the changes already made return s.lower() # make everyone lowercase axiom = &quot;a&quot; n=3 # number of iterations # displacements, ordered in a counter-clockwise direction dxdy = np.array([[ 1, 0], # right [ 0, 1], # up [-1, 0], # left [ 0,-1] ]) # down # displacement is of size 1, so the higher n is, the greater the domain length = 2**n-1; margin = 0.05*length domain = [0-margin,length+margin,0-margin,length+margin] # a 5% margin around the curve s = axiom # string to iterate upon for i in np.arange(n): s = apply_rules(s) . make_movie=True plt.ion() # interactive mode disabled if &quot;matplotlib.use(&#39;AGG&#39;)&quot; fig = plt.figure(figsize=(6,6)) ax = fig.add_subplot(111) ax.axis(&#39;off&#39;) # no frame ax.axis(domain) # domain size ax.set_aspect(&#39;equal&#39;) # square look ax.set_xticks([]); ax.set_yticks([]) # no ticks ax.set_title(r&quot;$n = {:d}$&quot;.format(n)) plt.show() # &quot;a&quot; and &quot;b&quot; can be erased now s=s.replace(&quot;a&quot;,&quot;&quot;) s=s.replace(&quot;b&quot;,&quot;&quot;) frame_names = [] # these two are only relevant if make_movie==True frame_counter=0 p = np.array([[0.0,0.0]]) # this is the starting point (0,0) p_plot, = plt.plot(p[:,0],p[:,1],color=&quot;black&quot;) # iterate on the string s for i,c in enumerate(s): # uncomment to see how fast things are going # print(&quot;{:d}/{:d}&quot;.format(i,len(s))) # rotations &quot;+&quot; and &quot;-&quot; change the displacement array dxdy # &quot;+&quot; means clockwise rotation if c == &#39;+&#39;: dxdy = np.roll(dxdy,+1,axis=0) # &quot;-&quot; means counter-clockwise rotation if c == &#39;-&#39;: dxdy = np.roll(dxdy,-1,axis=0) # forward &quot;f&quot; if c == &#39;f&#39;: # add one more point to array p p = np.vstack([p, [p[-1,0]+dxdy[0,0],p[-1,1]+dxdy[0,1]] ]) # update p_plot data, this is MUCH faster that plotting # several line segments separately p_plot.set_data(p[:,0],p[:,1]) fig.canvas.draw() if make_movie: fname = &quot;_tmp{:05d}.png&quot;.format(frame_counter) frame_names.append(fname) fig.savefig(fname,bbox_inches=&#39;tight&#39;,resolution=300) frame_counter += 1 . if make_movie: frames = &quot;_tmp%5d.png&quot; # movie_command = &quot;mencoder mf://*.png -mf fps=24:type=png --ovc lavc -lavcopts vcodec=mpeg4:mbd=2:trell -oac copy -o hil{:d}.avi&quot;.format(n) # we might have other .png figures in the directory # in this case, use the code below f = open(&quot;png_list.txt&quot;, &quot;w&quot;) for i in frame_names: f.write(i+&quot; n&quot;) f.close() movie_command = &quot;mencoder mf://@png_list.txt -mf fps=24:type=png -ovc lavc -lavcopts vcodec=mpeg4:mbd=2:trell -oac copy -o hil{:d}.avi&quot;.format(n) err=os.system(movie_command) if err!=0: raise RuntimeError(&quot;Couldn&#39;t run mencoder. Data in tmp*.png files&quot;) for fname in frame_names: os.remove(fname) # we now have one video ready. # if you want to join several videos, use this: # sudo apt-get install gpac # MP4Box -cat part1.avi -cat part2.avi -new joinedfile.avi .",
            "url": "https://yairmau.github.io/website/jupyter/2020/01/01/hilbertcurve.html",
            "relUrl": "/jupyter/2020/01/01/hilbertcurve.html",
            "date": " â€¢ Jan 1, 2020"
        }
        
    
  
    
        ,"post72": {
            "title": "Conway's Game of Life",
            "content": ". Introduction . This is a (slightly) modified version of Glowing Python&#39;s code. I make it available here because it features a few nice things: . how to make a movie using matplotlib.animation | how to write a generator (function with yield) | how to plot a sparce array (spy) | . The code . import numpy as np from matplotlib import pyplot as plt import matplotlib.animation as manimation def life(X, steps): &quot;&quot;&quot; Conway&#39;s Game of Life. - X, matrix with the initial state of the game. - steps, number of generations. &quot;&quot;&quot; def roll_it(x, y): # rolls the matrix X in a given direction # x=1, y=0 left; x=-1, y=0 right; return np.roll(np.roll(X, y, axis=0), x, axis=1) for _ in range(steps): # count the number of neighbours # the universe is considered toroidal Y = roll_it(1, 0) + roll_it(0, 1) + roll_it(-1, 0) + roll_it(0, -1) + roll_it(1, 1) + roll_it(-1, -1) + roll_it(1, -1) + roll_it(-1, 1) # game of life rules X = np.logical_or(np.logical_and(X, Y == 2), Y == 3) X = X.astype(int) yield X . dimensions = (90, 160) # height, width X = np.zeros(dimensions) # Y by X dead cells middle_y = dimensions[0] / 2 middle_x = dimensions[1] / 2 N_iterations = 600 # acorn initial condition # http://www.conwaylife.com/w/index.php?title=Acorn X[middle_y, middle_x:middle_x+2] = 1 X[middle_y, middle_x+4:middle_x+7] = 1 X[middle_y+1, middle_x+3] = 1 X[middle_y+2, middle_x+1] = 1 . FFMpegWriter = manimation.writers[&#39;ffmpeg&#39;] metadata = dict(title=&#39;Game of life&#39;, artist=&#39;Acorn initial condition&#39;) writer = FFMpegWriter(fps=10, metadata=metadata) fig = plt.figure() fig.patch.set_facecolor(&#39;black&#39;) with writer.saving(fig, &quot;game_of_life.mp4&quot;, 300): # last argument: dpi plt.spy(X, origin=&#39;lower&#39;) plt.axis(&#39;off&#39;) writer.grab_frame() plt.clf() for i, x in enumerate(life(X, N_iterations)): plt.title(&quot;iteration: {:03d}&quot;.format(i + 1)) plt.spy(x, origin=&#39;lower&#39;) plt.axis(&#39;off&#39;) writer.grab_frame() plt.clf() .",
            "url": "https://yairmau.github.io/website/jupyter/2020/01/01/gol.html",
            "relUrl": "/jupyter/2020/01/01/gol.html",
            "date": " â€¢ Jan 1, 2020"
        }
        
    
  
    
        ,"post73": {
            "title": "Fitzhugh-Nagumo Equation",
            "content": "https://youtu.be/5au-G5FuI_A . Introduction . We simulate the Fitzhugh-Nagumo equations $$ u_t = u - u^3 - v + nabla^2 u v_t = epsilon(u - a_1 v - a_0) + delta nabla^2 v, $$ using the semi-spectral time integration method. . This simultation was heavily inspired by Aric Hagberg&#39;s simulation in &quot;From Labyrinthine Patterns to Spiral Turbulence&quot;, PRL 1994. . The code below provides 3 initial conditions, &quot;squiggle, blocks, and random&quot;. For time integration, besides the spectral method, we also provide the Euler method. Details about the semi-spectral method can be found after the code. . Parameters: $ epsilon=0.3$, $ delta=2.0$, $a_1=1.4$, and $a_0=0$. . Other simulations and Python examples can be found on my website: yairmau.com. . The code . import packages . import numpy as np import matplotlib.pyplot as plt import os from matplotlib import rcParams rcParams[&#39;font.family&#39;] = &#39;monospace&#39; . define class with all the methods . class FitzHughNagumo(object): def __init__(self, epsilon=0.3, delta=2.0, a1=1.4, a0=0.0, n=(256, 256), l=(400, 400), start=0.0, step=1.0, finish=2000.0, dt=0.1, integration_type=&quot;spectral&quot;): self.epsilon = epsilon self.delta = delta self.a1 = a1 self.a0 = a0 self.n = n self.l = l self.start = start self.step = step self.finish = finish self.dt = dt self.integration_type = integration_type self.rhs_a = np.zeros((2, self.n[0], self.n[1])) def spectral_multiplier(self): dx = float(self.l[0]) / self.n[0] dy = float(self.l[1]) / self.n[1] # wave numbers fx = 2.0 * np.pi * np.fft.fftfreq(self.n[0], dx) fy = 2.0 * np.pi * np.fft.fftfreq(self.n[1], dy) kx = np.outer(fx, np.ones(self.n[0])) ky = np.outer(np.ones(self.n[1]), fy) # multiplier mult_a = np.zeros((2, self.n[0], self.n[1])) mult_a[0] = np.exp(-(kx ** 2 + ky ** 2) * self.dt) # u mult_a[1] = np.exp(-self.delta * (kx ** 2 + ky ** 2) * self.dt) # v return mult_a def rhs_reaction(self, a): u = a[0] # alias v = a[1] # alias # FHN right hand side self.rhs_a[0] = u - u ** 3 - v self.rhs_a[1] = self.epsilon * (u - self.a1 * v - self.a0) return self.rhs_a def rhs_euler(self, a): # boundary conditions in laplacian laplacian = self.periodic_laplacian u = a[0] # alias v = a[1] # alias dx = float(self.l[0]) / self.n[0] # FHN right hand side self.rhs_a[0] = u - u ** 3 - v + laplacian(u, dx=dx) self.rhs_a[1] = self.epsilon * (u - self.a1 * v - self.a0) + self.delta * laplacian(v, dx=dx) return self.rhs_a def draw(self, a, t): u = a[0] self.im = plt.imshow(u.real, cmap=&quot;Greys_r&quot;, origin=&#39;lower&#39;, vmin=-0.534522, vmax=0.534522, interpolation=&quot;gaussian&quot;) self.title = plt.title(&#39;time = {:&gt;4.0f}&#39;.format(0)) plt.xticks([]) plt.yticks([]) self.im.figure.canvas.draw() def draw_update(self, a, t): u = a[0] self.title.set_text(&#39;time = {:&gt;4.0f}&#39;.format(t)) self.im.set_data(u.real) self.im.figure.canvas.draw() def save_frame(self, i): fname = &quot;_tmp{:05d}.png&quot;.format(i) self.frame_names.append(fname) self.fig.savefig(fname, bbox_inches=&#39;tight&#39;, dpi=300) def periodic_laplacian(self, u, dx=1): &quot;&quot;&quot;Return finite difference Laplacian approximation of 2d array. Uses periodic boundary conditions and a 2nd order approximation.&quot;&quot;&quot; laplacian = (np.roll(u, -1, axis=0) + np.roll(u, +1, axis=0) + np.roll(u, -1, axis=1) + np.roll(u, +1, axis=1) - 4.0 * u) / (dx ** 2) return laplacian def random_ic(self): return 0.5 * (np.random.random((2, self.n[0], self.n[1])) - 0.5) def blocks_ic(self): a = np.ones((2, self.n[0], self.n[1])) a[0] = 0.534522 a[1] = 0.381802 n = self.n p = n[0] / 8 a[0][3 * p - 4:3 * p + 4, 5 * p - 4:5 * p + 4] = -0.534522 a[0][6 * p - 4:6 * p + 4, 3 * p - 4:3 * p + 4] = -0.534522 return a def squiggle_ic(self): a = np.ones((2, self.n[0], self.n[1])) l = self.l uplus = 0.534522 vplus = 0.381802 uminus = -uplus X, Y = np.meshgrid(np.linspace(0, self.l[0], self.n[0]), np.linspace(0, self.l[0], self.n[0])) cos_term = 0.05 * l[0] * np.sin(10 * (2 * np.pi) * Y / l[1] + np.pi * 0.3) exp_term = np.exp(-((Y - l[1] / 2) / (0.1 * l[1])) ** 2) width = 0.05 * l[0] Z = np.exp(-((X - l[0] / 2 + cos_term * exp_term) / width) ** 2) a[0] = uplus a[1] = vplus a[0][Z &gt; 0.8] = uminus return a . run simulation, save snapshots . plt.ion() plt.clf() foo = FitzHughNagumo() foo.fig = plt.figure(1) ax = foo.fig.add_subplot(111) a = foo.squiggle_ic() mult_a = foo.spectral_multiplier() fft_a = np.fft.fftn(a, axes=(1, 2)) t = foo.start foo.draw(a, t) foo.frame_names = [] foo.save_frame(0) for i, tout in enumerate(np.arange(foo.start + foo.step, foo.finish + foo.step, foo.step)): while t &lt; tout: if foo.integration_type == &quot;spectral&quot;: rhs_a = foo.rhs_reaction(a) fft_a = mult_a * (fft_a + foo.dt * np.fft.fftn(rhs_a, axes=(1, 2))) a = np.fft.ifftn(fft_a, axes=(1, 2)) if foo.integration_type == &quot;euler&quot;: a = a + foo.dt * foo.rhs_euler(a) t += foo.dt foo.draw_update(a, t) foo.save_frame(i + 1) . make movie, delete snapshots . fps = 24 frames = &quot;_tmp%5d.png&quot; movie_command = &quot;ffmpeg -y -r {:} -i {:} fhn.mp4&quot;.format(fps, frames) os.system(movie_command) for fname in foo.frame_names: os.remove(fname) . The semi-spectral method . The explanation below was taken from my thesis: &quot;Pattern Formation in Spatially Forced Systems: Application to Vegetation Restoration&quot;. . The semi-spectral method is extremely useful when working with reaction-diffusion systems, and with parabolic PDEs in general. This was the method used to run all the simulations of the Swift-Hohenberg model in this thesis, and it proved to be reliable and fast. The explanation below is a summary of &quot;Spectral algorithms for reaction-diffusion equations&quot;, by Richard V. Craster and Roberto Sassi, with a step by step recipe, so the reader can easily apply the method to any suitable problem. . the method . The semi-spectral transform method is very useful when we have to integrate a system that evolves really slowly. Let us say we have a (parabolic) system of the form: $$ begin{equation*} u_t= epsilon u + f(u)+D nabla^2u, label{eq:1} tag{1} end{equation*} $$ . where $f(u)$ is a nonlinear function. First, we compute the Fourier transform of eqref{eq:1}: $$ begin{equation*} hat{u}_t= epsilon hat{u} + hat{f}(u)-k^2D hat{u}, label{eq:2} tag{2} end{equation*} $$ where the hat denotes the Fourier transform. . We rearrange eqref{eq:2} in the following way: $$ begin{equation*} hat{u}_t+a hat{u}= hat{f}(u), label{eq:3} tag{3} end{equation*} $$ where $a=- epsilon +k^2D$, and now we make a variable substitution $$ begin{align*} hat{v}(k,t)&amp;= ; hat{u}(k,t) ,e^{at} label{eq:4a} tag{4a} hat{v}_t&amp;= ; hat{u}_te^{at}+a hat{u} ,e^{at}. label{eq:4b} tag{4b} end{align*} $$ . We multiply eqref{eq:3} by $e^{at}$ and we finally get $$ begin{equation*} hat{v}_t=e^{at} hat{f}(u). label{eq:5} tag{5} end{equation*} $$ . We can now advance $ hat{v}$ in time using a simple Euler step $$ begin{equation*} hat{v}^{t_{n+1}}= hat{v}^{t_n}+ Delta t left( e^{at_n} hat{f}(u) right). label{eq:6} tag{6} end{equation*} $$ . What we really want is $ hat{u}$, which, according to eqref{eq:4a}, is given by . $$ begin{align*} displaystyle hat{u}^{t_{n+1}}=&amp; ; hat{v}^{t_{n+1}}e^{-at_{n+1}} label{eq:7a} tag{7a} =&amp; ; hat{v}^{t_{n+1}}e^{-at_{n}}e^{-a Delta t} label{eq:7b} tag{7b} =&amp; ; left( hat{v}^{t_n}+ Delta t ; e^{a t_n} hat{f}(u) right)e^{-at_{n}}e^{-a Delta t} label{eq:7c} tag{7c} =&amp; ; left( hat{v}^{t_n}e^{-at_{n}}+ Delta t ;{e^{a t_n}} hat{f}(u) {e^{-at_{n}}} right)e^{-a Delta t} label{eq:7d} tag{7d} =&amp; ; left( hat{u}^{t_n}+ Delta t hat{f}(u) right)e^{-a Delta t} label{eq:7e} tag{7e}. end{align*} $$There is actually no need to use the variable substitution in eqref{eq:4a}. We now have an expression for $ hat{u}^{t_{n+1}}$: $$ begin{equation*} hat{u}^{t_{n+1}}= left( hat{u}^{t_n}+ Delta t hat{f}(u) right)e^{-a Delta t}. label{eq:8} tag{8} end{equation*} $$ . Now it is time to go back from the Fourier space to the real space, and for that we use an inverse Fourier transform $$ u^{t_{n+1}}= mathcal{F}^{-1}[ hat{u}^{t_{n+1}}]. label{eq:9} tag{9} $$ . step by step . To implement this technique, one just has to follow the steps below: . Calculate the Fourier transform of $u$: $ hat{u}= mathcal{F}[u]$. | Have $f(u)$ calculated and then take its Fourier transform: $ hat{f}(u)= mathcal{F}[f(u)]$. | For a given lattice with $N$ points, and $ delta x$ being the distance between them, make the frequency bin vector (matrix) $k$ for your one (two) dimensional system. In python the command would benumpy.fft.fftfreq(N, dx). . The frequency bin vector $k$ looks like: | . $$ begin{align} k&amp;=2 pi cdot left[ 0,1, cdots, tfrac{N}{2}-1,- tfrac{N}{2}, cdots,-1 right]/(N , delta x), qquad mbox{if N is even;} label{eq:10a} tag{10a} k&amp;=2 pi cdot left[ 0,1, cdots, tfrac{N-1}{2},- tfrac{N-1}{2}, cdots,-1 right]/(N , delta x), qquad mbox{if N is odd.} label{eq:10b} tag{10b} end{align} $$Remember that the domain size is given by $L=N , delta x$, which means that the denominator in the expressions above can be written simply as $L$. It is clear from that fact that $ delta k$, the tiniest slice of the Fourier space is $ delta k=2 pi/L$. Corollary: if you want to divide the Fourier space into very many parts, simply have a huge domain. If the system is two-dimensional, then have $k_x$ and $k_y$ calculated separately. The domain might not be square ($L_x neq L_y$), and you might want to divide the domain into a different number of points ($N_x neq N_y$). Anyway, prepare one-dimensional arrays of $k_x$ and $k_y$ as explained above, and then make an outer product of these arrays with a ones array of length $N$, as following: . $$ k_{x,2d} = begin{pmatrix} 1 1 vdots 1 end{pmatrix} begin{pmatrix} k_{x1} &amp; k_{x2} &amp; ... &amp; k_{xN} end{pmatrix} = begin{pmatrix} k_{x1} &amp; k_{x2} &amp; ... &amp; k_{xN} k_{x1} &amp; k_{x2} &amp; ... &amp; k_{xN} &amp; vdots &amp; &amp; k_{x1} &amp; k_{x2} &amp; ... &amp; k_{xN} end{pmatrix} label{eq:11} tag{11} $$and . $$ k_{y,2d} = begin{pmatrix} k_{x1} k_{x2} vdots k_{xN} end{pmatrix} begin{pmatrix} 1 &amp; 1 &amp; ... &amp; 1 end{pmatrix} = begin{pmatrix} k_{y1} &amp; k_{y1} &amp; &amp; k_{y1} k_{y2} &amp; k_{y2} &amp; ... &amp; k_{y2} vdots &amp; vdots &amp; &amp; vdots k_{yN} &amp; k_{yN} &amp; &amp; k_{yN} end{pmatrix}. label{eq:12} tag{12} $$Then factor $e^{-a Delta t}$ equals . $$ e^{-a Delta t}= e^{ left[ epsilon-D(k_x^2+k_y^2) right] Delta t}, label{eq:13} tag{13} $$where $k_x^2$ is the element-wise exponentiation of the 2d array $k_{x,2d}$. . Now that we have all the factors we need, we simply calculate $$ hat{u}^{t_{n+1}}= left( hat{u}^{t_n}+ Delta t hat{f}(u) right)e^{ left[ epsilon-D(k_x^2+k_y^2) right] Delta t}. label{eq:14} tag{14} $$ | We finally go back to the real space by applying the inverse Fourier transform: $u^{t_{n+1}}= mathcal{F}^{-1}[ hat{u}^{t_{n+1}}]$. | . example . For the parametrically forced Swift-Hohenberg equation $$ frac{ partial u}{ partial t} = [ epsilon + gamma cos(k_f x)]u - u^3 -( nabla^2+k_0^2)^2 u, label{eq:15} tag{15} $$ we have $$ f(u)= -u^3 + gamma u cos(k_f x), qquad a = epsilon - left(k_0- k_x^2 - k_y^2 right)^2. label{eq:16} tag{16} $$ .",
            "url": "https://yairmau.github.io/website/jupyter/2020/01/01/fhn.html",
            "relUrl": "/jupyter/2020/01/01/fhn.html",
            "date": " â€¢ Jan 1, 2020"
        }
        
    
  
    
        ,"post74": {
            "title": "Double Pendulum",
            "content": ". Introduction . The double pendulum is one of the most famous examples of chaos. Enjoy making your own animations! . The code . comment the lines below if you want interactive mode, i.e., if you want to see the animation in real time. . import matplotlib matplotlib.use(&#39;Agg&#39;) . import matplotlib.pyplot as plt import numpy as np import os from scipy.integrate import ode . define equations of motion and other functions . def equations(t, y, args): &quot;&quot;&quot; the equations for the double pendulum &quot;&quot;&quot; x1 = y[0] # x1 = theta1, angle x2 = y[1] # x2 = theta2, angle p1 = y[2] # p1 = omega1, angular velocity p2 = y[3] # p2 = omega2, angular velocity l1,l2,m1,m2,g = args x1_eq = p1 x2_eq = p2 p1_eq = -((g*(2*m1+m2)*np.sin(x1)+m2*(g*np.sin(x1-2*x2)+2*(l2*p2**2+l1*p1**2*np.cos(x1-x2))*np.sin(x1-x2)))/(2*l1*(m1+m2-m2*(np.cos(x1-x2))**2))) p2_eq = ((l1*(m1+m2)*p1**2+g*(m1+m2)*np.cos(x1)+l2*m2*p2**2*np.cos(x1-x2))*np.sin(x1-x2))/(l2*(m1+m2-m2*(np.cos(x1-x2))**2)) return [x1_eq, x2_eq, p1_eq, p2_eq] def calculate_trajectory(args,time,y0): &quot;&quot;&quot; uses scipy&#39;s ode itegrator to simulate the equations &quot;&quot;&quot; t0,t1,dt = time r = ode(equations).set_integrator(&#39;dopri5&#39;) r.set_initial_value(y0, t0).set_f_params(args) data=[[t0, y0[0], y0[1], y0[2], y0[3] ]] while r.successful() and r.t &lt; t1: r.integrate(r.t+dt) data.append([r.t, r.y[0], r.y[1], r.y[2], r.y[3] ]) return np.array(data) def from_angle_to_xy(args,angles): &quot;&quot;&quot; converts angles into xy positions &quot;&quot;&quot; l1,l2,m1,m2,g = args time,theta1,theta2 = angles.T x1 = l1*np.sin(theta1) y1 = -l1*np.cos(theta1) x2 = l2*np.sin(theta2) + x1 y2 = -l2*np.cos(theta2) + y1 return np.array([time,x1,y1,x2,y2]).T . parameters . l1 = 0.5 # length of arms l2 = 0.5 m1 = 1.0 # mass of the pendulum m2 = 1.0 g = 10.0 # acceleration of gravity args = [l1,l2,m1,m2,g] fps = 80 total_time = 5 # seconds time = [0.0,total_time,1.0/fps] # start, finish, dt ic = [np.pi*0.65, np.pi*1.1, 0.0, 0.0] . here the magic happens . d = calculate_trajectory(args,time,ic) data_TXY = from_angle_to_xy(args,d[:,:3]) . Let&#39;s plot stuff, and make a nice movie. Requrement: ffmpeg . make_movie=True params = {&#39;backend&#39;: &#39;ps&#39;, &#39;font.size&#39;: 20, &#39;font.family&#39;:&#39;serif&#39;, &#39;font.serif&#39;:[&#39;Computer Modern Roman&#39;], # Times, Palatino, New Century Schoolbook, Bookman, Computer Modern Roman &#39;ps.usedistiller&#39;: &#39;xpdf&#39;, &#39;text.usetex&#39;: True, } plt.rcParams.update(params) plt.ion() fig = plt.figure(figsize=(9.6,5.4),dpi=100) # 1920x1080 fig.subplots_adjust(left=0, right=1, top=1, bottom=0,hspace=0.02,wspace=0.02) ax = fig.add_subplot(111) ax.axis(&#39;off&#39;) # no frame def plot_last_seconds(data,index): &quot;&quot;&quot; Plots a line with the trajectory of the tip of pendulum 2 (x2,y2) &quot;&quot;&quot; how_long = 1.0 # seconds n = int(how_long/time[2]) to_plot = data[:index,:] if index &lt; n: prepend = np.tile(data[0],(n-index,1)) to_plot = np.vstack([prepend,to_plot]) index = n colormap = plt.cm.Greys_r colors = [colormap(i) for i in np.linspace(0.0, 1.0, n-1)] plots = [] for j in np.arange(n-1): p, = ax.plot(to_plot[index-j-1:index-j+1,3],to_plot[index-j-1:index-j+1,4], color=colors[j], zorder=-1) plots.append(p) return plots # &quot;plot&quot; returns a tuple of line objects, thus the comma t,x1,y1,x2,y2 = data_TXY[0] line1, = ax.plot([0.0,x1], [0.0,y1], &#39;r-&#39;) line2, = ax.plot([x1,x2], [y1,y2], &#39;r-&#39;) circ1, = ax.plot([x1], [y1], &#39;ro&#39;,markersize=10) circ2, = ax.plot([x2], [y2], &#39;ro&#39;,markersize=10) sizeY = 1.2 ax.axis([-sizeY*16/9,sizeY*16/9,-sizeY,sizeY]) frame_names = [] tex=ax.text(0.0,0.85,&#39;&#39;,ha=&quot;center&quot;) for i,v in enumerate(data_TXY): t,x1,y1,x2,y2 = v # print(&quot;t={:.2f}&quot;.format(t)) # you might want to know how things are going... line1.set_data([0.0,x1],[0.0,y1]) line2.set_data([x1,x2],[y1,y2]) circ1.set_data([x1],[y1]) circ2.set_data([x2],[y2]) # plot_last_seconds considerably slows down the simulation, # but makes it much prettier... pls = plot_last_seconds(data_TXY,i+1) tex.set_text(r&quot;$t={:.3f}$ s&quot;.format(t)) fig.canvas.draw() if make_movie: fname = &quot;_tmp{:05d}.png&quot;.format(i) frame_names.append(fname) fig.savefig(fname,bbox_inches=&#39;tight&#39;) for k in pls: k.remove() if make_movie: frames = &quot;_tmp%5d.png&quot; frames = &quot;_tmp%5d.png&quot; movie_command = &quot;ffmpeg -y -r {:} -i {:} double.mp4&quot;.format(fps,frames) os.system(movie_command) for fname in frame_names: os.remove(fname) .",
            "url": "https://yairmau.github.io/website/jupyter/2020/01/01/doublependulum.html",
            "relUrl": "/jupyter/2020/01/01/doublependulum.html",
            "date": " â€¢ Jan 1, 2020"
        }
        
    
  
    
        ,"post75": {
            "title": "Contour plots",
            "content": ". Introduction . Contour plots are great to show how a variable depends on two parameters. . The code . import numpy as np import matplotlib.pyplot as plt import matplotlib from IPython.display import Math # %matplotlib inline . configure plotting preferences . # this is a latex constant, don&#39;t change it. pts_per_inch = 72.27 # write &quot; the textwidth&quot; (or &quot; showthe columnwidth&quot; for a 2 collumn text) text_width_in_pts = 246.0 # inside a figure environment in latex, the result will be on the # dvi/pdf next to the figure. See url above. text_width_in_inches = text_width_in_pts / pts_per_inch # make rectangles with a nice proportion golden_ratio = 0.618 # figure.png or figure.eps will be intentionally larger, because it is prettier inverse_latex_scale = 2 # when compiling latex code, use # includegraphics[scale=(1/inverse_latex_scale)]{figure} # we want the figure to occupy 2/3 (for example) of the text width fig_proportion = (3.0 / 3.0) csize = inverse_latex_scale * fig_proportion * text_width_in_inches # always 1.0 on the first argument fig_size = (1.0 * csize,golden_ratio * csize) # find out the fontsize of your latex text, and put it here text_size = inverse_latex_scale * 12 tick_size = inverse_latex_scale * 8 # learn how to configure: # http://matplotlib.sourceforge.net/users/customizing.html params = {&#39;backend&#39;: &#39;ps&#39;, &#39;axes.labelsize&#39;: text_size, &#39;legend.fontsize&#39;: tick_size, &#39;legend.handlelength&#39;: 2.5, &#39;legend.borderaxespad&#39;: 0, &#39;xtick.labelsize&#39;: tick_size, &#39;ytick.labelsize&#39;: tick_size, &#39;font.family&#39;: &#39;serif&#39;, &#39;font.size&#39;: text_size, # Times, Palatino, New Century Schoolbook, # Bookman, Computer Modern Roman &#39;font.serif&#39;: [&#39;Times&#39;], &#39;ps.usedistiller&#39;: &#39;xpdf&#39;, &#39;text.usetex&#39;: True, &#39;figure.figsize&#39;: fig_size, # include here any neede package for latex &#39;text.latex.preamble&#39;: [r&#39; usepackage{amsmath}&#39;, ], } plt.rcParams.update(params) plt.ioff() plt.clf() # figsize accepts only inches. fig = plt.figure(1, figsize=fig_size) fig.subplots_adjust(left=0.12, right=0.96, top=0.96, bottom=0.18, hspace=0.02, wspace=0.02) ax = fig.add_subplot(111) . def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=256): new_cmap = matplotlib.colors.LinearSegmentedColormap.from_list( &#39;trunc({n},{a:.2f},{b:.2f})&#39;.format(n=cmap.name, a=minval, b=maxval), cmap(np.linspace(minval, maxval, n))) return new_cmap cmap = plt.get_cmap(&#39;YlOrBr&#39;) my_cmap = truncate_colormap(cmap, 0.2, 0.9) . minX = 0 maxX = 10 minY = 0 maxY = 3 N = 50j y, x = np.mgrid[minY:maxY:N, minX:maxX:N] z = 2 * np.exp(-(0.02 * (x + 1) ** 2 + 0.05 * (y - 3.1) **2 )) divisions = np.arange(0.3, 2.1, 0.3) divisions2 = np.append(divisions, 2.5) divisions2 = np.append(-0.5, divisions2) # contour filled with colors ax.contourf(x, y, z, divisions2, cmap=my_cmap, vmin=0.0,vmax=2.0) # contour lines cont = ax.contour(x, y, z, divisions, colors=2 * [&#39;black&#39;] + [&#39;green&#39;] + 3 * [&#39;black&#39;], linewidth=.5) zcontour = cont.collections[1] dash1=(15, 10, 15, 10) zcontour.set_dashes([(0, dash1)]) zcontour = cont.collections[4] dash2=(20, 30) zcontour.set(color=&#39;blue&#39;, linestyle=[(0, dash2)], linewidth=4) # labels manual_locations = [(1.0, 2.5), (2.5, 2.5), (3.8, 2.5), (5.0, 2.5), (6.2, 2.5), (8.3, 2.5)] ax.clabel(cont, inline=1, fontsize=tick_size, fmt=&#39;z=%.2f%%&#39;, manual=manual_locations, colors=5 * [&#39;black&#39;] + [&#39;white&#39;]) ax.set_xlabel(r&quot;$x$ axis&quot;) ax.set_ylabel(r&quot;$y$ axis&quot;) fig . /Users/yairmau/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: The following kwargs were not used by contour: &#39;linewidth&#39; . %matplotlib notebook fig.savefig(&quot;./python_figures/contours.png&quot;,dpi=300) # fig.savefig(&quot;cont.eps&quot;) .",
            "url": "https://yairmau.github.io/website/jupyter/2020/01/01/contours.html",
            "relUrl": "/jupyter/2020/01/01/contours.html",
            "date": " â€¢ Jan 1, 2020"
        }
        
    
  
    
        ,"post76": {
            "title": "What's the best path to save someone from drowning?",
            "content": ". Introduction . Snell&#39;s law of refraction can be understood in this example, where the lifeguard wants to minimize the time it takes to get to the drowning person. . Code . import matplotlib import matplotlib.pyplot as plt import numpy as np . # this is a latex constant, don&#39;t change it. pts_per_inch = 72.27 # write &quot; the textwidth&quot; (or &quot; showthe columnwidth&quot; for a 2 collumn text) text_width_in_pts = 252.0 # inside a figure environment in latex, the result will be on the # dvi/pdf next to the figure. See url above. text_width_in_inches = text_width_in_pts / pts_per_inch # make rectangles with a nice proportion golden_ratio = 0.618 # figure.png or figure.eps will be intentionally larger, because it is prettier inverse_latex_scale = 2 # when compiling latex code, use # includegraphics[scale=(1/inverse_latex_scale)]{figure} # we want the figure to occupy 2/3 (for example) of the text width fig_proportion = (3.0 / 3.0) csize = inverse_latex_scale * fig_proportion * text_width_in_inches # always 1.0 on the first argument fig_size = (1.0 * csize,golden_ratio * csize) # find out the fontsize of your latex text, and put it here text_size = inverse_latex_scale * 12 tick_size = inverse_latex_scale * 8 # learn how to configure: # http://matplotlib.sourceforge.net/users/customizing.html params = {&#39;axes.labelsize&#39;: text_size, &#39;legend.fontsize&#39;: tick_size, &#39;legend.handlelength&#39;: 2.5, &#39;legend.borderaxespad&#39;: 0, &#39;xtick.labelsize&#39;: tick_size, &#39;ytick.labelsize&#39;: tick_size, &#39;font.family&#39;: &#39;serif&#39;, &#39;font.size&#39;: text_size, # Times, Palatino, New Century Schoolbook, # Bookman, Computer Modern Roman # &#39;font.serif&#39;: [&#39;Times&#39;], # &#39;ps.usedistiller&#39;: &#39;xpdf&#39;, &#39;text.usetex&#39;: True, &#39;figure.figsize&#39;: fig_size, # include here any neede package for latex &#39;text.latex.preamble&#39;: [r&#39; usepackage{amsmath}&#39;, ], } plt.rcParams.update(params) # plt.clf() # figsize accepts only inches. plt.ioff() fig = plt.figure(1, figsize=fig_size) fig.subplots_adjust(left=0.0, right=1.0, top=1.0, bottom=0.0, hspace=0.02, wspace=0.02) ax = fig.add_subplot(111) . origin = [0, 0] lifeguard = [-3, -2] drowning = [2, 3] ax.set_xticks([]) ax.set_yticks([]) xlim = [-4, 4] ylim = [-4, 4] ax.axis([xlim[0], xlim[1], ylim[0], ylim[1]]) ##### drowning ##### # line ax.plot([origin[0], drowning[0]], [origin[1], drowning[1]], color=&quot;black&quot;, lw=2) # diamond ax.plot(drowning[0], drowning[1], &quot;D&quot;, markerfacecolor=&quot;black&quot;, markersize=10, markeredgewidth=3, color=&quot;black&quot;) # explanation ax.text(drowning[0], drowning[1] - 0.3, r&quot;drowning&quot;, verticalalignment=&quot;top&quot;) ax.text(drowning[0], drowning[1] - 0.8, r&quot;person&quot;, verticalalignment=&quot;top&quot;) ##### lifeguard ##### # line ax.plot([origin[0], lifeguard[0]], [origin[1], lifeguard[1]], color=&quot;black&quot;, lw=2) # circle ax.plot(lifeguard[0], lifeguard[1], &quot;o&quot;, markerfacecolor=&quot;black&quot;, markersize=10, markeredgewidth=3, color=&quot;black&quot;) # explanation ax.text(lifeguard[0], lifeguard[1] - 0.3, r&quot;lifeguard&quot;, verticalalignment=&quot;top&quot;) . Text(-3, -2.3, &#39;lifeguard&#39;) . background colors . sand = matplotlib.patches.Rectangle([xlim[0], ylim[0]], (xlim[1] - xlim[0]), (ylim[1] - ylim[0]) / 2.0, color=&quot;yellow&quot;, alpha=0.6) ax.add_patch(sand) sea = matplotlib.patches.Rectangle([xlim[0], 0], (xlim[1] - xlim[0]), (ylim[1] - ylim[0]) / 2.0, color=&quot;blue&quot;, alpha=0.4) ax.add_patch(sea) ###### sand ##### ax.text(0.95, 0.05, r&quot;(1) sand&quot;, transform=ax.transAxes, horizontalalignment=&#39;right&#39;) ###### sea ##### ax.text(0.95, 0.50, r&quot;(2) sea&quot;, transform=ax.transAxes, horizontalalignment=&#39;right&#39;, verticalalignment=&quot;bottom&quot;) . Text(0.95, 0.5, &#39;(2) sea&#39;) . spines through origin . ax.spines[&#39;left&#39;].set_position(&#39;zero&#39;) ax.spines[&#39;right&#39;].set_color(&#39;none&#39;) ax.spines[&#39;bottom&#39;].set_position(&#39;zero&#39;) ax.spines[&#39;top&#39;].set_color(&#39;none&#39;) ax.spines[&#39;left&#39;].set_smart_bounds(False) ax.spines[&#39;bottom&#39;].set_smart_bounds(False) # ax.xaxis.set_ticks_position(&#39;bottom&#39;) # ax.yaxis.set_ticks_position(&#39;left&#39;) ax.set_xticks([]) ax.set_yticks([]) . [] . annotations . ax.annotate(&quot;&quot;, xy=(lifeguard[0] - 0.3, lifeguard[1]), xycoords=&#39;data&#39;, xytext=(lifeguard[0] - 0.3, 0), textcoords=&#39;data&#39;, size=tick_size, arrowprops=dict(arrowstyle=&quot;&lt;-&gt;&quot;, connectionstyle=&quot;arc3&quot;), ) ax.text(lifeguard[0] - 0.2, lifeguard[1] / 2.0, r&quot;$h_1$&quot;, verticalalignment=&quot;center&quot;, horizontalalignment=&quot;left&quot;) # h_2 ax.annotate(&quot;&quot;, xy=(lifeguard[0] - 0.3, drowning[1]), xycoords=&#39;data&#39;, xytext=(lifeguard[0] - 0.3, 0), textcoords=&#39;data&#39;, size=tick_size, arrowprops=dict(arrowstyle=&quot;&lt;-&gt;&quot;, connectionstyle=&quot;arc3&quot;), ) ax.text(lifeguard[0] - 0.2, drowning[1] / 2.0, r&quot;$h_2$&quot;, verticalalignment=&quot;center&quot;, horizontalalignment=&quot;left&quot;) # L ax.annotate(&quot;&quot;, xy=(lifeguard[0], drowning[1] + 0.3), xycoords=&#39;data&#39;, xytext=(drowning[0], drowning[1] + 0.3), textcoords=&#39;data&#39;, size=tick_size, arrowprops=dict(arrowstyle=&quot;&lt;-&gt;&quot;, shrinkA=0, shrinkB=0, connectionstyle=&quot;arc3&quot;), ) ax.text((lifeguard[1] - lifeguard[0]) / 2.0, drowning[1] + 0.3, r&quot;$L$&quot;, verticalalignment=&quot;bottom&quot;, horizontalalignment=&quot;left&quot;) # l1 ax.text(lifeguard[0] / 2.0, 1.10 * lifeguard[1] / 2.0, r&quot;$ ell_1$&quot;, verticalalignment=&quot;top&quot;, horizontalalignment=&quot;left&quot;) # l2 ax.text(drowning[0] / 2.0, 0.95 * drowning[1] / 2.0, r&quot;$ ell_2$&quot;, verticalalignment=&quot;top&quot;, horizontalalignment=&quot;left&quot;) # theta_1 ax.annotate(&quot;&quot;, xy=(0, 0.5 * lifeguard[1]), xycoords=&#39;data&#39;, xytext=(0.2 * lifeguard[0], 0.2 * lifeguard[1]), textcoords=&#39;data&#39;, size=tick_size, arrowprops=dict(arrowstyle=&quot;-&quot;, lw=2, connectionstyle=&quot;angle3,angleA=-60,angleB=0&quot;), ) ax.text(0.1 * lifeguard[0], 0.5 * lifeguard[1], r&quot;$ theta_1$&quot;, verticalalignment=&quot;top&quot;, horizontalalignment=&quot;right&quot;) # theta_2 ax.annotate(&quot;&quot;, xy=(0, 0.3 * drowning[1]), xycoords=&#39;data&#39;, xytext=(0.2 * drowning[0], 0.2 * drowning[1]), textcoords=&#39;data&#39;, size=tick_size, arrowprops=dict(arrowstyle=&quot;-&quot;, lw=2, connectionstyle=&quot;angle3,angleA=120,angleB=0&quot;), ) ax.text(0.1 * drowning[0], 0.5 * drowning[1], r&quot;$ theta_2$&quot;, verticalalignment=&quot;top&quot;, horizontalalignment=&quot;left&quot;) . Text(0.2, 1.5, &#39;$ theta_2$&#39;) . %matplotlib inline fig.savefig(&quot;python_figures/bestpath.png&quot;, dpi=300) fig .",
            "url": "https://yairmau.github.io/website/jupyter/2020/01/01/bestpath.html",
            "relUrl": "/jupyter/2020/01/01/bestpath.html",
            "date": " â€¢ Jan 1, 2020"
        }
        
    
  
    
        ,"post77": {
            "title": "Bar plot",
            "content": ". Introduction . This code produces the figure above. Here we showcase the use of unicode text. . The code . from __future__ import unicode_literals import numpy as np import matplotlib import matplotlib.pyplot as plt from bidi import algorithm as bidialg # needed for arabic, hebrew import arabic_reshaper # needed for arabic . ModuleNotFoundError Traceback (most recent call last) &lt;ipython-input-2-aaa74850711b&gt; in &lt;module&gt; 4 import matplotlib.pyplot as plt 5 from bidi import algorithm as bidialg # needed for arabic, hebrew -&gt; 6 import arabic_reshaper # needed for arabic ModuleNotFoundError: No module named &#39;arabic_reshaper&#39; . pts_per_inch = 72.27 # this is a latex constant, don&#39;t change it. text_width_in_pts = 300.0 # write &quot; the textwidth&quot; (or &quot; showthe columnwidth&quot; for a 2 collumn text) # inside a figure environment in latex, the result will be on the dvi/pdf next to the figure. See url above. text_width_in_inches=text_width_in_pts/pts_per_inch golden_ratio = 0.618 # make rectangles with a nice proportion inverse_latex_scale = 2 # figure.png or figure.eps will be intentionally larger, because it is prettier # when compiling latex code, use includegraphics[scale=(1/inverse_latex_scale)]{figure} fig_proportion = (3.0 / 3.0) # we want the figure to occupy 2/3 (for example) of the text width csize = inverse_latex_scale * fig_proportion * text_width_in_inches fig_size = (1 * csize, 1.3 * csize) # always 1.0 on the first argument text_size = inverse_latex_scale * 10 # find out the fontsize of your latex text, and put it here label_size = inverse_latex_scale * 10 tick_size = inverse_latex_scale * 8 # learn how to configure: # http://matplotlib.sourceforge.net/users/customizing.html params = {&#39;backend&#39;: &#39;ps&#39;, &#39;axes.labelsize&#39;: 16, &#39;legend.fontsize&#39;: 14, &#39;legend.handlelength&#39;: 2.5, &#39;legend.borderaxespad&#39;: 0, &#39;axes.labelsize&#39;: label_size, &#39;xtick.labelsize&#39;: tick_size, &#39;ytick.labelsize&#39;: tick_size, # &#39;font.family&#39;: &#39;serif&#39;, &#39;font.size&#39;: text_size, # &#39;font.serif&#39;: [&#39;Computer Modern Roman&#39;], &#39;ps.usedistiller&#39;: &#39;xpdf&#39;, # &#39;text.usetex&#39;: True, &#39;figure.figsize&#39;: fig_size, #&#39;text.latex.unicode&#39;: True, } plt.rcParams.update(params) plt.ioff() fig = plt.figure(1, figsize=fig_size) # figsize accepts only inches. fig.clf() dpi = 100 pixel_size = (700,700) fig_size = (pixel_size[0]/dpi,pixel_size[1]/dpi) ax1=fig.add_subplot(211) ax2=fig.add_subplot(212) fig.subplots_adjust(left=0.30, right=0.97, top=0.95, bottom=0.06, hspace=0.2, wspace=0.1) . words = [u&#39;FuÃŸball Ã…ngstrÃ¶m nsÃ¸ster Ğ Ğ¾ÑÑĞ¸Ñ&#39;, u&#39;franÃ§ais maÃ®tre nvoilÃ  Ã©goÃ¯ste&#39;, u&#39;EspaÃ±a&#39;, u&#39;Ä°stanbul aÄŸzÄ±&#39;, u&#39;Anything Unicode&#39; ] values1 = [2575, 5851, 3191, 2303, 3029] values2 = [4813, 5219, 5505, 6229, 6961] values1 = np.array(values1) values2 = np.array(values2) width = 0.35 # the width of the bars r = np.arange(len(values1)) . ax1 , horizontal bars . v1 = ax1.barh(r, values1, width, color=&#39;pink&#39;) v2 = ax1.barh(r + width, values2, width, color=&#39;brown&#39;) ax1.axis([0, 8600, r.min() - 0.3, r.max() + 1]) ax1.set_yticks(r) ax1.set_yticks(r + 1 * width) ax1.set_yticklabels(words) xt = np.arange(0, 8100, 1000) ax1.set_xticks(xt) ax1.set_xticklabels(xt) ax1.set_xlabel(u&#39;the values&#39;, fontsize=16) ax1.set_title(u&#39;Title here&#39;, fontsize=18) ax1.xaxis.grid(True) ax1.tick_params( axis=&#39;y&#39;, # changes apply to the y-axis which=&#39;both&#39;, # both major and minor ticks are affected left=&#39;off&#39;, # ticks along the left edge are off right=&#39;off&#39;, # ticks along the right edge are off labelleft=&#39;on&#39;) # labels along the bottom edge are on ax1.legend((v1, v2), (u&#39;2016&#39;, u&#39;2015&#39;), loc=(0.74,0.05)) def autolabel_hor(rects,ax, offset_x, offset_y): # attach some text labels at the tip of the bars for i,rect in enumerate(rects): width = rect.get_width() height = rect.get_height() ax.text(width + offset_x, rect.get_y() + offset_y * height, &#39;%d&#39; % int(width), ha=&#39;left&#39;, va=&#39;bottom&#39;, fontsize=14) autolabel_hor(v1, ax1, 100.0, -0.20) autolabel_hor(v2, ax1, 100.0, -0.10) . ax2, vertical bars . from bidi import algorithm as bidialg w1 = ax2.bar(r, values1, width, color=&#39;pink&#39;) w2 = ax2.bar(r + width, values2, width, color=&#39;brown&#39;) ax2.axis([r.min() - 0.3, r.max() + 1, 0, 8600]) ax2.set_xticks(r) ax2.set_xticks(r + 1 * width) shalom = bidialg.get_display(u&#39;×©×œ×•×&#39;) salam = bidialg.get_display(arabic_reshaper.reshape(u&#39;Ø³Ù„Ø§Ù…&#39;)) ax2.set_xticklabels([shalom, salam, &#39;ccc&#39;, &#39;ddd&#39;, &#39;eee&#39;]) xt = np.arange(0, 8200, 1000) ax2.set_yticks(xt) ax2.set_yticklabels(xt) ax2.yaxis.grid(True) ax2.tick_params( axis=&#39;x&#39;, # changes apply to the x-axis which=&#39;both&#39;, # both major and minor ticks are affected top=&#39;off&#39;, # ticks along the top edge are off bottom=&#39;off&#39;, # ticks along the bottom edge are off labelbottom=&#39;on&#39;) # labels along the bottom edge are on ax2.legend((w1, w2), (u&#39;2016&#39;, u&#39;2015&#39;), loc=&quot;upper center&quot;) def autolabel_ver(rects,ax, offset_x, offset_y): # attach some text labels at the tip of the bars for i,rect in enumerate(rects): width = rect.get_width() height = rect.get_height() ax.text(rect.get_x() + offset_x * width, height + offset_y, &#39;%d&#39; % int(height), ha=&#39;left&#39;, va=&#39;bottom&#39;, fontsize=14) autolabel_ver(w1, ax2, -0.3, 100.0) autolabel_ver(w2, ax2, 0., 100.0) . %matplotlib inline fig.savefig(&quot;./python_figures/bars.png&quot;) plt.show() fig .",
            "url": "https://yairmau.github.io/website/jupyter/2020/01/01/bars.html",
            "relUrl": "/jupyter/2020/01/01/bars.html",
            "date": " â€¢ Jan 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.Â &#8617; . |",
          "url": "https://yairmau.github.io/website/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "More",
          "content": "Books . Artigos . Five-star Content . import packages . import numpy as np import matplotlib.pyplot as plt import os from matplotlib import rcParams rcParams[&#39;font.family&#39;] = &#39;monospace&#39; . Text here with code, also some python code import numpy as np .",
          "url": "https://yairmau.github.io/website/more/",
          "relUrl": "/more/",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "Artigos",
          "content": "",
          "url": "https://yairmau.github.io/website/more/artigos/",
          "relUrl": "/more/artigos/",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "Five-Star Content I Recommend",
          "content": "Books . [Check out my Goodreads profile]. . Superintelligence: Paths, Dangers, Strategies, by Nick Bostrom. | The Unfolding of Language: An Evolutionary Tour of Mankindâ€™s Greatest Invention, by Guy Deutscher. | . Blogs . Wait but why, by Tim Urban. | Fuck Yeah Fluid Dynamics, by Nicole Sharp. | . Short stories . Theyâ€™re made out of meat, by Terry Bisson. | The Sentinel, by Arthur C. Clarke. | On Exactitude in Science, by Jorge Luis Borges. | The Feeling Of Power , by Isaac Asimov. | . Technical stuff . Linux Shell Scripting Tutorial | Pythonic Perambulations, by Jake VanderPlas. | . YouTube channels . 3Blue1Brown, minute physics, Veritasium, Smarter Every Day, CGP Grey, Kurzgesagt, Primitive Technology, Vsauce, Nerdwriter, Mathologer, Sciencium, Steve Mould . Podcasts . Hardcore History with Dan Carlin, especially the series â€œBlueprint for Armageddonâ€ and â€œWrath of the Khansâ€. | 99% Invisible | Revisionist History | Lexicon Valley | Making Sense, with Sam Harris | .",
          "url": "https://yairmau.github.io/website/more/five-stars/",
          "relUrl": "/more/five-stars/",
          "date": ""
      }
      
  

  
      ,"page5": {
          "title": "Books",
          "content": "my read shelf:&lt;a href=â€https://www.goodreads.com/review/list/52721996?shelf=readâ€ title=â€Yair Mauâ€™s book recommendations, liked quotes, book clubs, book trivia, book lists (read shelf)â€, rel=â€nofollowâ€&gt;&lt;/a&gt; .",
          "url": "https://yairmau.github.io/website/more/books/",
          "relUrl": "/more/books/",
          "date": ""
      }
      
  

  
      ,"page6": {
          "title": "Art",
          "content": "",
          "url": "https://yairmau.github.io/website/blog/art/",
          "relUrl": "/blog/art/",
          "date": ""
      }
      
  

  
      ,"page7": {
          "title": "",
          "content": "â€ƒThis is my personal website. â€ƒI put here a lot of random stuff. â€ƒUse the menu above to navigate. . . â€ƒ Click here if you are looking for my professional website. . . . Piet Hein Problems worthy â€ƒâ€ƒof attack prove their worth â€ƒâ€ƒby hitting back. .",
          "url": "https://yairmau.github.io/website/",
          "relUrl": "/",
          "date": ""
      }
      
  

  
      ,"page8": {
          "title": "Euclid's Elements",
          "content": "",
          "url": "https://yairmau.github.io/website/blog/euclid/",
          "relUrl": "/blog/euclid/",
          "date": ""
      }
      
  

  
      ,"page9": {
          "title": "Blog",
          "content": "Geek thoughts . This is where I write about random geek/math/puzzles stuff . Art . Art, origami, mathematical constructions, music, random projects . Euclidâ€™s Elements . Some of Euclidâ€™s Elements propositions, heavily inspired by Oliver Byrne. . . . . Selected blogposts . Here are a few blogposts that you might find interesting .",
          "url": "https://yairmau.github.io/website/blog/",
          "relUrl": "/blog/",
          "date": ""
      }
      
  

  
      ,"page10": {
          "title": "Geek stuff",
          "content": "",
          "url": "https://yairmau.github.io/website/blog/geek/",
          "relUrl": "/blog/geek/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
      ,"page17": {
          "title": "",
          "content": "Sitemap: {{ â€œsitemap.xmlâ€ | absolute_url }} | .",
          "url": "https://yairmau.github.io/website/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}