{
  
    
        "post0": {
            "title": "Doubling Time",
            "content": "Suppose you have a process that can be described by exponential growth. It could be anything: interests on an investment, the early phases of infection in a pandemic, whatever. . It is often convenient to have an idea how fast is the growth by answering the question: . How long will it take for $x$ to double in size, given a growth of $n$% per year? . The rule of thumb I learned a while back is the following: . Doubling time = $ displaystyle frac{70}{n}$ (in years) . Of course, the time unit could be anything you like, I‚Äôll deal here with years for simplicity‚Äôs sake. Specifically, let‚Äôs answer the question: . Israel has currently (2021) a population of 9.2 million, and a growth rate of 1.8% per year. How long will it take for the population to double, assuming a fixed growth rate? . The answer is about 39 years (70 divided by 1.8), but why?! . Let‚Äôs call $x_0$ the population size now, and the growth rate $n$%. After one year, the population will be . $ displaystyle x_1 = x_0 * left( 1 + frac{n}{100} right) $ . Assume that after $k$ years the population will be double, i.e.: . $ displaystyle x_k = x_0 * left( 1 + frac{n}{100} right)^k = 2x_0. $ . Cancelling $x_0$ we get . $ displaystyle left( 1 + frac{n}{100} right)^k = 2. $ . We now take the natural logarith of both sides: . $ displaystyle k ln left( 1 + frac{n}{100} right) = ln(2). $ . Note that we took $k$ out of the exponent and it now multiplies the logarithm on the left-hand side. Multiplying both sides by 100 yields . $ displaystyle100k ln left( 1 + frac{n}{100} right) = 100 ln(2) simeq 69.3. $ . That surely explains the number 70 in the rule of thumb! Because of the properties of logarithms, we put the number 100 as the exponent of the parenthesis: . $ displaystyle k ln left( 1 + frac{n}{100} right)^{100} = 100 ln(2). $ . We are very close to the end! We now remind ourselves that we learned in Calculus the definition of the exponential function: . $ exp(x) = displaystyle lim_{m rightarrow infty} left( 1 + frac{x}{m} right)^{m}. $ . Because the number 100 is ‚Äúquite big‚Äù, we will approximate the parenthesis inside the logarithm with the exponential function, thus . $ k ln exp(n) = 100 ln(2). $ . The logarithm is the inverse function of the exponential, therefore . $ kn = displaystyle100 ln(2). $ . Finally, solving for $k$, we have . $ k = displaystyle frac{100 ln(2)}{n} simeq frac{70}{n}. $ . We have thus shown that the number of years $k$ it will take for Israel to double it‚Äôs population is about $70/n = 70/1.8 = 38.88$ years!! . The exact number, without any approximations, would be . $ k = displaystyle frac{ ln(2)}{ ln(1+n/100)} simeq 38.85. $ . Conclusion: üëç Very impressive rule of thumb üëç .",
            "url": "https://yairmau.github.io/markdown/2021/11/16/doubling-time.html",
            "relUrl": "/markdown/2021/11/16/doubling-time.html",
            "date": " ‚Ä¢ Nov 16, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Parabolic Hyperboloid",
            "content": ". Just like the hyperboloid of one sheet, the Parabolic Hyperboloid is a ruled surface. This means that it can be imagined as the surface one gets by swiping a straight line through space. In the image below, we see that each of the skewers is exactly straight, but together thay make this beautiful curved shape. . . This project is quite easy to make with skewers, see its instructions here. . The equation that defines the surface of the parabolic hyperboloid is . z=Ax2‚àíBy2,z = Ax^2 - By^2,z=Ax2‚àíBy2, . where both $A$ and $B$ are positive numbers. This website allows us to play with the parameters and see how the surface responds. . In my opinion, a static image can‚Äôt convey the beauty of this shape, so I made this gif: . In 2020, Dillon Berger noted on Twitter that the shape of Pringles is a parabolic hyperboloid: . The reason Pringles fit so nicely in a cylindrical tube is because they&#39;re hyperbolic paraboloids plotted over a circular domain pic.twitter.com/BUzjPw7e17 . &mdash; „Äà Berger | Dillon „Äâ (@InertialObservr) February 18, 2020 The company took notice of this (mildly) viral tweet, and sent Dillon a box full of parabolic hyperboloids üòÅ. . Thanks @Pringles, for sending me some of these delicious Hyperbolic Paraboloids! pic.twitter.com/L3WMgqObPM . &mdash; „Äà Berger | Dillon „Äâ (@InertialObservr) March 2, 2020 The lesson here is that it‚Äôs good to know your math üòú. . .",
            "url": "https://yairmau.github.io/markdown/2021/11/13/parabolic-hyperboloid.html",
            "relUrl": "/markdown/2021/11/13/parabolic-hyperboloid.html",
            "date": " ‚Ä¢ Nov 13, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Skewer Hyperboloid",
            "content": ". George Hart is again the hero of this project. Go check out his webpage to see how to build this beautiful shape. All you need to make the skewer hyperboloid is a bunch of skewers and thin elastic bands. . This is an easy project, and the end result is so fun to play with! It is surprising how smoothly this shape morphes when you pull and push the skewers to change the angle, it‚Äôs hard to explain, so you should definitely try it yourself üòÅ. See Annie Perkins‚Äôs video to get a better idea: . The one thing I can add to George Hart‚Äôs page is that after many weeks and months the elastic bands will dry out and break! I gave a skewer hyperboloid to a friend, and one day he arrived at his office and found out that the whole thing had disintegrated overnight, and the skewers were all over the place. I‚Äôm still looking for a suitable substitute to the elastic bands. It must be something flexible that enables us to play with the construction, and at the same time something durable‚Ä¶ I have not found yet the perfect material, if you have a suggestion please write to me. . I wanted my hyperboloid to last for a long time, so after the construction was done, I used super glue to fix the skewers in a particular angle. Now I can‚Äôt play with it, but at least it will not disintegrate overnight in a few months time (hopefully!). . . The mathematical name of this shape is ‚Äúhyperboloid of one sheet‚Äù, and its equation is . x2A2+y2B2‚àíz2C2=0. begin{equation} frac{x^2}{A^2} + frac{y^2}{B^2} - frac{z^2}{C^2} = 0. end{equation}A2x2‚Äã+B2y2‚Äã‚àíC2z2‚Äã=0.‚Äã‚Äã . When $A=B$ the horizontal cross-sections are circles, just like in our construction. The elastic bands effectively allow us to play with the parameter $C$, which controls how fast the hyperboloid grows sideways. There is a wonderful widget in the Interactive Gallery of Quadric Surfaces, its quite fun to play with. . To see how this curved surface can be made entirely out of straight lines, one can define the surface as the set of all parametric curves of the kind . x=A(cos‚Å°Œ∏‚àívsin‚Å°Œ∏)y=B(sin‚Å°Œ∏+vcos‚Å°Œ∏)z=CŒµv. begin{align} x &amp;= A ( cos theta - v sin theta) y &amp;= B ( sin theta + v cos theta) z &amp;= C varepsilon v. end{align}xyz‚Äã=A(cosŒ∏‚àívsinŒ∏)=B(sinŒ∏+vcosŒ∏)=CŒµv.‚Äã‚Äã . Substitute the parametric equations above into the equation of the paraboloid and see that they satisfy it. . There are actually two families of curves, for $ epsilon= pm 1$. I made a Mathematica plot of the two families of lines, in red ($ varepsilon=1$) and in blue ($ varepsilon=-1$), for ten $ theta$ values between 0 and $2 pi$. It‚Äôs easy to see that each family of lines swirls in a different direction. . . The Mathematica code I wrote is . a = 1; b = 1; c = 1; h = 4; n = 10; p1 = ContourPlot3D[ (*hyperboloid*) x^2/a^2 + y^2/b^2 - z^2/c^2 == 1, {x, -h, h}, {y, -h, h}, {z, -h, h}, Mesh -&gt; None, ContourStyle -&gt; Opacity[0.4], AxesLabel -&gt; {x, y, z}, LabelStyle -&gt; Large ]; p2 = ParametricPlot3D[ (*red lines*) Table[{ a (Cos[theta] - v Sin[theta]), b (Sin[theta] + v Cos[theta]), c v }, {theta, 0, 2 Pi, 2 Pi/n}], {v, -20, 20}, PlotStyle -&gt; {Red} ]; p3 = ParametricPlot3D[ (*blue lines*) Table[{ a (Cos[theta] - v Sin[theta]), b (Sin[theta] + v Cos[theta]), - c v }, {theta, 0, 2 Pi, 2 Pi/n}], {v, -20, 20}, PlotStyle -&gt; {Blue} ]; Show[{p1, p2, p3}] . .",
            "url": "https://yairmau.github.io/markdown/2021/11/12/skewer-hyperboloid.html",
            "relUrl": "/markdown/2021/11/12/skewer-hyperboloid.html",
            "date": " ‚Ä¢ Nov 12, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Tomoko Fuse Unit Origami",
            "content": ". This structure has the general shape of a dodecahedron, with 12 beautiful pentagonal stars embeded in it. I found a while back a scan of the book Tomoko Fuse Unit Origami Fantasy. The book is all written in japanese, but I could make sense of the diagrams. . Tomoko Fuse is a master of modular origami, just google her name and you‚Äôll find tons of fun projects. . I used craft paper to give the structure a muted tones and a rustic feel. I think it turned out pretty good üòä. . .",
            "url": "https://yairmau.github.io/markdown/2021/11/08/tomoko-fuse.html",
            "relUrl": "/markdown/2021/11/08/tomoko-fuse.html",
            "date": " ‚Ä¢ Nov 8, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "George Hart's Card Constructions",
            "content": ". George Hart is a sculptor and geometer, and his webpage has the best projects for the artistically-minded nerd! . The image above shows the 30-cards construction, but it is not identical to what George Hart shows on his card constructions page. I actually used Francesco De Comit√©‚Äôs version, also credited by Hart. The idea is that you strategically cut the cards in very specific places, so that they can be slided together. . Figuring out how to cut the cards was quite hard to me, I had to experiment a lot! George Hart provides printable templates, but their dimensions are different from the cards I used. Finally besides cutting the cards, the task of putting everything together was a nightmare. It‚Äôs been a while since I built this, but I think that I used clothes pegs to hold the cards together before the whole thing is ready. Once you finish building the structure, it should be much more stable (but not enough to throw it in the air!!). . Because there is a 5-fold symmetry, naturally the golden ratio . œÜ=1+52 varphi = frac{1+ sqrt{5}}{2}œÜ=21+5 . ‚Äã‚Äã . figures quite prominently in there. Francesco De Comit√© provides in his Flickr page a plan for how to cut the cards: . . I put the plan above for the future me to understand what it means, because I myself didn‚Äôt use this‚Ä¶ I found in one of my ‚Äúprojects‚Äù folders my own calculations of how to cut the cards. It shows only the cuts on the top side. One should rotate the card 180 degrees and do the same cuts on the other side. For a card of height H and length L, my cutting plan is the following. . . The cuts are the thick blue lines, and the red arrows show the length $L/ varphi$ of one of the cuts and of the uncut (dashed) part. From this I hope you can imagine that when the cards are slided together, the two cuts amount to the length of the whole ‚Äúdiagonal‚Äù, such that the two card ends are flush. I wish I could explain why the plan is what it is, but some time has passed and I have no idea any more ü§∑‚Äç‚ôÇÔ∏è‚Ä¶ . .",
            "url": "https://yairmau.github.io/markdown/2021/11/08/george-hart-card-constructions.html",
            "relUrl": "/markdown/2021/11/08/george-hart-card-constructions.html",
            "date": " ‚Ä¢ Nov 8, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "The Sonobe Module",
            "content": "All these modular origami constructions are made of a single basic module, the Sonobe unit. . The basic unit has flaps and pockets. . The actual folding of the Sonobe module is quite easy, see the instructions below, taken from Michael Naughton‚Äôs excellent diagram. . . This is an incredibly flexible unit, allowing us to combine it in many ways. This pdf shows how to combine different numbers of Sonobe modules to produce the shapes above, and many others. If all meeting points of the basic unit contain 3 modules, one gets a cube. If all meeting points contain 4 modules, then we produce a octahedron (with pyramids on each of its faces). . . The results can be very impressive! Above is a Truncated Icosahedron, made of pentagons surrounded by hexagons (like a soccer ball). This one is made of 90 Sonobe units, see detailed instructions. .",
            "url": "https://yairmau.github.io/markdown/2021/11/03/sonobe.html",
            "relUrl": "/markdown/2021/11/03/sonobe.html",
            "date": " ‚Ä¢ Nov 3, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Pythagorean theorem",
            "content": "I‚Äôve been interested lately in the Pythagorean theorem, and the myriad of ways one can prove it. . . An excellent resource is the website Cut The Knot. . Another great source is John C. Sparks‚Äô The Pythagorean Theorem: Crown Jewel of Mathematics. . I used my tablet to write down my own version of some of the nicest proofs, see below. .",
            "url": "https://yairmau.github.io/markdown/2021/10/28/pythagoras.html",
            "relUrl": "/markdown/2021/10/28/pythagoras.html",
            "date": " ‚Ä¢ Oct 28, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Hexastix",
            "content": "I‚Äôm very proud of this construction I made about 4 years ago. . . It sits prominently on my office shelf, it has never failed to impress the occasional visitor :) Although it looks complicated, this 72-pencil construction is not very hard to build! I followed Matt Parker‚Äôs youtube tutorial, it took me about 1.5 hours to make it, and another hour or so to glue it. . Materials: . 72 pencils | a few elastic bands | super glue | . Here in Israel, I couldn‚Äôt find non-sharpened pencils. There is some risk of (small) injury if you don‚Äôt take care üò¨. . Without the elastic bands, this construction would not be stable. The problem is that with time the elastic bands get dry and disintegrate, so if you want to have this construction standing for a long time, you have to glue it. I put tiny drops of super glue in most of the touching points between the pencils, and it worked great! . Enjoy making your own! . . PS: I‚Äôm in my office at the university writing these words, and exactly now a student passed in the corridor and asked me about the Hexastix üòÑ .",
            "url": "https://yairmau.github.io/markdown/2021/10/28/hexastix.html",
            "relUrl": "/markdown/2021/10/28/hexastix.html",
            "date": " ‚Ä¢ Oct 28, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "First blog post",
            "content": "This is my first blog post on this website ü•≥ . Every now and then I‚Äôll write about what‚Äôs going on with my research group, and also about random stuff that interest me. . See you around! .",
            "url": "https://yairmau.github.io/markdown/2021/10/28/first-post.html",
            "relUrl": "/markdown/2021/10/28/first-post.html",
            "date": " ‚Ä¢ Oct 28, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "The can problem",
            "content": "watch this space .",
            "url": "https://yairmau.github.io/markdown/2021/10/27/can-problem.html",
            "relUrl": "/markdown/2021/10/27/can-problem.html",
            "date": " ‚Ä¢ Oct 27, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Spatial distribution - lecture",
            "content": "The problem . Let&#39;s say we want to calculate the average rainfall on a watershed, and we have data available for 7 stations, as shown in the figure below [Dingman, figure 4.26]: . There are a number of methods for calculating the average precipitation. . Thiessen method [Voronoi diagram] . Brutsaert, Figure 3.11 . How to compute the areas: . Average areal precipitation is a weighted sum: . $$ langle P rangle = frac{ sum_i A_i P_i}{ sum_i A_i} $$A nice way to understand the Thiessen method is depicted in the gif below (from Wikipedia): . . Inverse distance method . Brutsaert, Figure 3.12 . The precipitation for square 17 is . $$ P_{17} = displaystyle frac { displaystyle sum_ text{$i$ = all stations} frac{P_i}{d_{i,17}^2}} { displaystyle sum_ text{$i$ = all stations} frac{1}{d_{i,17}^2}} $$The average precipitation for the whole watershed is the weighted average of all squares, where the weight is their area: . $$ langle P rangle = displaystyle frac { displaystyle sum_ text{$j$ = all squares} A_j P_j} { displaystyle sum_ text{$j$ = all squares} A_j} $$Brutsaert, page 93: . Dean and Snyder (1977) found that the exponent (for the distance $d^{-b}$) b = 2 yielded the best results in the Piedmont region of the southeastern United States, whereas Simanton and Osborn (1980) concluded from measurements in Arizona that b can range between 1 and 3 without significantly affecting the results. . Isohyetal method . Brutsaert, Figure 3.12 . The same equation of the Thiessen method can be used: . $$ langle P rangle = frac{ sum_i A_i P_i}{ sum_i A_i} $$ How it is actually done . Most often, Geographic Information System (GIS) software is used to analyze spatial data. Two of the most used programs are ArcGIS (proprietary) and QGIS (free). . A good discussion of the different methods can be found on Manuel Gimond&#39;s website, Intro to GIS and Spatial Analysis. . Attention, Don&#39;t mix precision with accuracy. There are many ways of interpolating, just because a result seems detailed, it does not imply that it is accurate! See below three interpolation methods. . . Below you can find a simple Python code that exemplifies some of the methods, producing the following figure: . . %matplotlib notebook import matplotlib.pyplot as plt import numpy as np from scipy.interpolate import griddata from scipy.spatial import Voronoi, voronoi_plot_2d, ConvexHull fig, ax = plt.subplots(1, 3, figsize=(10,7)) fig.subplots_adjust(left=0.0, right=1.0, top=0.96, bottom=0.05, hspace=0.02, wspace=0.02) N = 6 PI = &#39;3141592653589793&#39; points = np.random.rand(N, 2) points = np.vstack([points,[0,0], [0,1], [1,0], [1,1]]) values = np.array([int(x) for x in list(PI)])[:(N+4)] # values = np.array([3, 1, 4, 1, 5, 9, 2, 6, 5, 3]) grid_x, grid_y = np.mgrid[0:1:100j, 0:1:200j] grid_z_nearest = griddata(points, values, (grid_x, grid_y), method=&#39;nearest&#39;) grid_z_cubic = griddata(points, values, (grid_x, grid_y), method=&#39;cubic&#39;) ax[0].plot(points[:,0], points[:,1], &#39;o&#39;, ms=3, markerfacecolor=&quot;red&quot;, markeredgecolor=&quot;red&quot;) ax[0].set_aspect(&#39;equal&#39;, &#39;box&#39;) ax[0].set(xlim=[0,1], ylim=[0,1]) ax[0].set_title(&quot;the stations&quot;) for i, v in enumerate(values): ax[0].text(points[i,0], points[i,1], str(v)) ax[1].imshow(grid_z_nearest.T, extent=(0,1,0,1), origin=&#39;lower&#39;) ax[1].plot(points[:,0], points[:,1], &#39;o&#39;, ms=3, markerfacecolor=&quot;red&quot;, markeredgecolor=&quot;red&quot;) vor = Voronoi(points) voronoi_plot_2d(vor, show_vertices=False, line_colors=&#39;cyan&#39;, line_width=3, line_alpha=1, point_size=0, ax=ax[1]) ax[1].set_title(&quot;Thiessen Method&quot;) ax[2].plot(points[:,0], points[:,1], &#39;o&#39;, ms=3, markerfacecolor=&quot;red&quot;, markeredgecolor=&quot;red&quot;) nlines = int((values.max()-values.min()+1)/2) ax[2].contourf(grid_x, grid_y, grid_z_cubic, nlines) cont = ax[2].contour(grid_x, grid_y, grid_z_cubic, nlines, colors=&quot;black&quot;) ax[2].clabel(cont, inline=1, colors=&#39;white&#39;, fmt=&#39;%.0f&#39;) ax[2].set_title(&quot;Isohyetal Method&quot;) for i, a in enumerate(ax): a.set(xlim=[-0.2,1.2], ylim=[-0.2,1.2]) a.axis(&#39;off&#39;) a.set_aspect(&#39;equal&#39;, &#39;box&#39;) fig.savefig(&quot;spatial-distribution.png&quot;, dpi=500) .",
            "url": "https://yairmau.github.io/jupyter/2020/02/07/spatial-distribution.html",
            "relUrl": "/jupyter/2020/02/07/spatial-distribution.html",
            "date": " ‚Ä¢ Feb 7, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Budyko framework - lecture",
            "content": "Sources used: . (Daly et al., 2019), (Sposito, 2017), (Jones et al., 2012), (Krajewski et al., 2021), (Berghuijs et al., 2020), (Creed &amp; Spargo, 2012) . Water and surface energy balances . For long-term averages: . $$ P = ET+Q $$$$ R_n = lambda_w cdot ET + H $$ $P$: precipitation (L T$^{-1}$, e.g.: mm/day) | $ET$: evapotranspiration (L T$^{-1}$) | $Q$: streamflow (L T$^{-1}$) | $R_n$: net energy available at soil surface (M T$^{-3}$, e.g.: W m$^{-2}$) | $ lambda_w$: latent heat of vaporization of water (M L$^{-1}$T$^{-2}$, as defined here, the units will be weird) | $H$: sensible heat flux from the surface into the atmosphere (M T$^{-3}$) | $ lambda_w cdot ET$: latent heat flux (M T$^{-3}$) | . Assumptions . because we are dealing with long-term averages, there are negligible changes of watershed stored water. | negligible energy is stored at the soil surface, and heat transfer from soil surface to deeper soil layers ($G$) averages zero. | Question . Given measurements of rainfall and meteorological conditions, can we predict the partitioning of $P$ between $ET$ and $Q$? . Limits . For very dry watersheds (deserts, for example), almost all precipitation ($P$) is lost via evapotranspiration ($ET$). These watersheds are called water limited. . In wet watersheds, at the annual scale, the sensible heat ($H$) is directed from the surface to the atmosphere in almost all climatic zones on Earth (meaning: soil heats air). Therefore, $H$ cannot supply much energy to the soil surface, and it is assumed that $R_n$ provides entirely the energy required for evapotranspiration. Dividing the second equation by $ lambda_w$, we get $R_n/ lambda_w = ET + H/ lambda_w$. It is clear that the maximum possible $ET$ occurs when all incoming radiation energy $R_n$ is consumed by evapotranspiration $ET$, and there is negligible sensible heat flux $H$. As a result, the upper limit of $ lambda_w E$ is $R_n$, in wet watersheds. In these watersheds, called energy limited, $ET$ tends to the potential evapotranspiration ($ET_0$). . Summary: . For energy-limited watersheds . (1) As precipitation $P rightarrow infty$, evapotranspiration $ET rightarrow ET_0$ . For water-limited watersheds . (2) As potential evapotranspiration $ET_0 rightarrow infty$, actual evaporation $ET rightarrow P$ . In general, we can write . $$ ET = f(P,ET_0) $$The variables $P$ and $ET$ have the same dimenstions (L T$^{-1}$), and we can divide the equation above by $P$:$$ frac{ET}{P} = f(D_I), $$ . where $$ D_I = displaystyle frac{ET_0}{P} $$ is called the dryness index. A useful classification is . Dryness Index Classification . $D_I &lt; 1.54$ | Humid | . $1.54 &lt; D_I &lt; 2$ | Dry Subhumid | . $2 &lt; D_I &lt; 5$ | Semi-arid | . $5 &lt; D_I &lt; 20$ | Arid | . $20 &lt; D_I$ | Hyper-arid | . ATTENTION. The dryness index can also be called the &quot;Aridity Index&quot; ($AI$), however sometimes the $AI$ means the inverse of $D_I$: $$AI = 1/D_I$$ Be careful to check the definitions. . The summary (1) and (2) above can be now represented as: . (1) As $D_I rightarrow 0$, $ displaystyle frac{ET}{P} rightarrow D_I$ . (2) As $D_I rightarrow infty$, $ displaystyle frac{ET}{P} rightarrow 1$ . . . Budyko (1974), proposed the following equation: . $$ frac{ET}{P} = left[ D_I tanh left( frac{1}{D_I} right) left( 1-e^{-D_I} right) right]^{1/2} $$ . Source: (Jones et al., 2012) . Source: (Krajewski et al., 2021) . There are many alternatives to Budyko&#39;s equation. Many equations have adjustable parameters, such as Fu&#39;s equation: . $$ frac{ET}{P} = 1 + D_I - (1 + D_I^w)^{1/w}, $$where $w&gt;1$. Each catchment has its own specific parameter $w$, that may represent biophysical/landscape features. There is no concensus regarding the interpretation of $w$, ranging from an effective empirical parameter, whose relationship to biophysical features can be discerned, to an arbitrary empirical constant with no a priori physical meaning. Source: (Reaver et al., 2020) . Source: (Zhang et al., 2004) . Hypotheses for why dryness index controls so much the partitioning of P into ET and Q . Source: (Berghuijs et al., 2020) . The first is that the Budyko curve is accurate because landscape features (e.g., soils and vegetation) coevolve with the local climate in such a manner that precipitation partitioning into streamflow and evapotranspiration converges towards the Budyko curve | A second hypothesis is that catchments over time evolve towards the supply and demand limits (rather than towards a curve), because landscapes and their vegetation are unaware of the Budyko curve but do evolve to maximize their use of available resources (including water). However, because limiting factors such as climatic variability exist (which will reduce a catchment&#39;s ability to use all water because it cannot fully buffer the highly variable precipitation input), catchments will tend to not reach these limits. This may lead to an (apparent) existence of the Budyko curve which falls relatively close to the demand and supply limits. | A third hypothesis is that the existence of a strong universal relationship between aridity and catchment water balances might be explained by an underlying organizing principle such as maximum entropy production because the Budyko curve may be consistent with how hydrologic systems optimally partition water and energy | A fourth hypothesis is that virtually any landscape and climate combination (also those in heavily disturbed landscapes: e.g., a city, agricultural lands, etc.) will fall near the Budyko curve because climate aridity will dominate precipitation partitioning largely independent of the climate-landscape configuration or any optimization principle. | Hypotheses for deviations from Budyko curve . Source: (Creed &amp; Spargo, 2012) . Under stationary conditions (naturally occurring oscillations), catchments will fall on the Budyko Curve | Under non-stationary conditions (anthropogenic climate change), catchments will deviate from the Budyko Curve in a predictable manner | Reasons for falling off the Budyko Curve . Inadequate representation of P and T (Loch Vale) | Inadequate representation of ET (Andrews) | Inadequate representation of Q (Marcell) | Forest conversion (Coweeta) | Forest disturbance (Luquillo) | Critique . Source: (Berghuijs et al., 2020) . The (mathematical) specifics of such studies vary, but all approaches are founded on the assumption that catchments follow a (parametric) Budyko curve when aridity changes, and that consequently all other movements in the Budyko space are caused by other factors. The validity of this assumption remains mostly untested, which seems surprising given it underpins all of these studies&#39; findings. . References . Daly, E., Calabrese, S., Yin, J., &amp; Porporato, A. (2019). Linking parametric and water-balance models of the Budyko and Turc spaces. Advances in Water Resources, 134, 103435. https://www.sciencedirect.com/science/article/pii/S0309170819305688 | Sposito, G. (2017). Understanding the Budyko equation. Water, 9(4), 236. | Jones, J. A., Creed, I. F., Hatcher, K. L., Warren, R. J., Adams, M. B., Benson, M. H., Boose, E., Brown, W. A., Campbell, J. L., Covich, A., &amp; others. (2012). Ecosystem processes and human influences regulate streamflow response to climate change at long-term ecological research sites. BioScience, 62(4), 390‚Äì404. | Krajewski, A., Sikorska-Senoner, A. E., Hejduk, L., &amp; Banasik, K. (2021). An Attempt to Decompose the Impact of Land Use and Climate Change on Annual Runoff in a Small Agricultural Catchment. Water Resources Management, 35(3), 881‚Äì896. | Berghuijs, W. R., Gnann, S. J., &amp; Woods, R. A. (2020). Unanswered questions on the Budyko framework. Hydrological Processes, 34(26), 5699‚Äì5703. | Creed, I., &amp; Spargo, A. (2012). Budyko guide to exploring sustainability of water yields from catchments under changing environmental conditions. London, Ontario. http://www.uwo.ca/biology/faculty/creed/PDFs/presentations/PRE116.pdf | Reaver, N. G. F., Kaplan, D. A., Klammler, H., &amp; Jawitz, J. W. (2020). Reinterpreting the Budyko Framework. Hydrology and Earth System Sciences Discussions, 1‚Äì31. | Zhang, L., Hickel, K., Dawes, W. R., Chiew, F. H. S., Western, A. W., &amp; Briggs, P. R. (2004). A rational function approach for estimating mean annual evapotranspiration. Water Resources Research, 40(2). | .",
            "url": "https://yairmau.github.io/jupyter/2020/02/06/budyko-framework-lecture.html",
            "relUrl": "/jupyter/2020/02/06/budyko-framework-lecture.html",
            "date": " ‚Ä¢ Feb 6, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Unit Hydrograph - lecture",
            "content": "Linear reservoir model . . . . . Rainfall-Runoff Models . The Rational Method . The rational method postulates a simple pro- portionality between peak discharge, $q_{pk}$, and rainfall intensity, $p^*$: . $$ q_{pk} = varepsilon_R cdot C_R cdot A_D cdot p^* $$ $q_{pk}$: peak discharge (m$^3$/s) | $ varepsilon_R=0.278$: unit-conversion factor | $C_R$: dimensionless runoff coefficient | $A_D$: drainage area (km$^2$) | $p^*$: rainfall intensity (mm/h) | . Obviously the results obtained with the method are highly sensitive to the value chosen for CR; values range from 0.05 for gently sloping lawns up to 0.95 for highly urbanized areas of roofs and pavement. The rational method is widely used in urban drainage design, but Pilgrim and Cordery (1992) caution that there are typically few data available to guide the selection of CR, and that CR for a given watershed may vary widely from storm to storm due to differing antecedent conditions. . The Soil Conservation Service Curve-Number Method (SCS-CN) . Also called NRCS curve number procedure. NRCS = Natural Resources Conservation Service - USDA . $$ Q^* = P^* = frac{ left( P-S_{I} right)^2}{P-S_I+S_{max}} $$The initial abstraction $S_I$ is usually approximated as $0.2 cdot S_{max}$, therefore: . $$ Q^* = P^* = frac{ left( P-0.2 cdot S_{max} right)^2}{P+0.8 cdot S_{max}} $$$$ S_{max} = 25.4 left( frac{1000}{CN}-10 right) $$The number 25.4 is a conversion factor from inches to millimeters. . . . The curve number (CN) is a function of the ability of soils to infiltrate water, land use, and the soil water conditions at the start of a rainfall event (antecedent soil water condition). To account for the infiltration character- istics of soils, the NRCS has divided soils into four hydrologic soil groups, which are defined as follows (NRCS, 1984): . Group A (low runoff potential): Soils with high infiltration rates even when thoroughly wetted. These consist chiefly of deep, well-drained sands and gravels. These soils have a high rate of water transmission (final infiltration rate greater than 0.3 in./h). | Group B: Soils with moderate infiltration rates when thoroughly wetted. These consist chiefly of soils that are moderately deep to deep, moderately well drained to well drained with moderately fine to moderately coarse textures. These soils have a moderate rate of water transmission (final infil- tration rate 0.15 to 0.30 in./h). | Group C: Soils with slow infiltration rates when thoroughly wetted. These consist chiefly of soils with a layer that impedes downward movement of water or soils with moderately fine to fine texture. These soils have a slow rate of water transmission (final infiltration rate 0.05 to 0.15 in./h). | Group D (high runoff potential): Soils with very slow infiltration rates when thoroughly wetted. These consist chiefly of clay soils with a high swelling potential, soils with a permanent high water table, soils with a claypan or clay layer at or near the surface, and shallow soils over nearly impervious materials. These soils have a very slow rate of water transmission (final infiltration rate less than 0.05 in./h). | . There are also three categories for Antecedent Soil Moisture Condition (AMC): . AMC I: Dormant season antecedent soil moisture less than 0.5 in. Growing season antecedent soil moisture less than 1.4 in. | AMC II: Dormant season antecedent soil moisture between 0.5 and 1.1 in. Growing season anteced- ent soil moisture between 1.4 and 2.1 in. | AMC III: Dormant season antecedent soil mois- ture greater than 1.1 in. Growing season anteced- ent soil moisture greater than 2.1 in. | . See the table below to find curve numbers for AMC II: . P=21 ratio = 4.17e4/2.61e5 CN=86 Smax = 25.4 * (1000/CN - 10) Pmin = 0.2 * Smax Qstar = 0.0 if P &gt; Pmin: Qstar = (P - 0.2*Smax)**2 / (P+0.8*Smax) Qstar/P . 0.14270006393832066 . ratio . 0.15977011494252874 . Qstar / P . 0.9148811393863234 . %matplotlib notebook import numpy as np import matplotlib.pyplot as plt def Qstar_f(pe, CN): # Smax = 25.4*(1000/CN - 10) Smax = (1000/CN - 10) # Smax = (1000/CN - 10) / 25.4 Qstar = (pe - 0.2*Smax)**2 / (pe+0.8*Smax) return Qstar pe = np.linspace(0,8,101) # plt.plot(pe, Qstar_f(pe, 35)) plt.plot(pe, Qstar_f(pe, 50)) # plt.plot(pe, Qstar_f(pe, 85)) . [&lt;matplotlib.lines.Line2D at 0x7fd8b0e66610&gt;] . 8*25.4 . 203.2 .",
            "url": "https://yairmau.github.io/jupyter/2020/02/05/unit-hydrograph-lecture.html",
            "relUrl": "/jupyter/2020/02/05/unit-hydrograph-lecture.html",
            "date": " ‚Ä¢ Feb 5, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Streamflow - lecture",
            "content": "Watershed - &#1488;&#1490;&#1503; &#1492;&#1497;&#1511;&#1493;&#1493;&#1514; . . Watershed response: . The volume of water appearing in the appar- ent response hydrograph for a given event is usually only a fraction (often a very small frac- tion) of the total input. The remainder of the water input ultimately leaves the watershed as: (1) evapotranspiration; (2) streamflow that oc- curs so long after the event that it cannot be associated with that event; or (3) ground-water outflow from the watershed. | The water identified as the response to a given event may originate on only a fraction of the watershed; this fraction is called the contrib- uting area. | The extent of the contributing area may vary from event to event and during an event. | At least some of the water identified as the re- sponse to a given event may be ‚Äúold water‚Äù that entered the watershed in a previous event. | . base flow separation . Base flow . Base flow is the portion of streamflow that is presumed to have entered the watershed in previous events and to be derived from persistent, slowly varying sources. (Ground water is usually assumed to be the main, if not the only, such source.) . Event flow . Event flow (also called direct runoff, storm runoff, quick flow, or storm flow) is considered to be the direct response to a given water-input event. . Total flow . Total flow rate at any instant $q(t)$ is the sum of event-flow rate $q^*(t)$ and base-flow rate $q_{BF}$(t): . $$ q(t) = q^*(t) + q_{BF}(t) $$Attention! . Graphical flow separation techniques are heuristic and have no direct scientific basis. . Urbana, IL . hyetograph, hydrograph . notation . base flow separation . effective precipitation = effective discharge . $$ P^* = Q^* $$ . time lags . . It is commonly assumed that $T_{LPC} simeq 0.60 cdot T_c$, where $T_c$ is the time of concentration, i.e., the time it takes water to travel from the hydraulically most distant part of the contributing area to the outlet. . The centroid is a weighted-average time, each time instant is multiplied by the amount of flow in that instant. . Time of precipitation centroid: . $$ t_{pc} = frac{ displaystyle sum_{i=1}^n p_i^* cdot t_i}{P^*} $$Time of streamflow centroid: . $$ t_{qc} = frac{ displaystyle sum_{i=1}^n q_i^* cdot t_i}{Q^*} $$",
            "url": "https://yairmau.github.io/jupyter/2020/02/05/streamflow-lecture.html",
            "relUrl": "/jupyter/2020/02/05/streamflow-lecture.html",
            "date": " ‚Ä¢ Feb 5, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Streamflow - exercises",
            "content": "Import relevant packages . #collapse-hide import pandas as pd import numpy as np import matplotlib.pyplot as plt import matplotlib.dates as mdates import seaborn as sns sns.set(style=&quot;ticks&quot;, font_scale=1.5) from ipywidgets import * . . Import streamflow data from USGS&#39;s National Water Information System. We will be using data from Urbana, IL. . #collapse-hide # Drainage area: 4.78 square miles data_file = &quot;USGS 03337100 BONEYARD CREEK AT LINCOLN AVE AT URBANA, IL.dat&quot; df_q_2020 = pd.read_csv(data_file, header=31, # no headers needed, we&#39;ll do that later delim_whitespace=True, # blank spaces separate between columns na_values=[&quot;Bkw&quot;] # substitute these values for missing (NaN) values ) df_q_2020.columns = [&#39;agency_cd&#39;, &#39;site_no&#39;,&#39;datetime&#39;,&#39;tz_cd&#39;,&#39;EDT&#39;,&#39;discharge&#39;,&#39;code&#39;] # rename df columns with headers columns df_q_2020[&#39;date_and_time&#39;] = df_q_2020[&#39;datetime&#39;] + &#39; &#39; + df_q_2020[&#39;tz_cd&#39;] # combine date+time into datetime df_q_2020[&#39;date_and_time&#39;] = pd.to_datetime(df_q_2020[&#39;date_and_time&#39;]) # interpret datetime df_q_2020 = df_q_2020.set_index(&#39;date_and_time&#39;) # make datetime the index df_q_2020[&#39;discharge&#39;] = df_q_2020[&#39;discharge&#39;].astype(float) df_q_2020[&#39;discharge&#39;] = df_q_2020[&#39;discharge&#39;] * 0.0283168 # convert cubic feet to m3 fig, ax = plt.subplots(figsize=(10,7)) ax.plot(df_q_2020[&#39;discharge&#39;], &#39;-o&#39;) plt.gcf().autofmt_xdate() ax.set(xlabel=&quot;date&quot;, ylabel=r&quot;discharge (m$^3$/5min)&quot;); . . Import sub-hourly (5-min) rainfall data from NOAA&#39;s Climate Reference Network Data website . #collapse-hide data_file = &quot;Champaign - IL.txt&quot; df_p_2020 = pd.read_csv(data_file, header=None, # no headers needed, we&#39;ll do that later delim_whitespace=True, # blank spaces separate between columns na_values=[&quot;-99.000&quot;, &quot;-9999.0&quot;] # substitute these values for missing (NaN) values ) headers = pd.read_csv(&quot;HEADERS_sub_hourly.txt&quot;, # load headers file header=1, # skip the first [0] line delim_whitespace=True ) df_p_2020.columns = headers.columns # rename df columns with headers columns # LST = local standard time df_p_2020[&quot;LST_TIME&quot;] = [f&quot;{x:04d}&quot; for x in df_p_2020[&quot;LST_TIME&quot;]] # time needs padding of zeros, then convert to string df_p_2020[&#39;LST_DATE&#39;] = df_p_2020[&#39;LST_DATE&#39;].astype(str) # convert date into string df_p_2020[&#39;datetime&#39;] = df_p_2020[&#39;LST_DATE&#39;] + &#39; &#39; + df_p_2020[&#39;LST_TIME&#39;] # combine date+time into datetime df_p_2020[&#39;datetime&#39;] = pd.to_datetime(df_p_2020[&#39;datetime&#39;]) # interpret datetime df_p_2020 = df_p_2020.set_index(&#39;datetime&#39;) # make datetime the index . . Plot rainfall and streamflow. Does this makes sense? . #collapse-hide fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,7)) fig.subplots_adjust(hspace=0.05) start = &quot;2020-10-18&quot; end = &quot;2020-10-25&quot; ax1.plot(df_p_2020[start:end][&#39;PRECIPITATION&#39;]) ax2.plot(df_q_2020[start:end][&#39;discharge&#39;], color=&quot;tab:blue&quot;, lw=2) ax1.set(xticks=[], ylabel=r&quot;precipitation (mm)&quot;) ax2.set(xlabel=&quot;date&quot;, ylabel=r&quot;discharge (m$^3$/5min)&quot;) plt.gcf().autofmt_xdate() # makes slated dates . . Define smaller dataframes for $p(t)$ and $q(t)$, between the dates: . start = &quot;2020-10-20 14:00:00&quot; end = &quot;2020-10-21 04:00:00&quot; . Don&#39;t forget to convert the units to SI! . Calculate total rainfall $P^*$ and total discharge $Q^*$, in m$^3$. . #collapse-hide # Drainage area: 4.78 square miles area = 4.78 / 0.00000038610 # squared miles to squared meters start = &quot;2020-10-20 14:00:00&quot; end = &quot;2020-10-21 04:00:00&quot; df_p = df_p_2020.loc[start:end][&#39;PRECIPITATION&#39;].to_frame() df_p_mm = df_p_2020.loc[start:end][&#39;PRECIPITATION&#39;].to_frame() df_q = df_q_2020.loc[start:end][&#39;discharge&#39;].to_frame() df_p[&#39;PRECIPITATION&#39;] = df_p[&#39;PRECIPITATION&#39;].values * area / 1000 # mm to m3 in the whole watershed df_p[&#39;PRECIPITATION&#39;] = df_p[&#39;PRECIPITATION&#39;] / 60 / 5 # convert m3 per 5 min to m3/s P = df_p[&#39;PRECIPITATION&#39;].sum() * 60 * 5 Q = df_q[&#39;discharge&#39;].sum() * 60 * 5 print(&quot;total precipitation during event: Pstar = {:.1e} m3&quot;.format(P.sum())) print(&quot;total streamflow during event: Qstar = {:.1e} m3&quot;.format(Q.sum())) . . total precipitation during event: Pstar = 2.6e+05 m3 total streamflow during event: Qstar = 5.2e+04 m3 . Make another graph of $p(t)$ and $q(t)$, now with SI units. . #collapse-hide fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,7)) fig.subplots_adjust(hspace=0.05) start = &quot;2020-10-18&quot; end = &quot;2020-10-25&quot; ax1.plot(df_p[&#39;PRECIPITATION&#39;]) ax2.plot(df_q[&#39;discharge&#39;], color=&quot;tab:blue&quot;, lw=2) ax1.set(xticks=[], ylabel=r&quot;precipitation (m$^3$/s)&quot;, title=&quot;Precipitation and discharge, Boneyard Creek at Urbana, IL n 20-21 October 2020, 5-minute data&quot;) ax2.set(xlabel=&quot;date&quot;, ylabel=r&quot;discharge (m$^3$/s)&quot;) plt.gcf().autofmt_xdate() # makes slated dates . . It&#39;s time for base flow separation! Convert q(t) into q*(t) . #collapse-hide from matplotlib.dates import HourLocator, DateFormatter import matplotlib.dates as mdates import matplotlib.ticker as ticker fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,7)) fig.subplots_adjust(wspace=0.05) ax1.plot(df_q[&#39;discharge&#39;], color=&quot;black&quot;, lw=2) point1 = pd.to_datetime(&quot;2020-10-20 16:40:00&quot;) point2 = pd.to_datetime(&quot;2020-10-21 00:00:00&quot;) two_points = df_q.loc[[point1, point2]][&#39;discharge&#39;] ax1.plot(two_points, &#39;o&#39;, color=&quot;tab:red&quot;) new = pd.DataFrame(data=two_points, index=two_points.index) df_linear = (new.resample(&quot;5min&quot;) #resample .interpolate(method=&#39;time&#39;) #interpolate by time ) ax1.plot(df_linear, color=&quot;tab:blue&quot;) df_between_2_points = df_q.loc[df_linear.index] ax1.fill_between(df_between_2_points.index, df_between_2_points[&#39;discharge&#39;], y2=df_linear[&#39;discharge&#39;], color=&quot;tab:blue&quot;, alpha=0.3) qstar = df_q.loc[df_linear.index][&#39;discharge&#39;] - df_linear[&#39;discharge&#39;] Qstar = qstar.sum() * 60 * 5 ax2.plot(qstar, color=&quot;black&quot;, lw=2) ax2.fill_between(qstar.index, qstar, y2=0.0, color=&quot;tab:blue&quot;, alpha=0.3) ax1.set(xlim=[df_q.index[0], df_q.index[-1]], ylabel=r&quot;discharge (m$^3$/s)&quot;, ylim=[0, 5.5], yticks=[0,1,2,3,4], title=&quot;total discharge, q(t)&quot;) ax2.set(yticks=[], ylim=[0, 5.5], xlim=[df_q.index[0], df_q.index[-1]], title=&quot;effective discharge, q*(t)&quot; ) plt.gcf().autofmt_xdate() # makes slated dates . . We can calculate p* now, using . $$ P^* = Q^* $$One of the simplest methods is to multiply $p(t)$ by a fixed constant (&lt;1) to obtain $p^*$, so that the equation above holds true. . #collapse-hide ratio = Qstar/ P pstar = df_p[&#39;PRECIPITATION&#39;] * ratio Pstar = pstar.sum() * 5 * 60 print(f&quot;Qstar / P = {ratio:.2f}&quot;) . . Qstar / P = 0.16 . Calculate now the centroid ($t_pc$) for effective precipitation p and centroid ($t_{qc}$) of effective discharge q. Calculate also the time of peak discharge ($t_{pk}$). Then, calculate the centroid lag ($T_{LC}$), the centroid lag-to-peak ($T_{LPC}$), and the time of concentration ($T_c$). Use the equations below: . $T_{LPC} simeq 0.60 cdot T_c$ . Time of precipitation centroid: . $$ t_{pc} = frac{ displaystyle sum_{i=1}^n p_i^* cdot t_i}{P^*} $$Time of streamflow centroid: . $$ t_{qc} = frac{ displaystyle sum_{i=1}^n q_i^* cdot t_i}{Q^*} $$Centroid lag: . $$ T_{LC} = t_{qc} - t_{pc} $$Centroid lag-to-peak: $$ T_{LPC} = t_{pk} - t_{pc} $$ . Time of concentration: $$ T_{LPC} simeq 0.60 cdot T_c $$ . #collapse-hide # pstar centroid # time of the first (nonzero) rainfall data point t0 = pstar[pstar != 0.0].index[0] # time of the last (nonzero) rainfall data point tf = pstar[pstar != 0.0].index[-1] # duration of the rainfall event, in minutes td = (tf-t0) / pd.Timedelta(&#39;1 min&#39;) # make time array, add 2.5 minutes (half of dt) time = np.arange(0, td+1, 5) + 2.5 # create pi array, only with relevant data (during rainfall duration) pi = pstar.loc[(pstar.index &gt;= t0) &amp; (pstar.index &lt;= tf)] # convert from m3/5min to m3/s pi = pi.values * 60 * 5 # time of precipitation centroid t_pc = (pi * time).sum() / pi.sum() # add initial time t_pc = t0 + pd.Timedelta(minutes=t_pc) t_pc # qstar centroid # time of the first (nonzero) discharge data point t0 = qstar[qstar != 0.0].index[0] # time of the last (nonzero) discharge data point tf = qstar[pstar != 0.0].index[-1] # duration of the discharge event, in minutes td = (tf-t0) / pd.Timedelta(&#39;1 min&#39;) # make time array, add 2.5 minutes (half of dt) time = np.arange(0, td+1, 5) + 2.5 # create qi array, only with relevant data (during discharge duration) qi = qstar.loc[(qstar.index &gt;= t0) &amp; (qstar.index &lt;= tf)] # convert from m3/5min to m3/s qi = qi.values * 60 * 5 # time of discharge centroid t_qc = (qi * time).sum() / qi.sum() # add initial time t_qc = t0 + pd.Timedelta(minutes=t_qc) t_qc # time of peak discharge max_discharge = qstar.max() t_pk = qstar[qstar == max_discharge].index[0] # centroid lag T_LC = t_qc - t_pc # centroid lag-to-peak T_LPC = t_pk - t_pc # time of concentration T_c = T_LPC / 0.60 print(f&quot;T_LC = {T_LC}&quot;) print(f&quot;T_LPC = {T_LPC}&quot;) print(f&quot;T_c = {T_c}&quot;) . . T_LC = 0 days 00:53:03.186594 T_LPC = 0 days 01:22:59.857820 T_c = 0 days 02:18:19.763033333 .",
            "url": "https://yairmau.github.io/jupyter/2020/02/05/streamflow-exercises.html",
            "relUrl": "/jupyter/2020/02/05/streamflow-exercises.html",
            "date": " ‚Ä¢ Feb 5, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Assignment 3 - Streamflow",
            "content": "&#128210; instructions . This is where learning happens, not during a lecture. You&#39;ll learn a ton of things by doing them yourself. Much success! üòÑ . Create a Jupyter Notebook called assignment-03-IDNUMBER, where IDNUMBER is your 9-digit ID. This is the file only file we will check. . &#128204; locations and data . Choose one location in the US. . Import streamflow data from USGS&#39;s National Water Information System. Choose on the map any measuring station you see fit. Make sure there is available discharge data (usually given in cubic feet per second) in small time intervals, e.g., every 15 minutes. . | Go to NOAA&#39;s Climate Reference Network Data website. The sub-hourly (5-min) data contains information on many variables, we are interested in precipitation. . | Attention! Some os the USGS stations provide precipitation data. If you find one such station, step 2 above is unnecessary. If you only find discharge data in the USGS website, then make sure you choose two stations in very close proximity (USGS and NOAA). Because there are only a few high-resolution NOAA stations, you might want to start from there and then find discharge data for a stream near the NOAA station. . Bottom line: you are looking for precipitation and stream discharge data, for stations in close proximity, with a high temporal resolution (5 min, 15 min, etc). . &#128736; tasks . Choose a rain event of a few hours in your data set. Find the rate of effective water input (p) and the event flow rate (q). Analyze the data in a similar was as done during class (various graphs explaining what you see). Find also the characteristic times of the event (centroid lag $T_{LC}$, and centroid lag-to-peak $T_{LPC}$). . Try to find information on the climate, geography, soil, and land use of the watershed. Begin the assignment by explaining about the watershed you chose and characterizing it. When presenting the data and your analyses, discuss what you see based on the concepts learned in class (infiltration, runoff generation, and the factors that affect them). Does the information you found match what you see? What makes sense, and what doesn&#39;t? . Discussion is important! . You will have two weeks to deliver your assignment. You should not hand in a dry document with only figures and code, I&#39;m expecting text before and after each code/graph cell, explaining what you did, why you did it, and how it fits the story you are telling. Don&#39;t forget to put labels on your plot axes, title, legend, etc. . Your Jupyter Notebook should be fully functional: if we press Kernel &gt; Restart &amp; Run All, all the code must work without any errors. . &#127749; presentation . All the assignment must be in one single Jupyter Notebook. Use markdown cells to discuss the analysis and results, and in code cells show all the code you used to produce the figures and data analysis. Leave only the code necessary for your analysis, delete unnecessary lines your wrote while analyzing your data. Don&#39;t forget to comment your code, just like we did during exercise sessions. . You can write in English or in Hebrew, but the text in the figures must be in English. If you choose to write the discussion in Hebrew, be aware that Jupyter Notebooks don&#39;t have native right-to-left language support: . ◊†◊ô◊™◊ü ◊ú◊õ◊™◊ï◊ë ◊ë◊¢◊ë◊®◊ô◊™, ◊ú◊û◊®◊ï◊™ ◊©◊ñ◊î ◊ú◊ê ◊†◊®◊ê◊î ◊õ◊¥◊õ ◊ò◊ï◊ë... . You can use some HTML code to achieve best results in Hebrew. Type the following . &lt;p dir=&quot;rtl&quot; style=&quot;text-align: right;&quot;&gt; ◊¢◊õ◊©◊ô◊ï ◊î◊®◊ë◊î ◊ô◊ï◊™◊® ◊ò◊ï◊ë! &lt;/p&gt; . to get . ◊¢◊õ◊©◊ô◊ï ◊î◊®◊ë◊î ◊ô◊ï◊™◊® ◊ò◊ï◊ë! . If you have many paragraphs in hebrew, do the following: . ◊§◊°◊ß◊î ◊û◊°◊§◊® 1. . ◊§◊°◊ß◊î ◊û◊°◊§◊® 2. . ◊ê◊ù ◊ô◊© ◊ú◊õ◊ù ◊õ◊û◊î ◊§◊°◊ß◊ê◊ï◊™, ◊õ◊ú ◊ê◊ó◊™ ◊û◊î◊ü ◊™◊î◊ô◊î ◊ë◊™◊ï◊ö &quot;dir&quot; ◊û◊©◊ú◊î . In my opinion it is too complicated to write in Hebrew in Jupyter Notebooks, just write in English, your grade will not be affected by typos nor less-than-perfect English proficiency. . &#128175; evaluation . Your assignment will be evaluated according to the following criteria: . 40% Presentation. How the graphs look, labels, general organization, markdown, clean code. | 30% Discussion. This is where you explain what you did, what you found out, etc. | 15% Depth of analysis. You can analyze/explore the data with different levels of complexity, this is where we take that into consideration. | 10% Replicability: Your code runs flawlessly. | 5%: Code commenting. Explain in your code what you are doing, this is good for everyone, especially for yourself! | . &#128666; importing the data . You can use the code from previous assignments and from the exercise lectures. .",
            "url": "https://yairmau.github.io/jupyter/2020/02/05/assignment-03-streamflow.html",
            "relUrl": "/jupyter/2020/02/05/assignment-03-streamflow.html",
            "date": " ‚Ä¢ Feb 5, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Infiltration - lecture",
            "content": "Definitions . Hillel, Introduction to Environmental Soil Physics, figure 14.6 . Dingman, figure 8.13 . Dingman, figure 8.14 . Hillel, Introduction to Soil Physics, figure 12.3 . Source: Dingman, &quot;Physical Hydrology&quot;, 3rd edition, page 355 . The water-input rate, $w(t)$ [L T$^{-1}$], is the rate at which water arrives at the surface due to rain, snowmelt, or irrigation. A water-input event begins at time $t=0$ and ends at $t=T_w$. | The infiltration rate, $f(t)$ [L T$^{-1}$], is the rate at which water enters the soil from the surface. | The infiltrability, also called infiltration capacity, $f^*(t)$ [L T$^{-1}$], is the maximum rate at which infiltration could occur at any time; note that this changes during the infiltration event. | The depth of ponding, $H(t)$ [L], is the depth of water standing on the surface. | . Source: Ward &amp; Trimble, &quot;Environmantal Hydrology&quot;, 2nd edition, page 63, 64 . Infiltration capacity of absorbent paper is low, there is much runoff . Infiltration capacity of sponge is high, there is little runoff . Infiltration capacity of the sponge is limited by the overlying layer with low permeability . Infiltration capacity of the sponge is limited by the underlying layer . Ward &amp; Trimble, &quot;Environmantal Hydrology&quot;, 2nd edition, page 65 . Darcy . Darcy&#39;s equation for vertical flow $$ q = -K frac{ partial H_ text{total}}{ partial z} $$ . where the total head $H_ text{total}=-H_ text{suction}-z_ text{depth}$, and . $H_ text{suction}$ is the suction head (negative pressure head) | $z_ text{depth}$ is the depth, points downward. | . Substituting: . $$ q = K frac{ partial H_ text{suction}}{ partial z} + K $$Substituting the above into the continuity equation . $$ frac{ partial theta}{ partial t} = frac{ partial q}{ partial z} $$yields the Richards equation. . Richards . Richards equation: . $$ frac{ partial theta}{ partial t} = frac{ partial}{ partial z} left[ K( theta) frac{ partial H_ text{total}}{ partial z} right] $$Substituting $H_ text{total}=-H_ text{suction}-z_ text{depth}$ yields: . $$ frac{ partial theta}{ partial t} = frac{ partial}{ partial z} left[ K( theta) left( frac{ partial(-H_ text{suction} - z)}{ partial z} right) right] $$$$ frac{ partial theta}{ partial t} = - underbrace{ frac{ partial}{ partial z} left( K( theta) frac{ partial H_ text{suction}}{ partial z} right) } _{ text{matric}} - underbrace{ frac{ partial K( theta)}{ partial z} } _{ text{gravitational}} $$ short times . As the water starts to enter the relatively dry soil, the pressure differences in the water at the surface and in the soil are quite large and, as a result, the second term on the right is practically negligible compared to the first one. . $$ frac{ partial theta}{ partial t} = - frac{ partial}{ partial z} left( K( theta) frac{ partial H}{ partial z} right) $$ long times . As illustrated in the figure below (Davidson et al., 1963), after longer times of infiltration, the water content profile near the surface gradually becomes more uniform and it eventually assumes the satiation value, or $ theta rightarrow theta_0$; similarly, the pressure in the upper layers of the soil becomes gradually atmospheric, or $H rightarrow 0$. Hence, their vertical gradients . $$ frac{ partial theta}{ partial z} text{ and } frac{ partial H_ text{suction}}{ partial z} longrightarrow 0 $$From Darcy&#39;s equation we have that . $$ q = K( theta_0) = K_ text{sat} $$ . Rainfall infiltration . Infiltration rate is equal to rainfal rate, at least at first. If rainfall rate $w$ is lower than $K_ text{sat}$, than everything enters the soil, i.e., $f=K_ text{sat}$. However, if $w&gt;K_ text{sat}$, water content $ theta$ will increase at the surface, until it reaches $ theta_0$, and at that moment, called ponding time $t_p$, water will begin to accumulate at the surface. . Hillel, Introduction to Environmental Soil Physics, figure 12.1 . Hillel, Introduction to Environmental Soil Physics, figure 12.2 . Horton equation . One of the most widely used models, developed by R.E. Horton (1939), considered to be the father of modern hydrology. . $$ f = f_c+(f_0-f_c)e^{- beta t} $$ $f$: infiltration rate | $f_c$: infiltration capacity at large $t$ | $f_0$: initial infiltration capacity | $ beta$: best fit empirical parameter | . Advantages . Simple equation | Usually gives good fit to measured data because it is dependent on three parameters | . Disadvantages . This method has no physical significance, it is not based on any water transport mechanism | Does not describe infiltration prior to ponding | . Green &amp; Ampt . Dingman, figure 8.11 . Assumptions: . homogeneous soil, infinite depth (no water table) | horizontal surface | constant water head equal to zero is maintained at the surface | uniform water content prior to wetting, $ theta(t=0,z)= theta_0$ | moving front is characterized by a constant matric suction, $ psi_f$ | . Source: Dingman, page 370 . This equation was developed under the scenario of constant rainfall or irrigation on an initially dry soil as a sharp wetting front (such as piston flow). Water penetrates a dry soil with a certain initial moisture content, and wets the layer to a saturated moisture content as it traverses deeper. The connection between soil moisture and infiltration rate is modeled in the Green-Ampt equation: . $$ f(t) = K_ text{sat} left[ 1 + frac{| psi_f| cdot left( phi - theta_0 right)}{F(t)} right] $$ $f(t)$: infiltration rate | $F(t)$: cumulative infiltration rate, $F= int ! f text{ d}t$ | $ psi_f$: effective wetting-front suction | $ phi$: soil porosity | $ theta_0$: initial soil water content | . The same equation can be simply be rewritten as . $$f = frac{A}{F} + B$$ . where . $A = K_ text{sat} cdot| psi_f| cdot left( phi - theta_0 right)$ | $B= K_ text{sat}$ | . The porosity $ phi$ and the saturated hydraulic conductivity $K_ text{sat}$ can be estimated from the soil texture. The wetting-front suction $ psi_f$ can be estimated using the Brooks-Corey parameters: . $$ | psi_f| = frac{2b+3}{2b+6} cdot | psi_{ae}|, $$where $ psi_{ae}$ is the air-entry pressure head. Values for the parameters above can be found in this table: . . Best Fit, Least Squares Method .",
            "url": "https://yairmau.github.io/jupyter/2020/02/04/infiltration-lecture.html",
            "relUrl": "/jupyter/2020/02/04/infiltration-lecture.html",
            "date": " ‚Ä¢ Feb 4, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Infiltration - exercises",
            "content": "Tasks . Google the following: web plot digitizer | Load image &quot;nassif-16percent-slope.png&quot; (see below) | Create four csv files, one for each data set. Call them whatever you want. Legend: white circle = 312 mm/h, triangle = 234 mm/h, x = 156 mm/h, black circle = 78 mm/h. | The image is the second panel of Fig. 8, from . Nassif, S. H., and E. M. Wilson, 1975, &quot;THE INFLUENCE OF SLOPE AND RAIN INTENSITY ON RUNOFF AND INFILTRATION&quot;, Hydrological Sciences Journal. download here . Import relevant packages . #collapse-hide import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns.set(style=&quot;ticks&quot;, font_scale=1.5) from scipy.optimize import curve_fit import matplotlib.patches as patches . . Load all four files you created. Use numpy&#39;s function loadtxt. Make sure that the first point in each table corresponds to the appropriate rainfall rate. You can normalize the data if it is not. . #collapse-hide d1 = np.loadtxt(&quot;input_rate_078mm_per_h_16percent_slope.csv&quot;, delimiter=&#39;,&#39;) d2 = np.loadtxt(&quot;input_rate_156mm_per_h_16percent_slope.csv&quot;, delimiter=&#39;,&#39;) d3 = np.loadtxt(&quot;input_rate_234mm_per_h_16percent_slope.csv&quot;, delimiter=&#39;,&#39;) d4 = np.loadtxt(&quot;input_rate_312mm_per_h_16percent_slope.csv&quot;, delimiter=&#39;,&#39;) d1[:,1] = 78 * d1[:,1] / d1[:,1].max() d2[:,1] = 156 * d2[:,1] / d2[:,1].max() d3[:,1] = 234 * d3[:,1] / d3[:,1].max() d4[:,1] = 312 * d4[:,1] / d4[:,1].max() . . Reproduce the original figure, make it look good, something like this: . #collapse-hide fig, ax = plt.subplots(figsize=(10,7)) ax.plot(d4[:,0], d4[:,1], &#39;o&#39;, markerfacecolor=&quot;None&quot;, label=r&quot;water input = 312 mm h$^{-1}$&quot;) ax.plot(d3[:,0], d3[:,1], &#39;^&#39;, label=r&quot;water input = 234 mm h$^{-1}$&quot;) ax.plot(d2[:,0], d2[:,1], &#39;x&#39;, label=r&quot;water input = 156 mm h$^{-1}$&quot;) ax.plot(d1[:,0], d1[:,1], &#39;o&#39;, label=r&quot;water input = 78 mm h$^{-1}$&quot;) ax.set(xlabel=&quot;Time (min)&quot;, ylabel=r&quot;Infiltration rate (mm h$^{-1}$)&quot;) ax.legend(loc=&quot;upper right&quot;); . . Horton&#39;s equation . $$ f = f_c+(f_0-f_c)e^{- beta t} $$ $f$: infiltration rate | $f_c$: infiltration capacity at large $t$ | $f_0$: initial infiltration capacity | $ beta$: best fit empirical parameter | . Write a function called horton, that receives time t and the three parameters, and returns the right-hand side of the equation above. Plot one of the data sets, together with a guess of the parameters that should roughly fit the data. . #collapse-hide def horton(t, fc, f0, beta): return fc + (f0 - fc)*np.exp(-beta*t) fig, ax = plt.subplots(figsize=(10,7)) t = d1[:,0] t = t - t[0] f = d1[:,1] ax.plot(t, f, &#39;o&#39;, label=&quot;data&quot;) ax.plot(t, horton(t, 35, 80, 0.5), &#39;-&#39;, label=&quot;horton&quot;) ax.set(xlabel=&quot;time (min)&quot;, ylabel=&quot;infiltration rate (mm/h)&quot;) ax.legend(loc=&quot;upper right&quot;); . . Find the best fit for the parameters $f_c, f_0, beta$. Calculate the $R^2$ for each data set. . For the best fit, use scipy&#39;s curve_fit. Write a function to compute the R-squared of your fit. . #collapse-hide def horton(t, fc, f0, beta): return fc + (f0 - fc)*np.exp(-beta*t) def best_fit(data): t = data[:,0] t0 = t[0] t = t - t0 f = data[:,1] # best fit popt, pcov = curve_fit(f=horton, # model function xdata=t, # x data ydata=f, # y data p0=(130, 800, 0.5), # initial guess of the parameters ) return [popt, pcov] def calculate_r_squared(data, popt): t = data[:,0] t = t - t[0] f = data[:,1] # Calculate residuals residuals = f - horton(t, *popt) # You can get the residual sum of squares (ss_res) with ss_res = np.sum(residuals**2) # You can get the total sum of squares (ss_tot) with ss_tot = np.sum((f - np.mean(f))**2) # And finally, the r_squared-value with, r_squared = 1 - (ss_res / ss_tot) return r_squared def plot_best_fit(data, axis, marker, markercolor): # calculate best fit parameters popt, pcov = best_fit(data) t = data[:,0] f = data[:,1] # plot data points ax.plot(t, f, marker, markerfacecolor=markercolor, markeredgecolor=&quot;black&quot;) # plot best fit line r_squared = calculate_r_squared(data, popt) labeltext = r&quot;$f_c=$ {:.2f}, $f_0=$ {:.2f}, $ beta=$ {:.2f}, $R^2=$ {:.2f}&quot;.format(popt[0],popt[1],popt[2], r_squared) ax.plot(t, horton(t-t[0], *popt), color=markercolor, label=labeltext) fig, ax = plt.subplots(figsize=(10,7)) plot_best_fit(d1, ax, &#39;o&#39;, &quot;tab:red&quot;) plot_best_fit(d2, ax, &#39;x&#39;, &quot;tab:blue&quot;) plot_best_fit(d3, ax, &#39;^&#39;, &quot;tab:orange&quot;) plot_best_fit(d4, ax, &#39;d&#39;, &quot;tab:green&quot;) ax.set(xlabel=&quot;time (min)&quot;, ylabel=&quot;infiltration rate (mm/h)&quot;) ax.legend(); . . Make a graph of the infiltration rate and of the runoff, as a function of time. Use any of the four data sets you have. . #collapse-hide fig, ax = plt.subplots(figsize=(10,7)) data = d4 t = data[:, 0] f = data[:, 1] t = np.concatenate([ [0], t]) f = np.concatenate([ [f[0]], f]) runoff = f[0] - f ax.plot(t, f*0 + f[0], ls=&quot;--&quot;, color=&quot;black&quot;, label=&quot;rainfall&quot;) ax.plot(t, f, color=&quot;tab:blue&quot;, lw=3, label=r&quot;infiltration&quot;) ax.plot(t, runoff, color=&quot;tab:orange&quot;, lw=3, label=r&quot;runoff&quot;) ax.set(xlabel=&quot;Time (min)&quot;, ylabel=r&quot;Rate (mm h$^{-1}$)&quot;) ax.legend(loc=&quot;lower right&quot;); . . Green &amp; Ampt . $$f = frac{A}{F} + B$$ . where . $A = K_ text{sat} cdot| psi_f| cdot left( phi - theta_0 right)$ | $B= K_ text{sat}$ | . Write a function that calculates the cumulative of the infiltration rate. . $$ F(t) = int_0^t f(t) text{ d}t $$Use numpy&#39;s trapz function, that implements the &quot;trapezoidal rule&quot; . . #collapse-hide def cumulative_F(t, f): F = np.array([0]) t = t/60 # convert minute to hour for i in np.arange(2,len(t)+1): area = np.trapz(f[:i], t[:i]) F = np.concatenate([F, [area]]) return F fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,7)) t, f = d1[:,0], d1[:,1] F = cumulative_F(t, f) ax1.plot(t, f, label=&quot;f, rate&quot;) ax2.plot(t, F, label=&quot;F, cumulative&quot;) ax1.set(xlabel=&quot;t (min)&quot;, ylabel=&quot;f (mm/h)&quot;) ax2.set(xlabel=&quot;t (min)&quot;, ylabel=&quot;F (mm)&quot;) ax2.yaxis.set_label_position(&quot;right&quot;) . . Plot $f$ as a function of $F$. Try to guess $A$ and $B$ that give reasonable results. . #collapse-hide fig, ax = plt.subplots(figsize=(10,7)) t, f = d1[:,0], d1[:,1] F = cumulative_F(t, f) ax.plot(F, f) A=50; B=30; ax.plot(F, A/F + B, &#39;o&#39;) ax.set(xlabel=&quot;F&quot;, ylabel=&quot;f&quot;) . . /Users/yairmau/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: RuntimeWarning: divide by zero encountered in true_divide . [Text(0.5, 0, &#39;F&#39;), Text(0, 0.5, &#39;f&#39;)] . Use the curve_fit to find the optimal values for $A$ and $B$. . #collapse-hide def G_and_A(F, A, B): return A/F + B popt, pcov = curve_fit(f=G_and_A, # model function xdata=F[1:], # x data ydata=f[1:], # y data p0=(50, 30), # initial guess of the parameters ) # popt, pcov = curve_fit(G_and_A, F[1:], f[1:], p0=(50, 30)) # p0 = initial guess print(popt) fig, ax = plt.subplots(figsize=(10,7)) ax.plot(F, f) ax.plot(F[1:], popt[0]/F[1:] + popt[1], &#39;o&#39;) ax.set(xlabel=&quot;F&quot;, ylabel=&quot;f&quot;) . . [24.12368526 36.34242813] . [Text(0.5, 0, &#39;F&#39;), Text(0, 0.5, &#39;f&#39;)] . Homework . Go to Soil Texture Calculator, estimate the texture of &quot;standard soil&quot; in Nassif &amp; Wilson, 1975. .",
            "url": "https://yairmau.github.io/jupyter/2020/02/04/infiltration-exercises.html",
            "relUrl": "/jupyter/2020/02/04/infiltration-exercises.html",
            "date": " ‚Ä¢ Feb 4, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Evapotranspiration - lecture",
            "content": "Dingman, &quot;Physical Hydrology&quot;, chapter 6. . Globally, about 62% of the precipitation that falls on the continents is evapotranspirated, amounting to 73 thousand km$^3$/yr. Of this, about 42% (29 thousand km$^3$/yr) is transpiration, and about 3% is open-water evaporation. Most of the remainder is interception loss; soil evaporation is a minor component of the total. . . . Potential Evapotranspiration . Potential Evapotranspiration (PET) is the rate at which evapotranspiration would occur from a large area completely and uniformly covered with growing vegetation with access to an unlimited supply of soil water and without advection or heat-storage effects. . Several characteristics of a vegetative surface have a strong influence on ET rate. a) the albedo of the surface, which determines the net radiation; b) the maximum leaf conductance; c) the atmospheric conductance, largely determined by vegetation height; d) presence or absence of intercepted water. . Reference-Crop Evapotranspiration . Reference-crop evapotranspiration (RET) is the amount of water transpired by a short green crop, completely shading the ground, of uniform height, and never short of water. . The magnitude of PET is often calculated from meteorological data collected under conditions in which the actual ET rate is less than the potential rate. If ET had been occurring at the potential rate, the latent- and sensible-heat exchanges between air and the surface, and hence the air temperature and humidity, would have been considerably different. (Brutsaert 1982) . . Review of methods . There are a variety of ways to estimate evaporative flux in nature. The following table categorizes each method based on the data that must be acquired to apply it: . . These methods also vary in the timescales in which they are relevant, typically in correlation with the variety of data needed: . Thornthwaite and SCS Blaney-Criddle: monthly or seasonal estimations (minimal data) | Jensen-Haise: 5-day estimates (good enough timescale and data for irrigation scheduling) | Penman: daily estimates | Penman-Monteith: hourly estimates (requires a lot of data) | . . Thornthwaite . Source: Ward &amp; Trimble, &quot;Environmental Hydrology&quot;, 2nd Edition, pages 107-108. . Thornthwaite (1948) developed an equation to predict monthly evapotranspiration from mean monthly tempera- ture and latitude data (Equation 4.27). The small amount of data needed is attractive because often ET needs to be predicted for sites where few weather data are available. Based on what we know about ET, we should be skeptical about the general applicability of such a simple equation. Thornthwaite (1948) was not satisfied with the proposed approach: ‚ÄúThe mathematical development is far from satisfactory. It is empirical. ... The chief obstacle at present to the development of a rational equation is the lack of understanding of why potential ET corresponding to a given temperature is not the same everywhere.‚Äù Taylor and Ashcroft (1972), as cited in Skaggs (1980), provided insight into the answer to Thornthwaite‚Äôs ques- tion. They said: . This equation, being based entirely upon a temperature relationship, has the disadvantage of a rather flimsy phys- ical basis and has only weak theoretical justification. Since temperature and vapor pressure gradients are mod- ified by the movement of air and by the heating of the soil and surroundings, the formula is not generally valid, but must be tested empirically whenever the climate is appreciably different from areas in which it has been tested. ... In spite of these shortcomings, the method has been widely used. Because it is based entirely on temper- ature data that are available in a large number of localities, it can be applied in situations where the basic data of the Penman method are not available. . M.E. Jensen et al. (1990) warn that Thornthwaite‚Äôs method is generally only applicable to areas that have climates similar to that of the east central U.S., and it is not applicable to arid and semiarid regions. . Thornthwaite (1948) found that evapotranspiration could be predicted from an equation of the form . $$ begin{equation} E = 16 left[ frac{10 ,T^ text{monthly mean}}{I} right]^a, end{equation} $$where $$ begin{equation} I = sum_{i=1}^{12} left[ frac{T_i^ text{monthly mean}}{5} right]^{1.514}, end{equation} $$ and $$ begin{align} a &amp;= 6.75 times 10^{-7}I^3 &amp;- 7.71 times 10^{-5}I^2 nonumber &amp;+ 1.792 times 10^{-2}I nonumber &amp;+ 0.49239 nonumber end{align} $$ . $E$ is the monthly potential ET (mm) | $T_ text{monthly mean}$ is the mean monthly temperature in ¬∞C | $I$ is a heat index | $a$ is a location-dependent coefficient | . . Penman . Sources: Brutsaert, &quot;Hydrology: An Introduction&quot;, pages 123-127. Ward &amp; Trimble, &quot;Environmental Hydrology&quot;, 2nd Edition, subsections 4.5.2, 4.5.3, 4.5.5, 4.6.6. Allen et al. (1998), &quot;Crop evapotranspiration - Guidelines for computing crop water requirements - FAO Irrigation and drainage paper 56&quot; . The Penman model is almost entirely a theory-based formula for predicting evaporative flux. It can run on a much finer timescale, and requires a much wider variety of data than most models. In addition to temperature, the Penman functions on measurements of radiation, wind speed, elevation above sea level, vapor-pressure deficit, and heat flux density to the ground. The potential ET (in mm d$^{-1}$) is given by: . $$ begin{equation} E = frac{1}{ lambda} left[ frac{ Delta}{ Delta+ gamma}Q_{ne}+ frac{ gamma}{ Delta+ gamma}E_A right], end{equation} $$where $Q_n$ is the available energy flux density . $$ begin{equation} Q_n = R_n - G, end{equation} $$and $E_A$ is the drying power of the air . $$ begin{equation} E_A = 6.43 cdot f(u) cdot text{VPD}. end{equation} $$The constituents of the equations above are . $E$: potential evapotranspiration (mm d$^{-1}$) | $ Delta$: slope of the saturation water vapor pressure curve (kPa ¬∞C$^{-1}$) | $ gamma$: psychrometric constant (kPA ¬∞C$^{-1}$) | $ lambda$: latent heat of vaporization (MJ kg$^{-1}$) | $R_n$: net radiation (MJ m$^{-2} d^{-1}$) | $G$: heat flux density to the ground (MJ m$^{-2} d^{-1}$) | $f(u)$: wind function (dimensionless) | VPD: vapor pressure deficit (kPa) | . and the number 6.43 adjusts the units of $E_A$ so it is in MJ m$^{-2} d^{-1}$. In what follows, we will further discuss these constituents. . Psychrometric Constant . The psychrometric constant $ gamma$ (kPA ¬∞C$^{-1}$) relates the partial pressure of water in air to the air temperature: . $$ begin{equation} gamma = frac{c_p , P}{ lambda cdot MW_ text{ratio}} end{equation} $$$$ begin{equation} P = 101.3-0.01055 H end{equation} $$ . $$ begin{equation} lambda = 2.501 - 2.361 times 10^{-3} ,T end{equation} $$ . $MW_ text{ratio}=0.622$: ratio molecular weight of water vapor/dry air | $P$: atmospheric pressure (kPa). Can be either measured or inferred from station height above sea level (m). | $ lambda$: latent heat of water vaporization (MJ kg$^{-1}$) | $c_p=0.001013$: specific heat capacity of moist air (MJ kg$^{-1}$ ¬∞C$^{-1}$) | . Net Radiation . Source: Ward &amp; Trimble, &quot;Environmental Hydrology&quot;, 2nd Edition, page 99. . $R_n$ (MJ m$^{-2} d^{-1}$) is net radiation, the balance between net short wave $R_s$ and the long wave $R_b$ components of the radiation: . $$R_n = (1- alpha)R_s ! ! downarrow -R_b ! ! uparrow,$$ . where $ alpha$ (dimensionless) is the albedo. The net outgoing thermal radiation $R_b$ is given by . $$R_b = left( a frac{R_s}{R_{so}+b} right)R_{bo},$$ . where $R_{so}$ is the solar radiation on a cloudless day, and it depends on latitude and day of the year. $R_{bo}$ is given by . $$R_{bo} = epsilon , sigma , T^4_{Kelvin},$$ . where $ sigma=4.903 times 10^{-9}$ MJ m$^{-2}$ d$^{-1}$ K$^{-4}$, and $ epsilon$ is net net emissivity: . $$ epsilon=-0.02+0.261 exp left(-7.77 times10^{-4}T_{Celcius}^2 right).$$ . The parameters $a$ and $b$ are determined for the climate of the area: . $a=1.0$, $b=0.0$ for humid areas, | $a=1.2$, $b=-0.2$ for arid areas, | $a=1.1$, $b=-0.1$ for semihumid areas. | . We can find below a table for $R_{so}$, from Ward &amp; Trimble, &quot;Environmental Hydrology&quot;, 2nd Edition, page 100. . Heat Flux Density to the Ground . The heat flux density to the ground $G$ (MJ m$^{-2} d^{-1}$) can be calculated using . $$ begin{equation} G = 4.2 frac{T_{i+1}-T_{i-1}}{ Delta t}, end{equation} $$where $ Delta t$ is the time in days between midpoints of time periods $i+1$ and $i‚àí1$, and $T$ is the air temperature (¬∞C). This expression is really a finite differences implementation of a time derivative: . $$ displaystyle frac{ text{d}T}{ text{d}t} = lim_{ Delta t rightarrow 0} frac{T(t+ Delta t) - T(t- Delta t)}{2 Delta t}. $$Later on, we will take advantage of numpy&#39;s gradient function to calculate $G$. . Vapor Pressure . from: Ward &amp; Trimble, &quot;Environmental Hydrology&quot;, 2nd Edition, page 95. . The Vapor Pressure Deficit (VPD, in kPa) is the difference between saturation vapor pressure $e_s$ and actual vapor pressure $e_d$: . $$ text{VPD} = e_s - e_d.$$ . For temperatures ranging from 0 to 50 ¬∞C, the saturation vapor pressure can be calculated with . $$ begin{equation} e_s = exp left[ frac{16.78 , T -116.9}{T+237.3} right], end{equation} $$and the actual vapor pressure is given by . $$ begin{equation} e_d = e_s frac{RH}{100}, end{equation} $$where $RH$ is the relative humidity (%), and the temperature $T$ in the equations above is in degrees Celcius. . We can see below a graph of $e_s(T)$ (Ward &amp; Trimble, &quot;Environmental Hydrology&quot;, 2nd Edition, page 96) . . The factor $ Delta$ is the slope of $e_s(T)$. See the figure below from Brutsaert, where the saturation vapor pressure is called $e^*$ (Brutsaert, &quot;Hydrology: An Introduction&quot;, page 28)): . . There are a few ways of defining the function for $ Delta(T)$ (kPa ¬∞C$^{-1}$). Ward &amp; Trimble give the following: . $$ begin{equation} Delta = 0.200 cdot (0.00738 ,T + 0.8072)^7 - 0.000116, end{equation} $$while differentiating the exponential expression given before yields: . $$ begin{equation} Delta = frac{ text{d} e_s}{ text{d}T} = e_s(T) cdot frac{4098.79}{(T+237.3)^2}. end{equation} $$ Wind Function . Source: (Ward &amp; Trimble, &quot;Environmental Hydrology&quot;, 2nd Edition, page 108) . $$ begin{equation} f(u) = 0.26(1.0 + 0.54 , u_2) end{equation} $$ Meaning of &quot;potential&quot; evapotranspiration . Crop Coefficient . $$ E_{t} = k_c E_{tr, , tp} $$ . $E_{t}=$ actual ET $k_c=$ crop coefficient $E_{tr}=$ reference crop ET $E_{tp}=$ potential ET . . Pitfalls . Different books and papers will present slightly different versions of the Penman equation. Basically, they differ in the units they use for the various components, and one should be vary aware of what inputs any given equation is expecting to get. .",
            "url": "https://yairmau.github.io/jupyter/2020/02/03/evapotranspiration-lecture.html",
            "relUrl": "/jupyter/2020/02/03/evapotranspiration-lecture.html",
            "date": " ‚Ä¢ Feb 3, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "Evapotranspiration - exercises",
            "content": "We will calculate the evapotranspiration using two methods: Thornthwaite and Penman. . Download data from the IMS . Go to the Israel Meteorological Service website, and download the following data: . hourly data on the first page, choose all options and press continue. | on the next page, choose the following date range: 01/01/2020 to 01/01/2021, then press continue. | Choose station Bet Dagan (◊ë◊ô◊™ ◊ì◊í◊ü 2523), then select (◊ë◊ó◊®), then continue. | Choose option &quot;by station&quot; (◊ú◊§◊ô ◊™◊ó◊†◊ï◊™), then produce report. | Download report as csv, call it &quot;bet-dagan-3h.csv&quot;. | . | daily data on the first page, choose all options and press continue. | on the next page, choose the following date range: 01/01/2020 to 01/01/2021, then press continue. | Choose station Bet Dagan Meuyeshet (◊ë◊ô◊™ ◊ì◊í◊ü ◊û◊ê◊ï◊ô◊©◊™ 2520), then select (◊ë◊ó◊®), then continue. | Choose option &quot;by station&quot; (◊ú◊§◊ô ◊™◊ó◊†◊ï◊™), then produce report. | Download report as csv, call it &quot;bet-dagan-day-pan.csv&quot;. | . | radiation data on the first page, choose all options, then on the bottom right option &quot;radiation&quot; (◊ß◊®◊ô◊†◊î), choose kJ/m2, and then press continue. | on the next page, choose the following date range: 01/01/2020 to 01/01/2021, then press continue. | Choose station Bet Dagan Krina (◊ë◊ô◊™ ◊ì◊í◊ü ◊ß◊®◊ô◊†◊î 2524), then select (◊ë◊ó◊®), then continue. | Choose option &quot;by station&quot; (◊ú◊§◊ô ◊™◊ó◊†◊ï◊™), then produce report. | Download report as csv, call it &quot;bet-dagan-radiation.csv&quot;. | . | Import relevant packages . #collapse-hide import matplotlib.pyplot as plt import matplotlib import numpy as np import pandas as pd from pandas.plotting import register_matplotlib_converters register_matplotlib_converters() # datetime converter for a matplotlib import seaborn as sns sns.set(style=&quot;ticks&quot;, font_scale=1.5) . . import hourly data . #collapse-hide df = pd.read_csv(&#39;bet-dagan-3h.csv&#39;, encoding = &#39;unicode_escape&#39;, na_values=[&quot;-&quot;]) # find out what hebrew gibberish means: http://www.pixiesoft.com/flip/ name_conversion_dictionary = {&quot;√π√≠ √∫√ß√∞√§&quot;: &quot;station name&quot;, &quot;√Æ√±√¥√∏ √∫√ß√∞√§&quot;: &quot;station number&quot;, &quot;√∫√†√∏√©√™&quot;: &quot;Date&quot;, &quot;√π√≤√§-LST&quot;: &quot;LST time&quot;, &quot;√®√Æ√¥√∏√®√•√∏√§(C¬∞)&quot;: &quot;T&quot;, &quot;√®√Æ√¥√∏√®√•√∏√§ √¨√ß√§(C¬∞)&quot;: &quot;wet-bulb temperature (¬∞C)&quot;, &quot;√®√Æ√¥√∏√®√•√∏√∫ √∞√∑√•√£√∫ √§√®√¨(C¬∞)&quot;: &quot;dew_point_T&quot;, &quot;√¨√ß√•√∫ √©√ß√±√©√∫(%)&quot;: &quot;relative humidity (%)&quot;, &quot;√Æ√§√©√∏√•√∫ √§√∏√•√ß(m/s)&quot;: &quot;wind_speed&quot;, &quot;√´√©√•√•√Ø √§√∏√•√ß(√Æ√≤√¨√•√∫)&quot;: &quot;wind direction (degrees)&quot;, &quot;√¨√ß√µ √°√¢√•√°√§ √§√∫√ß√∞√§(hPa)&quot;: &quot;Pressure&quot;, &quot;√¨√ß√µ √°√¢√•√°√§ √¥√∞√© √§√©√≠(hPa)&quot;: &quot;pressure at sea level (hPa)&quot;, &quot;&quot;&quot;√§√∫√†√£√•√∫ √©√•√Æ√©√∫ √Æ√¢√©√¢√©√∫ √±√•√¢ √†&#39;(√Æ&quot;√Æ)&quot;&quot;&quot;: &quot;pan evaporation (mm)&quot;, &quot;√±√•√¢ √∑√∏√©√∞√§()&quot;: &quot;radiation type&quot;, } # units # T = temperature (¬∞C) # dew_point_T = dew point temperature (¬∞C) # wind_speed = wind speed (m/s) # Pressure = pressure at station height (hPa = 0.1 kPa) df = df.rename(columns=name_conversion_dictionary) df[&#39;timestamp&#39;] = df[&#39;Date&#39;] + &#39; &#39; + df[&#39;LST time&#39;] df[&#39;timestamp&#39;] = pd.to_datetime(df[&#39;timestamp&#39;], dayfirst=True) df = df.set_index(&#39;timestamp&#39;) df . . station name station number Date LST time T wet-bulb temperature (¬∞C) dew_point_T relative humidity (%) wind_speed wind direction (degrees) ... pressure at sea level (hPa) √´√Æ√•√∫ √≤√∞√∞√©√≠ √´√•√¨√¨√∫(√∑√•√£) √´√Æ√•√∫ √≤√∞√∞√©√≠ √∞√Æ√•√´√©√≠(√∑√•√£) √¢√•√°√§ √°√±√©√± √≤√∞√∞√©√≠ √∞√Æ√•√´√©√≠(√∑√•√£) √±√•√¢ √§√≤√∞√∞√©√≠ √§√∞√Æ√•√´√©√≠(√∑√•√£) √±√•√¢ √§√≤√∞√∞√©√≠ √§√°√©√∞√•√∞√©√©√≠(√∑√•√£) √±√•√¢ √§√≤√∞√∞√©√≠ √§√¢√°√•√§√©√≠(√∑√•√£) √Æ√¶√¢ √†√•√•√©√∏ √∞√•√´√ß√©(√∑√•√£) √Æ√¶√¢ √†√•√•√©√∏ √π√ß√¨√≥(√∑√•√£) √∏√†√•√∫ √†√¥√∑√©√∫(√∑√•√£) . timestamp . 2020-01-01 02:00:00 √°√©√∫ √£√¢√Ø ... | 2523 | 01-01-2020 | 02:00 | 7.9 | 7.2 | 6.4 | 90 | 1.7 | 117.0 | ... | 1018.8 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2020-01-01 05:00:00 √°√©√∫ √£√¢√Ø ... | 2523 | 01-01-2020 | 05:00 | 7.5 | 7.0 | 6.4 | 93 | 1.2 | 116.0 | ... | 1018.1 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2020-01-01 08:00:00 √°√©√∫ √£√¢√Ø ... | 2523 | 01-01-2020 | 08:00 | 8.6 | 8.3 | 8.0 | 96 | 1.1 | 107.0 | ... | 1018.2 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2020-01-01 11:00:00 √°√©√∫ √£√¢√Ø ... | 2523 | 01-01-2020 | 11:00 | 15.9 | 13.1 | 10.6 | 71 | 2.4 | 196.0 | ... | 1017.4 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2020-01-01 14:00:00 √°√©√∫ √£√¢√Ø ... | 2523 | 01-01-2020 | 14:00 | 18.1 | 14.0 | 10.4 | 61 | 2.8 | 264.0 | ... | 1015.3 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2021-01-31 11:00:00 √°√©√∫ √£√¢√Ø ... | 2523 | 31-01-2021 | 11:00 | 19.0 | 13.7 | 8.9 | 52 | 5.6 | 235.0 | ... | 1017.3 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2021-01-31 14:00:00 √°√©√∫ √£√¢√Ø ... | 2523 | 31-01-2021 | 14:00 | 19.2 | 14.7 | 11.0 | 59 | 4.6 | 252.0 | ... | 1016.7 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2021-01-31 17:00:00 √°√©√∫ √£√¢√Ø ... | 2523 | 31-01-2021 | 17:00 | 18.2 | 14.8 | 12.2 | 68 | 0.8 | 203.0 | ... | 1017.0 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2021-01-31 20:00:00 √°√©√∫ √£√¢√Ø ... | 2523 | 31-01-2021 | 20:00 | 13.1 | 12.3 | 11.7 | 91 | 1.2 | 79.0 | ... | 1018.2 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2021-01-31 23:00:00 √°√©√∫ √£√¢√Ø ... | 2523 | 31-01-2021 | 23:00 | 10.8 | 10.6 | 10.3 | 97 | 1.7 | 111.0 | ... | 1018.9 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3172 rows √ó 21 columns . import daily data with pan evaporation . #collapse-hide df2 = pd.read_csv(&#39;bet-dagan-day-pan.csv&#39;, encoding = &#39;unicode_escape&#39;, na_values=[&quot;-&quot;]) df2 = df2.rename(columns=name_conversion_dictionary) df2[&#39;Date&#39;] = pd.to_datetime(df2[&#39;Date&#39;], dayfirst=True) df2 = df2.set_index(&#39;Date&#39;) df2 . . station name station number √®√Æ√¥√∏√®√•√∏√∫ √Æ√∑√±√©√Æ√•√≠(C¬∞) √®√Æ√¥√∏√®√•√∏√∫ √Æ√©√∞√©√Æ√•√≠(C¬∞) √®√Æ√¥√∏√®√•√∏√∫ √Æ√©√∞√©√Æ√•√≠ √¨√©√£ √§√∑√∏√∑√≤(C¬∞) √Æ√π√™ √¶√§√©√∏√∫ √π√Æ√π(√£√∑√•√∫) pan evaporation (mm) √∑√•√£ √§√∫√†√£√•√∫ √©√•√Æ√©√∫() . Date . 2020-01-01 √°√©√∫ √£√¢√Ø √Æ√†√•√©√π√∫ ... | 2520 | NaN | NaN | NaN | NaN | 0.8 | 0.0 | . 2020-01-02 √°√©√∫ √£√¢√Ø √Æ√†√•√©√π√∫ ... | 2520 | NaN | NaN | NaN | NaN | NaN | NaN | . 2020-01-03 √°√©√∫ √£√¢√Ø √Æ√†√•√©√π√∫ ... | 2520 | NaN | NaN | NaN | NaN | NaN | NaN | . 2020-01-04 √°√©√∫ √£√¢√Ø √Æ√†√•√©√π√∫ ... | 2520 | NaN | NaN | NaN | NaN | NaN | NaN | . 2020-01-05 √°√©√∫ √£√¢√Ø √Æ√†√•√©√π√∫ ... | 2520 | NaN | NaN | NaN | NaN | 2.4 | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 2021-01-27 √°√©√∫ √£√¢√Ø √Æ√†√•√©√π√∫ ... | 2520 | NaN | NaN | NaN | NaN | 2.5 | 0.0 | . 2021-01-28 √°√©√∫ √£√¢√Ø √Æ√†√•√©√π√∫ ... | 2520 | NaN | NaN | NaN | NaN | 1.2 | 0.0 | . 2021-01-29 √°√©√∫ √£√¢√Ø √Æ√†√•√©√π√∫ ... | 2520 | NaN | NaN | NaN | NaN | NaN | NaN | . 2021-01-30 √°√©√∫ √£√¢√Ø √Æ√†√•√©√π√∫ ... | 2520 | NaN | NaN | NaN | NaN | NaN | NaN | . 2021-01-31 √°√©√∫ √£√¢√Ø √Æ√†√•√©√π√∫ ... | 2520 | NaN | NaN | NaN | NaN | 2.6 | 0.0 | . 397 rows √ó 8 columns . import daily data with radiation . #collapse-hide df3 = pd.read_csv(&#39;bet-dagan-radiation.csv&#39;, encoding = &#39;unicode_escape&#39;, na_values=[&quot;-&quot;]) df3 = df3.rename(columns=name_conversion_dictionary) df3[&#39;Date&#39;] = pd.to_datetime(df3[&#39;Date&#39;], dayfirst=True) df3 = df3.set_index(&#39;Date&#39;) df3 = df3.replace({&quot;√©√π√©√∏√§&quot;: &quot;direct&quot;, &quot;√Æ√¥√•√¶√∏√∫&quot;: &quot;diffuse&quot;, &quot;√¢√¨√•√°√†√¨√©√∫&quot;: &quot;global&quot;}) df3[&#39;daily_radiation_MJ_per_m2_per_day&#39;] = df3.iloc[:, 3:].sum(axis=1)/1000 df_radiation = df3.loc[df3[&quot;radiation type&quot;] == &quot;global&quot;, &quot;daily_radiation_MJ_per_m2_per_day&quot;].to_frame() df_radiation . . daily_radiation_MJ_per_m2_per_day . Date . 2020-01-01 10.0296 | . 2020-01-02 4.3128 | . 2020-01-03 11.6748 | . 2020-01-04 1.6452 | . 2020-01-05 6.8544 | . ... ... | . 2021-01-27 12.2652 | . 2021-01-28 7.1640 | . 2021-01-29 7.2936 | . 2021-01-30 9.3276 | . 2021-01-31 13.5468 | . 396 rows √ó 1 columns . Part 1: Thornthwaite estimation . $$ begin{equation} E = 16 left[ frac{10 ,T^ text{monthly mean}}{I} right]^a, end{equation} $$where $$ begin{equation} I = sum_{i=1}^{12} left[ frac{T_i^ text{monthly mean}}{5} right]^{1.514}, end{equation} $$ and $$ begin{align} a &amp;= 6.75 times 10^{-7}I^3 &amp;- 7.71 times 10^{-5}I^2 nonumber &amp;+ 1.792 times 10^{-2}I nonumber &amp;+ 0.49239 nonumber end{align} $$ . $E$ is the monthly potential ET (mm) | $T_ text{monthly mean}$ is the mean monthly temperature in ¬∞C | $I$ is a heat index | $a$ is a location-dependent coefficient | . From df, make a new dataframe, df_th, that stores monthly temperatures means. Use resample function. . #collapse-hide # monthly data df_th = (df[&#39;T&#39;].resample(&#39;MS&#39;) # MS assigns mean to first day in the month .mean() .to_frame() ) # we now add 14 days to the index, so that all monthly data is in the middle of the month # not really necessary, makes plot look better df_th.index = df_th.index + pd.DateOffset(days=14) df_th . . T . timestamp . 2020-01-15 12.484274 | . 2020-02-15 14.046983 | . 2020-03-15 16.439113 | . 2020-04-15 18.512500 | . 2020-05-15 23.166532 | . 2020-06-15 24.600000 | . 2020-07-15 27.353226 | . 2020-08-15 28.090323 | . 2020-09-15 28.462500 | . 2020-10-15 25.120161 | . 2020-11-15 19.308475 | . 2020-12-15 15.916129 | . 2021-01-15 14.123790 | . Calculate $I$, then $a$, and finally $E_p$. Add $E_p$ as a new column in df_th. . #collapse-hide # Preparing &quot;I&quot; for the Thornthwaite equation I = np.sum( (df_th[&#39;T&#39;]/5)**(1.514) ) # Preparing &quot;a&quot; for the Thornthwaite equation a = (+6.75e-7 * I**3 -7.71e-5 * I**2 +1.792e-2 * I + 0.49239) # The final Thornthwaite model for monthly potential ET (mm) df_th[&#39;Ep&#39;] = 16*((10*df_th[&#39;T&#39;]/I)**a) df_th . . T Ep . timestamp . 2020-01-15 12.484274 | 20.163427 | . 2020-02-15 14.046983 | 27.179636 | . 2020-03-15 16.439113 | 40.472053 | . 2020-04-15 18.512500 | 54.671821 | . 2020-05-15 23.166532 | 96.461219 | . 2020-06-15 24.600000 | 112.296873 | . 2020-07-15 27.353226 | 146.898516 | . 2020-08-15 28.090323 | 157.128632 | . 2020-09-15 28.462500 | 162.453109 | . 2020-10-15 25.120161 | 118.406386 | . 2020-11-15 19.308475 | 60.820862 | . 2020-12-15 15.916129 | 37.291178 | . 2021-01-15 14.123790 | 27.557481 | . Plot the Thornthwaite ET that you calculated. . #collapse-hide fig, ax = plt.subplots(1, figsize=(10,7)) ax.plot(df_th[&#39;Ep&#39;]) ax.set(xlabel=&quot;date&quot;, ylabel=r&quot;$E_p$ (mm)&quot;, title=&quot;Thornthwaite potential evapotranspiration&quot;); . . Part 2: Penman . The Penman model is almost entirely a theory based formula for predicting evaporative flux. It can run on a much finer timescale, and requires a much wider variety of data than most models. In addition to temperature, the Penman functions on measurements of radiation, wind speed, elevation above sea level, vapour-pressure deficit, and heat flux density to the ground. . $$ begin{equation} E = frac{1}{ lambda} left[ frac{ Delta}{ Delta+ gamma}Q_{ne}+ frac{ gamma}{ Delta+ gamma}E_A right], end{equation} $$where $Q_n$ is the available energy flux density . $$ begin{equation} Q_n = R_n - G, end{equation} $$and $E_A$ is the drying power of the air . $$ begin{equation} E_A = 6.43 cdot f(u) cdot text{VPD}. end{equation} $$$$ begin{equation} gamma = frac{c_p , P}{ lambda cdot MW_ text{ratio}} end{equation} $$$$ begin{equation} P = 101.3-0.01055 H end{equation} $$ . $$ begin{equation} lambda = 2.501 - 2.361 times 10^{-3} ,T end{equation} $$ . $MW_ text{ratio}=0.622$: ratio molecular weight of water vapor/dry air | $P$: atmospheric pressure (kPa). Can be either measured or inferred from station height above sea level (m). | $ lambda$: latent heat of water vaporization (MJ kg$^{-1}$) | . $$R_n = (1- alpha)R_s ! ! downarrow -R_b ! ! uparrow,$$ . where $ alpha$ (dimensionless) is the albedo. The net outgoing thermal radiation $R_b$ is given by . $$R_b = left( a frac{R_s}{R_{so}+b} right)R_{bo},$$ . where $R_{so}$ is the solar radiation on a cloudless day, and it depends on latitude and day of the year. $R_{bo}$ is given by . $$R_{bo} = epsilon , sigma , T^4_{Kelvin},$$ . where $ sigma=4.903 times 10^{-9}$ MJ m$^{-2}$ d$^{-1}$ K$^{-4}$, and $ epsilon$ is net net emissivity: . $$ epsilon=-0.02+0.261 exp left(-7.77 times10^{-4}T_{Celcius}^2 right).$$ . The parameters $a$ and $b$ are determined for the climate of the area: . $a=1.0$, $b=0.0$ for humid areas, | $a=1.2$, $b=-0.2$ for arid areas, | $a=1.1$, $b=-0.1$ for semihumid areas. . $$ begin{equation} G = 4.2 frac{T_{i+1}-T_{i-1}}{ Delta t} end{equation} $$ . | . $$ text{VPD} = e_s - e_d.$$ . For temperatures ranging from 0 to 50 ¬∞C, the saturation vapor pressure can be calculated with . $$ begin{equation} e_s = exp left[ frac{16.78 , T -116.9}{T+237.3} right], end{equation} $$and the actual vapor pressure is given by . $$ begin{equation} e_d = e_s frac{RH}{100}, end{equation} $$$$ begin{equation} Delta = frac{ text{d} e_s}{ text{d}T} = e_s(T) cdot frac{4098.79}{(T+237.3)^2}. end{equation} $$$$ begin{equation} f(u) = 0.26(1.0 + 0.54 , u_2) end{equation} $$The various components of the equations above are: . $$ begin{equation} Delta = 0.200 cdot (0.00738 ,T + 0.8072)^7 - 0.000116 end{equation} $$ . $$ begin{equation} gamma = frac{c_p , P}{0.622 lambda} end{equation} $$ . $$ begin{equation} P = 101.3-0.01055 H end{equation} $$ . $$ begin{equation} lambda = 2.501 - 2.361 times 10^{-3} ,T end{equation} $$ . $$ begin{equation} f_e(u) = 1.0 + 0.53 , u_2 end{equation} $$ . $$ begin{equation} G = 4.2 frac{T_{i+1}-T_{i-1}}{ Delta t} end{equation} $$ . $$ begin{equation} e_s = exp left[ frac{16.78 , T -116.9}{T+237.3} right] end{equation} $$ . $$ begin{equation} e_d = e_s frac{RH}{100} end{equation} $$ where $ Delta t$ is the time in days between midpoints of time periods $i+1$ and $i‚àí1$, and $T$ is the air temperature (¬∞C). . $ Delta$: slope of the saturation water vapor pressure curve (kPa ¬∞C$^{-1}$) | $ gamma$: psychrometric constant (kPA ¬∞C$^{-1}$) | $c_p=0.001013$: specific heat of water at constant pressure (MJ kg$^{-1}$ ¬∞C$^{-1}$) | $P$: atmospheric pressure (kPa) | $H$: elevation above sea level (m) | $ lambda$: latent heat of vaporization (MJ kg$^{-1}$) | $R_n$: net radiation (MJ m$^{-2} d^{-1}$) | $G$: heat flux density to the ground (MJ m$^{-2} d^{-1}$) | $u_{2}$: wind speed measured 2 m above ground (m s$^{-1}$) | $e_{s} - e_{d}$: vapor pressure deficit (kPa) | $e_{s}$: saturation vapor pressure (kPa) | $e_{d}$: actual vapor pressure (kPa) | . Calculate daily means for the following columns: temperature T, wind speed wind_speed, atmospheric pressure Pressure, and relative humidity relative humidity (%). Remember that pressure data was given in hectopascal, 1 hPa = 0.1 kPa. Store all the calculated values in a new dataframe, called df_pen. . #collapse-hide # Resampling hourly data over same day and taking mean, to obtain daily averages df_pen = (df[&#39;T&#39;].resample(&#39;D&#39;) .mean() .to_frame() ) df_pen[&#39;dew_point&#39;] = (df[&#39;dew_point_T&#39;].resample(&#39;D&#39;) .mean() ) df_pen[&#39;u&#39;] = (df[&#39;wind_speed&#39;].resample(&#39;D&#39;) .mean() ) df_pen[&#39;P&#39;] = (df[&#39;Pressure&#39;].resample(&#39;D&#39;) .mean() )/10 df_pen[&#39;RH&#39;] = (df[&#39;relative humidity (%)&#39;].resample(&#39;D&#39;) .mean() ) df_pen . . T dew_point u P RH . timestamp . 2020-01-01 12.3625 | 9.0625 | 1.5250 | 101.30875 | 81.500 | . 2020-01-02 11.9750 | 9.8250 | 1.9250 | 101.20125 | 87.000 | . 2020-01-03 13.0500 | 4.9750 | 5.1750 | 101.37125 | 58.500 | . 2020-01-04 10.8625 | 6.6875 | 5.5625 | 101.15500 | 78.375 | . 2020-01-05 12.9375 | 9.2125 | 4.5625 | 101.23625 | 79.125 | . ... ... | ... | ... | ... | ... | . 2021-01-27 13.8125 | 8.2375 | 1.8875 | 100.83750 | 72.375 | . 2021-01-28 14.4000 | 10.2250 | 3.5250 | 101.12750 | 76.750 | . 2021-01-29 12.3500 | 7.9125 | 5.0250 | 101.22125 | 75.250 | . 2021-01-30 12.9625 | 7.6500 | 4.4250 | 101.49500 | 71.375 | . 2021-01-31 15.0625 | 8.3125 | 3.7500 | 101.32500 | 66.000 | . 397 rows √ó 5 columns . With average $T$ for every day of the year, we can now calculate daily latent heat of vaporization $ lambda$, the slope of the saturation-vapor pressure-temperature curve $ Delta$, and the heat flux density to the ground $G$. Add each of these to dataframe df_pen. . Calculate also the wind function using the data for wind speed, and add this to df_pen. . #collapse-hide def lambda_latent_heat(T): &quot;&quot;&quot;daily latent heat of vaporization (MJ/kg)&quot;&quot;&quot; return 2.501 - 2.361e-3*T def Delta(T): &quot;&quot;&quot;slope of saturation-vapor curve (kPa/¬∞C)&quot;&quot;&quot; return 0.2000*(0.00738*T + 0.8072)**7 - 0.000116 def G(T): &quot;&quot;&quot;heat flux density to the ground, G (MJ/m2/d)&quot;&quot;&quot; return 4.2*np.gradient(T.values) cp = 0.001013 # (MJ kg‚àí1 ¬∞C‚àí1) df_pen[&#39;lambda&#39;] = lambda_latent_heat(df_pen[&#39;T&#39;]) df_pen[&#39;Delta&#39;] = Delta(df_pen[&#39;T&#39;]) df_pen[&#39;G&#39;] = G(df_pen[&#39;T&#39;]) df_pen[&#39;gamma&#39;] = (cp*df_pen[&#39;P&#39;])/(0.622*df_pen[&#39;lambda&#39;]) df_pen[&#39;f_wind&#39;] = 1.0 + 0.53 * df_pen[&#39;u&#39;] df_pen . . T dew_point u P RH lambda Delta G gamma f_wind . timestamp . 2020-01-01 12.3625 | 9.0625 | 1.5250 | 101.30875 | 81.500 | 2.471812 | 0.094385 | -1.62750 | 0.066750 | 1.808250 | . 2020-01-02 11.9750 | 9.8250 | 1.9250 | 101.20125 | 87.000 | 2.472727 | 0.092300 | 1.44375 | 0.066654 | 2.020250 | . 2020-01-03 13.0500 | 4.9750 | 5.1750 | 101.37125 | 58.500 | 2.470189 | 0.098185 | -2.33625 | 0.066835 | 3.742750 | . 2020-01-04 10.8625 | 6.6875 | 5.5625 | 101.15500 | 78.375 | 2.475354 | 0.086530 | -0.23625 | 0.066553 | 3.948125 | . 2020-01-05 12.9375 | 9.2125 | 4.5625 | 101.23625 | 79.125 | 2.470455 | 0.097554 | 4.25250 | 0.066739 | 3.418125 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2021-01-27 13.8125 | 8.2375 | 1.8875 | 100.83750 | 72.375 | 2.468389 | 0.102551 | 4.77750 | 0.066532 | 2.000375 | . 2021-01-28 14.4000 | 10.2250 | 3.5250 | 101.12750 | 76.750 | 2.467002 | 0.106028 | -3.07125 | 0.066760 | 2.868250 | . 2021-01-29 12.3500 | 7.9125 | 5.0250 | 101.22125 | 75.250 | 2.471842 | 0.094317 | -3.01875 | 0.066691 | 3.663250 | . 2021-01-30 12.9625 | 7.6500 | 4.4250 | 101.49500 | 71.375 | 2.470396 | 0.097694 | 5.69625 | 0.066911 | 3.345250 | . 2021-01-31 15.0625 | 8.3125 | 3.7500 | 101.32500 | 66.000 | 2.465437 | 0.110070 | 8.82000 | 0.066933 | 2.987500 | . 397 rows √ó 10 columns . It&#39;s time to calculate net radiation $R_n$. The monthly mean solar radiation $R_{so}$ for latitude 30 degrees is [17.46, 21.65, 25.96, 29.85, 32.11, 33.20, 32.66, 30.44, 26.67, 22.48, 18.30, 16.04] (MJ m$^{-2}$ d$^{-1}$) (Israel&#39;s latitude is ~ 31 degrees). . Add a new column Rso_monthly to df_pen, where each day has the appropriate $R_{so}$ given by the data above. | Add a new columns Rs with the global radiation data imported in the 3rd file. | . #collapse-hide # Rso: mean solar radiation from a cloudless sky (based on latitude) # MJ/m2/d Rso_monthly = np.array([17.46, 21.65, 25.96, 29.85, 32.11, 33.20, 32.66, 30.44, 26.67, 22.48, 18.30, 16.04]) # create empty columns df_pen[&quot;Rso_monthly&quot;] = &quot;&quot; # every day in the month will have the same values for Rso for i in range(12): df_pen.loc[df_pen.index.month==(i+1), &quot;Rso_monthly&quot;] = Rso_monthly[i] df_pen[&quot;Rs&quot;] = df_radiation[&quot;daily_radiation_MJ_per_m2_per_day&quot;] fig, ax = plt.subplots(1, figsize=(10,7)) ax.plot(df_pen[&#39;Rso_monthly&#39;]) plt.gcf().autofmt_xdate() ax.set_ylabel(r&quot;$R_{so}$ (MJ m$^{-2} d^{-1}$)&quot;) . . Text(0, 0.5, &#39;$R_{so}$ (MJ m$^{-2} d^{-1}$)&#39;) . #collapse-hide middle = pd.date_range(start=&#39;1/1/2020&#39;, periods=13, freq=&#39;MS&#39;) + pd.DateOffset(days=14) new = df_pen.loc[middle, &#39;Rso_monthly&#39;].astype(&#39;float&#39;) new df_i = (pd.DataFrame(data=new, index=new.index) #create the dataframe .resample(&quot;D&quot;) #resample daily .interpolate(method=&#39;time&#39;) #interpolate by time ) fig, ax = plt.subplots(1, figsize=(10,7)) ax.plot(df_i, &#39;o&#39;) ax.plot(df_pen[&#39;Rso_monthly&#39;]) plt.gcf().autofmt_xdate() ax.set_title(&quot;time interpolation&quot;) ax.set_ylabel(r&quot;$R_{so}$ (MJ m$^{-2} d^{-1}$)&quot;) . . Text(0, 0.5, &#39;$R_{so}$ (MJ m$^{-2} d^{-1}$)&#39;) . from: Ward &amp; Trimble, &quot;Environmental Hydrology&quot;, 2nd Edition, page 99. . Calculate $$R_{bo} = epsilon , sigma , T^4_{Kelvin},$$ where $$ epsilon=-0.02+0.261 exp left(-7.77 times10^{-4}T_{Celcius}^2 right),$$ $$ sigma=4.903 times 10^{-9} text{ MJ m$^{-2}$ d$^{-1}$ K$^{-4}$},$$ and $$T_{Kelvin}=T_{Celcius}+273.15$$ | Calculate $$R_b = left( a frac{R_s}{R_{so}+b} right)R_{bo},$$ where for humid areas, $a=1.0$ and $b=0$, | for arid areas, $a=1.2$ and $b=-0.2$, | for semihumid areas, $a=1.1$ and $b=-0.1$ | . | Finally, calculate $$R_n = (1- alpha)R_s ! ! downarrow -R_b ! ! uparrow,$$ where $ alpha= 0.23$ for most green crops with a full cover | $ alpha= 0.04$ for fresh asphalt | $ alpha= 0.12$ for worn-out asphalt | $ alpha= 0.55$ for fresh concrete | . | . Add a new column Rn to df_pen dataframe. . #collapse-hide # Stefan-Boltzmann constant sigma = 4.903e-9 emissivity = -0.02 + 0.261 * np.exp(-7.77e-4 * df_pen[&#39;T&#39;]**2) # Rbo: net longwave radiation for clear skies, otherwise known as diffuse radiation or emitted radiation from the # atmosphere - &#39;how hot is it?&#39; Rbo = emissivity*sigma*((df_pen[&#39;T&#39;]+273.15)**4) # net outgoing long-wave radiation (note: Rs/Rso = proportion of how clear the day is) # for humid areas, a=1.0 and b=0 # for arid areas, a=1.2 and b=-0.2 # for semihumid areas, a=1.1 and b=-0.1 a = 1.2 b = -0.2 Rb = (a*df_pen[&#39;Rs&#39;]/df_pen[&#39;Rso_monthly&#39;] + b)*Rbo # Œ± is the albedo, or short-wave reflectance (dimensionless) alpha = 0.23 # net radiation Rn = (1 - alpha) * df_pen[&#39;Rs&#39;] - Rb # (MJ/m2/d) df_pen[&#39;Rn&#39;] = Rn df_pen . . T dew_point u P RH lambda Delta G gamma f_wind Rso_monthly Rs Rn . timestamp . 2020-01-01 12.3625 | 9.0625 | 1.5250 | 101.30875 | 81.500 | 2.471812 | 0.094385 | -1.62750 | 0.066750 | 1.808250 | 17.46 | 10.0296 | 4.346568 | . 2020-01-02 11.9750 | 9.8250 | 1.9250 | 101.20125 | 87.000 | 2.472727 | 0.092300 | 1.44375 | 0.066654 | 2.020250 | 17.46 | 4.3128 | 2.653905 | . 2020-01-03 13.0500 | 4.9750 | 5.1750 | 101.37125 | 58.500 | 2.470189 | 0.098185 | -2.33625 | 0.066835 | 3.742750 | 17.46 | 11.6748 | 4.854942 | . 2020-01-04 10.8625 | 6.6875 | 5.5625 | 101.15500 | 78.375 | 2.475354 | 0.086530 | -0.23625 | 0.066553 | 3.948125 | 17.46 | 1.6452 | 1.871722 | . 2020-01-05 12.9375 | 9.2125 | 4.5625 | 101.23625 | 79.125 | 2.470455 | 0.097554 | 4.25250 | 0.066739 | 3.418125 | 17.46 | 6.8544 | 3.415474 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2021-01-27 13.8125 | 8.2375 | 1.8875 | 100.83750 | 72.375 | 2.468389 | 0.102551 | 4.77750 | 0.066532 | 2.000375 | 17.46 | 12.2652 | 5.060995 | . 2021-01-28 14.4000 | 10.2250 | 3.5250 | 101.12750 | 76.750 | 2.467002 | 0.106028 | -3.07125 | 0.066760 | 2.868250 | 17.46 | 7.1640 | 3.534995 | . 2021-01-29 12.3500 | 7.9125 | 5.0250 | 101.22125 | 75.250 | 2.471842 | 0.094317 | -3.01875 | 0.066691 | 3.663250 | 17.46 | 7.2936 | 3.537119 | . 2021-01-30 12.9625 | 7.6500 | 4.4250 | 101.49500 | 71.375 | 2.470396 | 0.097694 | 5.69625 | 0.066911 | 3.345250 | 17.46 | 9.3276 | 4.152687 | . 2021-01-31 15.0625 | 8.3125 | 3.7500 | 101.32500 | 66.000 | 2.465437 | 0.110070 | 8.82000 | 0.066933 | 2.987500 | 17.46 | 13.5468 | 5.513874 | . 397 rows √ó 13 columns . Calculate the vapor pressure deficit, VPD, add a new column to df_pen. . $$e_d = e_s cdot frac{RH}{100}$$ . $$e_s = exp left( frac{16.78 ,T-116.9}{T+237.3} right)$$ . #collapse-hide # vapor pressure deficit = VPD def vp_sat(T): return np.exp((16.78*T - 116.9)/(T + 237.3)) df_pen[&#39;es&#39;] = vp_sat(df_pen[&#39;T&#39;]) df_pen[&#39;ed&#39;] = df_pen[&#39;es&#39;] * df_pen[&#39;RH&#39;] / 100 df_pen[&#39;VPD&#39;] = df_pen[&#39;es&#39;] - df_pen[&#39;ed&#39;] df_pen . . T dew_point u P RH lambda Delta G gamma f_wind Rso_monthly Rs Rn es ed VPD . timestamp . 2020-01-01 12.3625 | 9.0625 | 1.5250 | 101.30875 | 81.500 | 2.471812 | 0.094385 | -1.62750 | 0.066750 | 1.808250 | 17.46 | 10.0296 | 4.346568 | 1.437148 | 1.171276 | 0.265872 | . 2020-01-02 11.9750 | 9.8250 | 1.9250 | 101.20125 | 87.000 | 2.472727 | 0.092300 | 1.44375 | 0.066654 | 2.020250 | 17.46 | 4.3128 | 2.653905 | 1.400935 | 1.218813 | 0.182122 | . 2020-01-03 13.0500 | 4.9750 | 5.1750 | 101.37125 | 58.500 | 2.470189 | 0.098185 | -2.33625 | 0.066835 | 3.742750 | 17.46 | 11.6748 | 4.854942 | 1.503424 | 0.879503 | 0.623921 | . 2020-01-04 10.8625 | 6.6875 | 5.5625 | 101.15500 | 78.375 | 2.475354 | 0.086530 | -0.23625 | 0.066553 | 3.948125 | 17.46 | 1.6452 | 1.871722 | 1.301383 | 1.019959 | 0.281424 | . 2020-01-05 12.9375 | 9.2125 | 4.5625 | 101.23625 | 79.125 | 2.470455 | 0.097554 | 4.25250 | 0.066739 | 3.418125 | 17.46 | 6.8544 | 3.415474 | 1.492399 | 1.180860 | 0.311538 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2021-01-27 13.8125 | 8.2375 | 1.8875 | 100.83750 | 72.375 | 2.468389 | 0.102551 | 4.77750 | 0.066532 | 2.000375 | 17.46 | 12.2652 | 5.060995 | 1.580054 | 1.143564 | 0.436490 | . 2021-01-28 14.4000 | 10.2250 | 3.5250 | 101.12750 | 76.750 | 2.467002 | 0.106028 | -3.07125 | 0.066760 | 2.868250 | 17.46 | 7.1640 | 3.534995 | 1.641414 | 1.259785 | 0.381629 | . 2021-01-29 12.3500 | 7.9125 | 5.0250 | 101.22125 | 75.250 | 2.471842 | 0.094317 | -3.01875 | 0.066691 | 3.663250 | 17.46 | 7.2936 | 3.537119 | 1.435967 | 1.080565 | 0.355402 | . 2021-01-30 12.9625 | 7.6500 | 4.4250 | 101.49500 | 71.375 | 2.470396 | 0.097694 | 5.69625 | 0.066911 | 3.345250 | 17.46 | 9.3276 | 4.152687 | 1.494843 | 1.066944 | 0.427899 | . 2021-01-31 15.0625 | 8.3125 | 3.7500 | 101.32500 | 66.000 | 2.465437 | 0.110070 | 8.82000 | 0.066933 | 2.987500 | 17.46 | 13.5468 | 5.513874 | 1.713106 | 1.130650 | 0.582456 | . 397 rows √ó 16 columns . Now that all variables have been defined, daily E_penman can be calculated. . $$ begin{equation} E_{tp} = frac{ Delta}{ Delta+ gamma}Q_{ne}+ frac{ gamma}{ Delta+ gamma}E_A end{equation} $$$Q_n$ is the available energy flux density: . $$ begin{equation} Q_n = R_n - G, end{equation} $$and $E_A$ is the drying power of the air: . $$ begin{equation} E_A = f_e(u) cdot text{VPD} end{equation} $$Add a new column E_penman to df_pen. . #collapse-hide def E_penman(df): T = df[&#39;T&#39;] Delta = df[&#39;Delta&#39;] gamma = df[&#39;gamma&#39;] Rn = df[&#39;Rn&#39;] G = df[&#39;G&#39;] EA = 6.43*df[&#39;f_wind&#39;] * df[&#39;VPD&#39;] lambd = df[&#39;lambda&#39;] return ((Delta / (Delta + gamma))*(Rn - G) + ((gamma / (Delta + gamma))*EA)) / lambd # daily_data df_pen[&#39;E_penman&#39;] = E_penman(df_pen) fig, ax = plt.subplots(1, figsize=(10,7)) ax.plot(df_pen[&#39;E_penman&#39;]) plt.gcf().autofmt_xdate() ax.set_ylabel(r&quot;$ET_{penman}$ (mm d$^{-1}$)&quot;) . . Text(0, 0.5, &#39;$ET_{penman}$ (mm d$^{-1}$)&#39;) . Make a plot with the following: . the Penman (daily) estimate of the potential evapotranspiration. | the Thornthwaite (monthly) estimate of the potential ET. | daily evaporation pan data. | #collapse-hide fig, ax = plt.subplots(1, 1, figsize=(10,7)) ax.plot(df_pen[&#39;E_penman&#39;], color=&quot;tab:red&quot;, label=&quot;Penman&quot;, linewidth=2) ax.plot(df_th[&#39;Ep&#39;]/30, color=&quot;tab:blue&quot;, label=&quot;Thornthwaite&quot;, linewidth=2) ax.plot(1*df2[&#39;pan evaporation (mm)&#39;], color=&quot;black&quot;, label=&quot;pan&quot;, linewidth=2) ax.set(xlabel=&quot;date&quot;, ylabel=&quot;evaporation (mm)&quot;) ax.legend(); . . Plot the mean temperatures used in the Penman calculation (daily mean) and in the Thornthwaite calculation (monthly mean). . #collapse-hide fig, ax = plt.subplots(1, 1, figsize=(10,7)) ax.plot(df_pen[&#39;T&#39;], color=&quot;tab:blue&quot;, label=&quot;Penman&quot;, linewidth=2) ax.plot(df_th[&#39;T&#39;], color=&quot;tab:orange&quot;, label=&quot;Thornthwaite&quot;, linewidth=2) ax.set(xlabel=&quot;date&quot;, ylabel=&quot;temperature (¬∞C)&quot;) ax.legend(); . .",
            "url": "https://yairmau.github.io/jupyter/2020/02/03/evapotranspiration-exercises.html",
            "relUrl": "/jupyter/2020/02/03/evapotranspiration-exercises.html",
            "date": " ‚Ä¢ Feb 3, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "Assignment 2 - Evapotranspiration",
            "content": "&#128210; instructions . This is where learning happens, not during a lecture. You&#39;ll learn a ton of things by doing them yourself. Much success! üòÑ . Create a Jupyter Notebook called assignment-02-IDNUMBER, where IDNUMBER is your 9-digit ID. This is the file only file we will check. . &#128204; locations and data . Choose two stations with different climates. . Go to NOAA&#39;s Climate Reference Network Data website. The sub-hourly (5-min) data contains information on . air temperature, | precipitation, | global solar radiation, | surface infrared temperature, | relative humidity, | soil moisture and temperature, | wetness, and | 1.5 meter wind speed. | . There is no data on air pressure, so one needs to use the stations coordinates (lat, lon) to find its height above sea level, and from that infer the air pressure. You can use Google Earth or any other means to find the station&#39;s height. . In the Data Access link, choose a year and a station you would like to analyze. If you are not sure where the stations are, find them using the 2-letter state abbreviation and the station name. . Download the following files: . One full year of data for each station. Make sure important data we need to calculate Penman&#39;s ET estimation is available. | The headers file | The documentation file | Make sure you understand what are the units provided for each measurement (see documentation). . &#128736; tasks . Produce potential ET estimates using Thornthwaite&#39;s equation and Penman&#39;s equation. Produce plots of ET as a function of time for each station, comparing the two methods you used. Also, using Penman&#39;s ET estimates, compare the two stations and discuss about their differences/similarities. . You might find interesting things in the data, such as periods of unusually high/low temperatures, radiation, etc. Discuss how these factors might have affected the ET estimates that you calculated. . You will have two weeks to deliver your assignment. You should not hand in a dry document with only figures and code, I&#39;m expecting text before and after each code/graph cell, explaining what you did, why you did it, and how it fits the story you are telling. Don&#39;t forget to put labels on your plot axes, title, legend, etc. . Your Jupyter Notebook should be fully functional: if we press Kernel &gt; Restart &amp; Run All, all the code must work without any errors. . &#127749; presentation . All the assignment must be in one single Jupyter Notebook. Use markdown cells to discuss the analysis and results, and in code cells show all the code you used to produce the figures and data analysis. Leave only the code necessary for your analysis, delete unnecessary lines your wrote while analyzing your data. Don&#39;t forget to comment your code, just like we did during exercise sessions. . You can write in English or in Hebrew, but the text in the figures must be in English. If you choose to write the discussion in Hebrew, be aware that Jupyter Notebooks don&#39;t have native right-to-left language support: . ◊†◊ô◊™◊ü ◊ú◊õ◊™◊ï◊ë ◊ë◊¢◊ë◊®◊ô◊™, ◊ú◊û◊®◊ï◊™ ◊©◊ñ◊î ◊ú◊ê ◊†◊®◊ê◊î ◊õ◊¥◊õ ◊ò◊ï◊ë... . You can use some HTML code to achieve best results in Hebrew. Type the following . &lt;p dir=&quot;rtl&quot; style=&quot;text-align: right;&quot;&gt; ◊¢◊õ◊©◊ô◊ï ◊î◊®◊ë◊î ◊ô◊ï◊™◊® ◊ò◊ï◊ë! &lt;/p&gt; . to get . ◊¢◊õ◊©◊ô◊ï ◊î◊®◊ë◊î ◊ô◊ï◊™◊® ◊ò◊ï◊ë! . If you have many paragraphs in hebrew, do the following: . ◊§◊°◊ß◊î ◊û◊°◊§◊® 1. . ◊§◊°◊ß◊î ◊û◊°◊§◊® 2. . ◊ê◊ù ◊ô◊© ◊ú◊õ◊ù ◊õ◊û◊î ◊§◊°◊ß◊ê◊ï◊™, ◊õ◊ú ◊ê◊ó◊™ ◊û◊î◊ü ◊™◊î◊ô◊î ◊ë◊™◊ï◊ö &quot;dir&quot; ◊û◊©◊ú◊î . In my opinion it is too complicated to write in Hebrew in Jupyter Notebooks, just write in English, your grade will not be affected by typos nor less-than-perfect English proficiency. . &#128175; evaluation . Your assignment will be evaluated according to the following criteria: . 40% Presentation. How the graphs look, labels, general organization, markdown, clean code. | 30% Discussion. This is where you explain what you did, what you found out, etc. | 15% Depth of analysis. You can analyze/explore the data with different levels of complexity, this is where we take that into consideration. | 10% Replicability: Your code runs flawlessly. | 5%: Code commenting. Explain in your code what you are doing, this is good for everyone, especially for yourself! | . &#128666; importing the data . Below you can find an example of how to import the data file provided by NOAA&#39;s Climate Reference Network Data website. You might have to make some adjustments to it. . data_file = &quot;CRNS0101-05-2020-CO_Boulder_14_W.txt&quot; df = pd.read_csv(data_file, header=None, # no headers needed, we&#39;ll do that later delim_whitespace=True, # blank spaces separate between columns na_values=[&quot;-99.000&quot;, &quot;-9999.0&quot;] # substitute these values for missing (NaN) values ) headers = pd.read_csv(&quot;HEADERS_sub_hourly.txt&quot;, # load headers file header=1, # skip the first [0] line delim_whitespace=True ) df.columns = headers.columns # rename df columns with headers columns # LST = local standard time df[&quot;LST_TIME&quot;] = [f&quot;{x:04d}&quot; for x in df[&quot;LST_TIME&quot;]] # time needs padding of zeros, then convert to string df[&#39;LST_DATE&#39;] = df[&#39;LST_DATE&#39;].astype(str) # convert date into string df[&#39;datetime&#39;] = df[&#39;LST_DATE&#39;] + &#39; &#39; + df[&#39;LST_TIME&#39;] # combine date+time into datetime df[&#39;datetime&#39;] = pd.to_datetime(df[&#39;datetime&#39;]) # interpret datetime df = df.set_index(&#39;datetime&#39;) # make datetime the index df .",
            "url": "https://yairmau.github.io/jupyter/2020/02/03/assignment-02-ET.html",
            "relUrl": "/jupyter/2020/02/03/assignment-02-ET.html",
            "date": " ‚Ä¢ Feb 3, 2020"
        }
        
    
  
    
        ,"post21": {
            "title": "Exercises, inter- and intra-annual variability of precipitation",
            "content": "Import relevant packages . #collapse-hide import matplotlib.pyplot as plt import numpy as np import pandas as pd from calendar import month_abbr import seaborn as sns sns.set(style=&quot;ticks&quot;, font_scale=1.5) import urllib.request . . intra-annual variability . Go to NOAA&#39;s National Centers for Environmental Information (NCEI) Climate Data Online: Dataset Discovery . Find station codes in this map. On the left, click on the little wrench next to &quot;Global Summary of the Month&quot;, then click on &quot;identify&quot; on the panel that just opened, and click on a station (purple circle). You will see the station&#39;s name, it&#39;s ID, and the period of record. For example, for Ben-Gurion&#39;s Airport in Israel: BEN GURION, IS STATION ID: ISM00040180 Period of Record: 1951-01-01 to 2020-03-01 . You can download daily or monthly data for each station. Use the function below to download this data to your computer. . #collapse-hide def download_data(station_name, station_code): url_daily = &#39;https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access/&#39; url_monthly = &#39;https://www.ncei.noaa.gov/data/gsom/access/&#39; # download daily data - uncomment the next 2 lines to make this work # urllib.request.urlretrieve(url_daily + station_code + &#39;.csv&#39;, # station_name + &#39;_daily.csv&#39;) # download monthly data urllib.request.urlretrieve(url_monthly + station_code + &#39;.csv&#39;, station_name + &#39;_monthly.csv&#39;) . . Now, choose any station with a period of record longer than 30 years, and download its data: . download_data(&#39;BEN_GURION&#39;, &#39;ISM00040180&#39;) . Load the data into a datafram, and before you continue with the analysis, plot the rainfall data, to see how it looks like. . #collapse-hide download_data(&#39;BEN_GURION&#39;, &#39;ISM00040180&#39;) df = pd.read_csv(&#39;BEN_GURION_monthly.csv&#39;, sep=&quot;,&quot;) # make &#39;DATE&#39; the dataframe index df[&#39;DATE&#39;] = pd.to_datetime(df[&#39;DATE&#39;]) df = df.set_index(&#39;DATE&#39;) plt.plot(df[&#39;PRCP&#39;]) . . [&lt;matplotlib.lines.Line2D at 0x7fb600c72150&gt;] . It doesn&#39;t look great for Ben-Gurion airport, lots of missing data! You might need to choose another station... Download data for Beer Sheva, ID IS000051690. . #collapse-hide download_data(&#39;BEER_SHEVA&#39;, &#39;IS000051690&#39;) df = pd.read_csv(&#39;BEER_SHEVA_monthly.csv&#39;, sep=&quot;,&quot;) # make &#39;DATE&#39; the dataframe index df[&#39;DATE&#39;] = pd.to_datetime(df[&#39;DATE&#39;]) df = df.set_index(&#39;DATE&#39;) plt.plot(df[&#39;PRCP&#39;]) . . [&lt;matplotlib.lines.Line2D at 0x7fb6404ce150&gt;] . That&#39;s much better! We need to aggregate all data from each month, so we can calculate monthly averages. How to do that? . #collapse-hide # choose only the precipitation column df_month = df[&#39;PRCP&#39;] # calculate monthly mean monthly_mean = np.array([]) # empty array month_numbers = np.arange(1,13) month_names = [month_abbr[i] for i in month_numbers] for m in month_numbers: # cycle over months (1, 2, 3, etc) this_month_all_indices = (df_month.index.month == m) # indices in df_month belonging to month m this_month_mean = df_month[this_month_all_indices].mean() # this is the monthly mean monthly_mean = np.append(monthly_mean, this_month_mean) # append . . Now it is time to create a new dataframe with the monthly means. . #collapse-hide df_beersheva = pd.DataFrame({&#39;monthly rainfall (mm)&#39;:monthly_mean, &#39;month names&#39;:month_names, &#39;month number&#39;:month_numbers }) df_beersheva . . monthly rainfall (mm) month names month number . 0 48.743158 | Jan | 1 | . 1 37.347368 | Feb | 2 | . 2 26.551579 | Mar | 3 | . 3 9.038947 | Apr | 4 | . 4 2.735789 | May | 5 | . 5 0.013830 | Jun | 6 | . 6 0.000000 | Jul | 7 | . 7 0.002128 | Aug | 8 | . 8 0.271277 | Sep | 9 | . 9 6.669474 | Oct | 10 | . 10 21.850526 | Nov | 11 | . 11 41.786316 | Dec | 12 | . Plot the data and see if it makes sense. Try to get a figure like this one. . #collapse-hide fig, ax = plt.subplots(figsize=(10,7)) ax.bar(df_beersheva[&#39;month number&#39;], df_beersheva[&#39;monthly rainfall (mm)&#39;]) ax.set(xlabel=&quot;months&quot;, ylabel=&quot;monthly average (mm)&quot;, title=&quot;Beer Sheva&quot;, xticks=df_beersheva[&#39;month number&#39;], xticklabels=df_beersheva[&#39;month names&#39;]); plt.savefig(&quot;hydrology_figures/beersheva_monthly_average.png&quot;) . . Let&#39;s calculate now the Walsh and Lawler Seasonality Index. Write a function that receives a dataframe like the one we have just created, and returns the seasonality index. http://leddris.aegean.gr/ses-parameters/293-rainfall-seasonality.html#:~:text=Rainfall%20seasonality%20index%20is%20a,in%20relation%20to%20water%20availability . $R=$ mean annual precipitation $m_i$ precipitation mean for month $i$ . $$ SI = displaystyle frac{1}{R} sum_{n=1}^{n=12} left| m_i - frac{R}{12} right| $$ . SI Precipitation Regime . &lt;0.19 | Precipitation spread throughout the year | . 0.20-0.39 | Precipitation spread throughout the year, but with a definite wetter season | . 0.40-0.59 | Rather seasonal with a short dry season | . 0.60-0.79 | Seasonal | . 0.80-0.99 | Marked seasonal with a long dry season | . 1.00-1.19 | Most precipitation in &lt; 3 months | . #collapse-hide def walsh_index(df): mi = df[&quot;monthly rainfall (mm)&quot;] R = df[&quot;monthly rainfall (mm)&quot;].sum() SI = np.sum(np.abs(mi - R/12)) / R return SI beersheva_SI = walsh_index(df_beersheva) print(f&quot;Beer Sheva, SI = {beersheva_SI:.2f}&quot;) . . Beer Sheva, SI = 0.97 . interannual variability . Plot monthly rainfall for your station. . #collapse-hide fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,7)) ax1.plot(df[&#39;PRCP&#39;]) ax2.plot(df[&#39;PRCP&#39;][&#39;2010-07-01&#39;:&#39;2015-07-01&#39;]) . . [&lt;matplotlib.lines.Line2D at 0x7fb620dd45d0&gt;] . How to aggregate rainfall accoding to the hydrological year? We use the function resample. . read more about resampling options: https://pandas.pydata.org/pandas-docs/version/0.12.0/timeseries.html#offset-aliases . also, annual resampling can be anchored to the end of specific months: https://pandas.pydata.org/pandas-docs/version/0.12.0/timeseries.html#anchored-offsets . #collapse-hide # annual frequency, anchored 31 December df_year_all = df[&#39;PRCP&#39;].resample(&#39;A&#39;).sum().to_frame() # annual frequency, anchored 01 January df_year_all = df[&#39;PRCP&#39;].resample(&#39;AS&#39;).sum().to_frame() # annual frequency, anchored end of September df_year_all = df[&#39;PRCP&#39;].resample(&#39;A-SEP&#39;).sum().to_frame() # rename &#39;PRCP&#39; column to &#39;rain (mm)&#39; df_year_all.columns = [&#39;rain (mm)&#39;] df_year_all . . rain (mm) . DATE . 1922-09-30 136.6 | . 1923-09-30 144.5 | . 1924-09-30 130.4 | . 1925-09-30 165.3 | . 1926-09-30 188.7 | . ... ... | . 2012-09-30 145.7 | . 2013-09-30 175.3 | . 2014-09-30 259.2 | . 2015-09-30 249.3 | . 2016-09-30 257.6 | . 95 rows √ó 1 columns . You might need to exclude the first or the last line, since their data might have less that 12 months. For example: . #collapse-hide # exclude 1st row df_year = df_year_all.iloc[1:] # exclude last row df_year = df_year_all.iloc[:-1] # exclude both 1st and last rows df_year = df_year_all.iloc[1:-1] df_year . . rain (mm) . DATE . 1923-09-30 144.5 | . 1924-09-30 130.4 | . 1925-09-30 165.3 | . 1926-09-30 188.7 | . 1927-09-30 130.2 | . ... ... | . 2011-09-30 151.6 | . 2012-09-30 145.7 | . 2013-09-30 175.3 | . 2014-09-30 259.2 | . 2015-09-30 249.3 | . 93 rows √ó 1 columns . Calculate the average annual rainfall. Plot annual rainfall for the whole range, together with the average. You should get something like this: . #collapse-hide fig, ax = plt.subplots(figsize=(10,7)) # plot YEARLY precipitation ax.bar(df_year.index, df_year[&#39;rain (mm)&#39;], width=365, align=&#39;edge&#39;, color=&quot;tab:blue&quot;) # plot mean rain_mean = df_year[&#39;rain (mm)&#39;].mean() ax.plot(ax.get_xlim(), [rain_mean]*2, linewidth=3, color=&quot;tab:orange&quot;) ax.set(xlabel=&quot;date&quot;, ylabel=&quot;yearly rainfall (mm)&quot;, title=f&quot;Beer Sheva, mean = {rain_mean:.0f} mm&quot;); # save figure plt.savefig(&quot;hydrology_figures/beersheva_yearly_rainfall_1923_2016.png&quot;) . . Plot a histogram of annual rainfall, with the mean and standard deviation. Calculate the coefficient of variation. Try to plot something like this: . #collapse-hide fig, ax = plt.subplots(figsize=(10,7)) # calculate mean and standard deviation rain_mean = df_year[&#39;rain (mm)&#39;].mean() rain_std = df_year[&#39;rain (mm)&#39;].std() # plot histogram b = np.arange(0, 401, 50) # bins from 0 to 400, width = 50 ax.hist(df_year[&#39;rain (mm)&#39;], bins=b) # plot vertical lines with mean, std, etc ylim = np.array(ax.get_ylim()) ylim[1] = ylim[1]*1.1 ax.plot([rain_mean]*2, ylim, linewidth=3, color=&quot;tab:orange&quot;) ax.plot([rain_mean+rain_std]*2, ylim, linewidth=3, linestyle=&quot;--&quot;, color=&quot;tab:olive&quot;) ax.plot([rain_mean-rain_std]*2, ylim, linewidth=3, linestyle=&quot;--&quot;, color=&quot;tab:olive&quot;) ax.set(ylim=ylim, xlabel=&quot;annual rainfall (mm)&quot;, ylabel=&quot;number of years&quot;, title=f&quot;Beer Sheva, 1922‚Äì2016. Mean={rain_mean:.0f} mm, STD={rain_std:.0f} mm&quot;) ax.text(300, 25, f&quot;CV = {rain_std/rain_mean:.2f}&quot;) plt.savefig(&quot;histogram_beersheva.png&quot;) . . Calculate the mean annual rainfall for various 30-year intervals . #collapse-hide ####### the hard way ####### # fig, ax = plt.subplots(figsize=(10,7)) # mean_30_59 = df_year.loc[&#39;1930-09-30&#39;:&#39;1959-09-01&#39;,&#39;rain (mm)&#39;].mean() # mean_40_69 = df_year.loc[&#39;1940-09-30&#39;:&#39;1969-09-01&#39;,&#39;rain (mm)&#39;].mean() # mean_50_79 = df_year.loc[&#39;1950-09-30&#39;:&#39;1979-09-01&#39;,&#39;rain (mm)&#39;].mean() # mean_60_89 = df_year.loc[&#39;1960-09-30&#39;:&#39;1989-09-01&#39;,&#39;rain (mm)&#39;].mean() # mean_70_99 = df_year.loc[&#39;1970-09-30&#39;:&#39;1999-09-01&#39;,&#39;rain (mm)&#39;].mean() # mean_80_09 = df_year.loc[&#39;1980-09-30&#39;:&#39;2009-09-01&#39;,&#39;rain (mm)&#39;].mean() # ax.plot([mean_30_59, # mean_40_69, # mean_50_79, # mean_60_89, # mean_70_99, # mean_80_09]) ####### the easy way ####### fig, ax = plt.subplots(figsize=(10,7)) # use list comprehension windows = [[x, x+29] for x in [1930,1940,1950,1960,1970,1980]] mean = [df_year.loc[f&#39;{w[0]:d}-09-30&#39;:f&#39;{w[1]:d}-09-01&#39;,&#39;rain (mm)&#39;].mean() for w in windows] ax.plot(mean) ax.set(xticks=np.arange(len(mean)), xticklabels=[str(w) for w in windows], ylabel=&quot;window average (mm)&quot; ); . . homework . Download both daily and monthly data for London (LONDON HEATHROW, ID: UKM00003772). You should be aware that &#39;PRCP&#39; for monthly data is in millimeters, while &#39;PRCP&#39; for daily data is in tens of millimiters. | Aggregate daily data into monthly intervals using resample(&#39;MS&#39;).sum(). &#39;MS&#39; means that the sum of all days in the month will be stored in the first day of the month. Supposedly both datasets are equal now. | Calculate the average annual rainfall, using each of these datasets. | Why is there such a big difference? |",
            "url": "https://yairmau.github.io/jupyter/2020/02/02/variability-of-precipitation-exercises.html",
            "relUrl": "/jupyter/2020/02/02/variability-of-precipitation-exercises.html",
            "date": " ‚Ä¢ Feb 2, 2020"
        }
        
    
  
    
        ,"post22": {
            "title": "Return period",
            "content": "Bilbao, Spain . Today . August 1983 . more photos . On Friday, August 26, 1983, Bilbao was celebrating its Aste Nagusia or Great Week, the main annual festivity in the city, when it and other municipalities of the Basque Country, Burgos, and Cantabria suffered devastating flooding due to heavy rains. In 24 hours, the volume of water registered 600 liters per square meter. Across all the affected areas, the weather service recorded 1.5 billion tons of water. In areas of Bilbao, the water reached a height of 5 meters (15 feet). Transportation, electricity and gas services, drinking water, food, telephone, and many other basic services were severely affected. 32 people died in Biscay, 4 people died in Cantabria, 2 people died in Alava, and 2 people died Burgos. 5 more people went missing. . How often will such rainfall happen? . How often does it rain 50 mm in 1 day? What about 100 mm in 1 day? How big is a &quot;once-in-a-century event&quot;? . Let&#39;s examine Bilbao&#39;s daily rainfall (mm), between 1947 to 2021 . . On the week of 22-28 August 1983, Bilbao&#39;s weather station measured 4.5 m of rainfall! . . Let&#39;s analyze this data and find out how rare such events are. First we need to find the annual maximum for each hydrological year. . #collapse-hide import matplotlib.pyplot as plt import numpy as np import pandas as pd df = pd.read_csv(&#39;BILBAO_daily.csv&#39;, sep=&quot;,&quot;) df[&#39;DATE&#39;] = pd.to_datetime(df[&#39;DATE&#39;]) df = df.set_index(&#39;DATE&#39;) # IMPORTANT!! daily precipitation data is in tenths of mm, divide by 10 to get it in mm. df[&#39;PRCP&#39;] = df[&#39;PRCP&#39;] / 10 import altair as alt alt.data_transformers.disable_max_rows() # Altair only recognizes column data; it ignores index values. # You can plot the index data by first resetting the index # I know that I&#39;ve just made &#39;DATE&#39; the index, but I want to have this here nonetheless so I can refer to this in the future df_new = df.reset_index()#.replace({0.0:np.nan}) source = df_new[[&#39;DATE&#39;, &#39;PRCP&#39;]] brush = alt.selection(type=&#39;interval&#39;, encodings=[&#39;x&#39;]) base = alt.Chart(source).mark_line().encode( x = &#39;DATE:T&#39;, y = &#39;PRCP:Q&#39; ).properties( width=600, height=200 ) upper = base.encode( alt.X(&#39;DATE:T&#39;, scale=alt.Scale(domain=brush)), alt.Y(&#39;PRCP:Q&#39;, scale=alt.Scale(domain=(0,100))) ) lower = base.properties( height=60 ).add_selection(brush) alt.vconcat(upper, lower) . . We will consider a hydrological year starting on 1 August. . Histogram of annual maximum events . . How many years, on average, do we have to wait to get an annual maximum above a given threshold? . . . . . . Now everything together in one gif: . . Return Period . We will follow Brutsaert&#39;s derivation (&quot;Hydrology, an introduction&quot;, page 513). It defines quantities is a little different from what we did above. . $F(x)$ is the CDF of the PDF $f(x)$. $F(x)$ indicates the non-exceedance probability, i.e., the probability that a certain event above $x$ has not occurred (or that an event below $x$ has occurred, same thing). Modifying the graph shown above, we have . . $1-F(x)$ is the probability that a certain event above $x$ has occurred. It&#39;s reciprocal is the return period: . $$ T_r(x) = frac{1}{1-F(x)} $$This return period is the expected number of observations required until $x$ is exceeded once. In our case, we can ask the question: how many years will pass (on average) until we see a rainfall event greater that that of 26 August 1983? . Let&#39;s call $p=F(x)$ the probability that we measured once and that an event greater than $x$ has not occurred. What is the probability that a rainfall above $x$ will occur only on year number $k$? . it hasn&#39;t occurred on year 1 (probability p) | it hasn&#39;t occurred on year 2 (probability p) | it hasn&#39;t occurred on year 3 (probability p) | ... | it has occurred on year k (probability 1-p) | . $P {k text{ trials until }X&gt;x } = p^{k-1}(1-p)$ . Every time the number $k$ will be different. What will be $k$ on average? . $$ bar{k} = displaystyle sum_{k=1}^{ infty} k P(k) = displaystyle sum_{k=1}^{ infty} k p^{k-1}(1-p)$$ . Let&#39;s open that up: . $$ begin{align} bar{k} &amp;= 1-p + 2p(1-p) + 3p^2(1-p) + 4p^3(1-p)+ cdots bar{k} &amp;= 1-p + 2p - 2p^2 + 3p^2 - 3p^4 + 4p^3 - 4p^4+ cdots bar{k} &amp;= 1 + p + p^2 + p^3 + p^4 + cdots end{align} $$For $p&lt;1$, the series converges to $$ 1 + p + p^2 + p^3 + p^4 + cdots = frac{1}{1-p}, $$ therefore $$ bar{k} = frac{1}{1-p}. $$ . We conclude that if we know the exceedance probability, we immediately can say what the return times are. We now need a way of estimating this exceedance probability. . Plotting Position . Source: Brutsaert, Hydrology, pages 514-516 . The Plotting Position is used as an estimate of the exceedance probability. Many formulas have been suggested (see source above), we will use the Weibull plotting position: . $P_m=$ plotting position, or probability of occurence for each event $n=$ total number of events $m=$ rank of each event, where $m=1$ is the lowest value, and $m=n$ is the highest . Return period: . $$ text{Return period} = T_r = frac{1}{1-P_m} $$Weibull plotting position: . $$ P_m = frac{m}{n+1} $$Now let&#39;s calculate that for Bilbao: . #collapse-hide # resample daily data into yearly data (maximum yearly value) max_annual = df[&#39;PRCP&#39;].resample(&#39;A-JUL&#39;).max().to_frame() # sort yearly max from lowest to highest max_annual = max_annual.sort_values(by=[&#39;PRCP&#39;], ascending=True) max_annual[&#39;m&#39;] = np.arange(1, len(max_annual) + 1) n = len(max_annual[&#39;m&#39;]) max_annual[&#39;Pm&#39;] = max_annual[&#39;m&#39;] / (n+1) max_annual[&#39;Tr&#39;] = 1 / (1 - max_annual[&#39;Pm&#39;]) max_annual . . PRCP m Pm Tr . DATE . 2011-07-31 27.0 | 1 | 0.013158 | 1.013333 | . 2002-07-31 28.5 | 2 | 0.026316 | 1.027027 | . 2021-07-31 35.8 | 3 | 0.039474 | 1.041096 | . 2001-07-31 38.6 | 4 | 0.052632 | 1.055556 | . 2004-07-31 41.1 | 5 | 0.065789 | 1.070423 | . ... ... | ... | ... | ... | . 2010-07-31 108.1 | 71 | 0.934211 | 15.200000 | . 1960-07-31 137.2 | 72 | 0.947368 | 19.000000 | . 1964-07-31 143.5 | 73 | 0.960526 | 25.333333 | . 1954-07-31 172.6 | 74 | 0.973684 | 38.000000 | . 1984-07-31 252.6 | 75 | 0.986842 | 76.000000 | . 75 rows √ó 4 columns . How well does $P_m$ approximate $F(x)$? . . We can now see in this graph how long it takes, on average, for an annual maximum event above any threshold. . . For times longer than $n$, we need to extrapolate from the curve above. . . . . .",
            "url": "https://yairmau.github.io/jupyter/2020/02/02/return-period-lecture.html",
            "relUrl": "/jupyter/2020/02/02/return-period-lecture.html",
            "date": " ‚Ä¢ Feb 2, 2020"
        }
        
    
  
    
        ,"post23": {
            "title": "Return period - exercises",
            "content": "Import relevant packages . #collapse-hide import matplotlib.pyplot as plt import numpy as np import pandas as pd from functools import reduce import re import probscale import seaborn as sns sns.set(style=&quot;ticks&quot;, font_scale=1.5) from pandas.plotting import register_matplotlib_converters register_matplotlib_converters() import urllib.request . . Go to NOAA&#39;s National Centers for Environmental Information (NCEI) Climate Data Online: Dataset Discovery . Find station codes in this map. On the left, click on the little wrench next to &quot;Global Summary of the Month&quot;, then click on &quot;identify&quot; on the panel that just opened, and click on a station (purple circle). You will see the station&#39;s name, it&#39;s ID, and the period of record. For example, for Ben-Gurion&#39;s Airport in Israel: BEN GURION, IS STATION ID: ISM00040180 Period of Record: 1951-01-01 to 2020-03-01 . You can download daily or monthly data for each station. Use the function below to download this data to your computer. station_name can be whatever you want, station_code is the station ID. . #collapse-hide def download_data(station_name, station_code): url_daily = &#39;https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access/&#39; url_monthly = &#39;https://www.ncei.noaa.gov/data/gsom/access/&#39; # download daily data - uncomment the following 2 lines to make this work # urllib.request.urlretrieve(url_daily + station_code + &#39;.csv&#39;, # station_name + &#39;_daily.csv&#39;) # download monthly data urllib.request.urlretrieve(url_monthly + station_code + &#39;.csv&#39;, station_name + &#39;_monthly.csv&#39;) . . Download daily rainfall data for Eilat, Israel. ID: IS000009972 . #collapse-hide download_data(&#39;Eilat&#39;, &#39;IS000009972&#39;) . . Then load the data into a dataframe. IMPORTANT!! daily precipitation data is in tenths of mm, divide by 10 to get it in mm. . #collapse-hide df = pd.read_csv(&#39;Eilat_daily.csv&#39;, sep=&quot;,&quot;) # make &#39;DATE&#39; the dataframe index df[&#39;DATE&#39;] = pd.to_datetime(df[&#39;DATE&#39;]) df = df.set_index(&#39;DATE&#39;) # IMPORTANT!! daily precipitation data is in tenths of mm, divide by 10 to get it in mm. df[&#39;PRCP&#39;] = df[&#39;PRCP&#39;] / 10 df . . STATION LATITUDE LONGITUDE ELEVATION NAME PRCP PRCP_ATTRIBUTES TMAX TMAX_ATTRIBUTES TMIN TMIN_ATTRIBUTES TAVG TAVG_ATTRIBUTES . DATE . 1949-11-30 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,E | NaN | NaN | NaN | NaN | NaN | NaN | . 1949-12-01 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,E | NaN | NaN | NaN | NaN | NaN | NaN | . 1949-12-02 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,E | NaN | NaN | NaN | NaN | NaN | NaN | . 1949-12-03 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,E | NaN | NaN | NaN | NaN | NaN | NaN | . 1949-12-04 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,E | NaN | NaN | NaN | NaN | NaN | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2021-03-24 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,S | 287.0 | ,,S | NaN | NaN | 227.0 | H,,S | . 2021-03-25 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | NaN | NaN | 253.0 | ,,S | 154.0 | ,,S | 202.0 | H,,S | . 2021-03-26 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | NaN | NaN | 251.0 | ,,S | 134.0 | ,,S | 186.0 | H,,S | . 2021-03-27 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | NaN | NaN | 222.0 | ,,S | 119.0 | ,,S | 173.0 | H,,S | . 2021-03-28 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | NaN | NaN | 238.0 | ,,S | 119.0 | ,,S | 188.0 | H,,S | . 26045 rows √ó 13 columns . Plot precipitation data (&#39;PRCP&#39; column) and see if everything is all right. . #collapse-hide fig, ax = plt.subplots(figsize=(10,7)) ax.plot(df[&#39;PRCP&#39;]) ax.set_xlabel(&quot;date&quot;) ax.set_ylabel(&quot;daily rainfall (mm)&quot;) ax.set_title(&quot;Eilat, 1949‚Äì2021&quot;) . . Text(0.5, 1.0, &#39;Eilat, 1949‚Äì2021&#39;) . Based on what you see, you might want to exclude certain periods, e.g.: . #collapse-hide last_date = &#39;2018-08-01&#39; first_date = &#39;1950-08-01&#39; df = df[((df.index &lt; last_date) &amp; (df.index &gt; first_date))] df . . STATION LATITUDE LONGITUDE ELEVATION NAME PRCP PRCP_ATTRIBUTES TMAX TMAX_ATTRIBUTES TMIN TMIN_ATTRIBUTES TAVG TAVG_ATTRIBUTES . DATE . 1950-08-02 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,E | 400.0 | ,,G | 240.0 | ,,G | NaN | NaN | . 1950-08-03 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,E | 410.0 | ,,G | 260.0 | ,,G | NaN | NaN | . 1950-08-04 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,E | 400.0 | ,,G | 260.0 | ,,G | NaN | NaN | . 1950-08-05 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,E | NaN | NaN | 240.0 | ,,G | NaN | NaN | . 1950-08-06 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,E | 370.0 | ,,G | 240.0 | ,,G | NaN | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2018-07-27 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,S | 414.0 | ,,S | NaN | NaN | 359.0 | H,,S | . 2018-07-28 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,S | 386.0 | ,,S | NaN | NaN | 329.0 | H,,S | . 2018-07-29 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,S | NaN | NaN | 268.0 | ,,S | 334.0 | H,,S | . 2018-07-30 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,S | 375.0 | ,,S | 277.0 | ,,S | 327.0 | H,,S | . 2018-07-31 IS000009972 | 29.55 | 34.95 | 12.0 | ELAT, IS | 0.0 | ,,S | 390.0 | ,,S | NaN | NaN | 336.0 | H,,S | . 24836 rows √ó 13 columns . The rainfall data for Eilat is VERY seasonal, it&#39;s easy to see that there is no rainfall at all during the summer. We can assume a hydrological year starting on 1 August. If you&#39;re not sure, you can plot the monthly means (see last week&#39;s lecture) and find what date makes sense best. . #collapse-hide df_month = df[&#39;PRCP&#39;].resample(&#39;M&#39;).sum().to_frame() month_numbers = np.arange(1,13) monthly_mean = np.array([]) # empty array for m in month_numbers: # cycle over months (1, 2, 3, etc) this_month_mean = df_month[df_month.index.month == m].mean() # this is the monthly mean monthly_mean = np.append(monthly_mean, this_month_mean) # append # make new df and return it df_month = pd.DataFrame({&#39;monthly rainfall (mm)&#39;:monthly_mean, &#39;month number&#39;:month_numbers }) fig, ax = plt.subplots(figsize=(10,7)) ax.bar(df_month[&#39;month number&#39;], df_month[&#39;monthly rainfall (mm)&#39;]) ax.set(xlabel=&quot;month&quot;, ylabel=&quot;monthly rainfall (mm)&quot;, title=&quot;Monthly average, Eilat, 1949--2018&quot;, xticks=np.arange(1,13)); . . Let&#39;s resample the data according to the hydrological year (1 August), and we&#39;ll keep the maximum value: . #collapse-hide max_annual = (df[&#39;PRCP&#39;].resample(&#39;A-JUL&#39;) .max() .to_frame() ) max_annual . . PRCP . DATE . 1951-07-31 10.8 | . 1952-07-31 15.0 | . 1953-07-31 34.4 | . 1954-07-31 24.3 | . 1955-07-31 19.0 | . ... ... | . 2014-07-31 11.5 | . 2015-07-31 2.4 | . 2016-07-31 8.5 | . 2017-07-31 34.5 | . 2018-07-31 11.7 | . 68 rows √ó 1 columns . Make two graphs: a) the histogram for the annual maximum (pdf) b) the cumulative probability (cdf) . #collapse-hide fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,8)) h=max_annual[&#39;PRCP&#39;].values ax1.hist(h, bins=np.arange(0,100,10), density=True) ax2.hist(h, bins=np.arange(0,100,10), cumulative=1, density=True) ax1.set(ylabel=&quot;pdf&quot;) ax2.set(xlabel=&quot;annual max (mm)&quot;, ylabel=&quot;cdf&quot;, ); . . Compute the plotting position and return time. You&#39;ll need to order the data in ascending order: . max_annual = max_annual.sort_values(by=[&#39;PRCP&#39;], ascending=True) . $P_m=$ plotting position, or probability of occurence for each event $n=$ total number of events $m=$ rank of each event, where $m=1$ is the lowest value, and $m=n$ is the highest . Weibull plotting position: . $$ P_m = frac{m}{n+1} $$Return period: . $$ text{Return period} = T_r = frac{1}{1-P_m} $$Plot the annual maximum against $P_m$ or against $T_r$. . #collapse-hide fig, ax = plt.subplots(figsize=(10, 7)) # resample daily data into yearly data (maximum yearly value) max_annual = df[&#39;PRCP&#39;].resample(&#39;A-JUL&#39;).max().to_frame() # sort yearly max from lowest to highest max_annual = max_annual.sort_values(by=[&#39;PRCP&#39;], ascending=True) max_annual[&#39;rank&#39;] = np.arange(1, len(max_annual) + 1) print(max_annual) n = len(max_annual[&#39;rank&#39;]) m = max_annual[&#39;rank&#39;] Pm = m / (n+1) Tr = 1 / (1 - Pm) # ax.plot(Tr, max_annual[&#39;PRCP&#39;]) # ax.set(xlabel=&quot;return period (y)&quot;, # ylabel=&quot;annual maximum (mm/24h)&quot;) ax.plot(Pm, max_annual[&#39;PRCP&#39;]) ax.set(xlabel=&quot;non-exeedance probability&quot;, ylabel=&quot;annual maximum (mm/24h)&quot;); . . PRCP rank DATE 1996-07-31 0.5 1 2008-07-31 0.9 2 2000-07-31 1.2 3 2012-07-31 1.3 4 1959-07-31 1.5 5 ... ... ... 1966-07-31 33.8 64 1953-07-31 34.4 65 2017-07-31 34.5 66 1981-07-31 40.6 67 1975-07-31 64.3 68 [68 rows x 2 columns] . Plot the annual maximum against the exceedance probability ($1-P_m$), in a log-log scale. Use . ax.set(xscale=&quot;log&quot;, yscale(&quot;log&quot;) ) . See what data you&#39;ll want to use for a linear fit. . #collapse-hide fig, ax = plt.subplots(figsize=(10, 6)) depth = max_annual[&#39;PRCP&#39;].values exc_prob = (1-Pm).values ax.plot(exc_prob, depth, lw=3) exclude = 40 depth_tofit = depth[exclude:] exc_prob_tofit = exc_prob[exclude:] ax.plot(exc_prob_tofit, depth_tofit, &#39;o&#39;) ax.set(ylabel=&quot;annual maximum (mm/24h)&quot;, xlabel=&quot;exceedance probability&quot;, xscale=&quot;log&quot;, yscale=&quot;log&quot;, ); . . Let&#39;s make a linear fit. Attention! Our data is not annual_max and exceedance_prob, but their log. . We make a linear fit using: . slope, intercept = np.polyfit(xdata, ydata, 1) # the number 1 in the order of the polynomial = linear . Write a function that receives an exceedance probability and returns the corresponding rainfall depth. . #collapse-hide fig, ax = plt.subplots(figsize=(10, 6)) depth = max_annual[&#39;PRCP&#39;].values exc_prob = (1-Pm).values ax.plot(exc_prob, depth, lw=3, label=&quot;Weibull plotting position&quot;) ax.set(ylabel=&quot;annual maximum (mm/24h)&quot;, xlabel=&quot;exceedance probability&quot;) ax.set_xscale(&quot;log&quot;) ax.set_yscale(&quot;log&quot;) exclude = 40 depth_tofit = depth[exclude:] exc_prob_tofit = exc_prob[exclude:] ax.plot(exc_prob_tofit, depth_tofit, &#39;o&#39;) exc_prob_tofit_log = np.log(exc_prob_tofit) depth_tofit_log = np.log(depth_tofit) slope, intercept = np.polyfit(exc_prob_tofit_log, depth_tofit_log, 1) def equation(p): return np.exp(slope*np.log(p) + intercept) prob = [1e-3,1-1e-3] ax.plot(prob, equation(prob), lw=3, color=&quot;tab:red&quot;, alpha=0.4) . . [&lt;matplotlib.lines.Line2D at 0x7fc1aa9feb50&gt;] . Homework . Everything we did today was for 24h rainfall events. We might be interested in extreme events in longer or shorter time scales. Using the following code, calculate the return time for 3-day rainfall events: . number_of_days = 3 df2 = (df[&#39;PRCP&#39;].rolling(number_of_days) .sum() .dropna() ) . All the rest after that is the same... .",
            "url": "https://yairmau.github.io/jupyter/2020/02/02/return-period-exercises.html",
            "relUrl": "/jupyter/2020/02/02/return-period-exercises.html",
            "date": " ‚Ä¢ Feb 2, 2020"
        }
        
    
  
    
        ,"post24": {
            "title": "Return period - code",
            "content": "import matplotlib.pyplot as plt import numpy as np import pandas as pd from functools import reduce import re import probscale import seaborn as sns sns.set(style=&quot;ticks&quot;, font_scale=1.5) from pandas.plotting import register_matplotlib_converters register_matplotlib_converters() import urllib.request . def download_data(station_name, station_code): url_daily = &#39;https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access/&#39; url_monthly = &#39;https://www.ncei.noaa.gov/data/gsom/access/&#39; # download daily data - uncomment to make this work urllib.request.urlretrieve(url_daily + station_code + &#39;.csv&#39;, station_name + &#39;_daily.csv&#39;) # download monthly data urllib.request.urlretrieve(url_monthly + station_code + &#39;.csv&#39;, station_name + &#39;_monthly.csv&#39;) . download_data(&#39;BILBAO&#39;, &#39;SPE00120611&#39;) . df = pd.read_csv(&#39;BILBAO_daily.csv&#39;, sep=&quot;,&quot;) # make &#39;DATE&#39; the dataframe index df[&#39;DATE&#39;] = pd.to_datetime(df[&#39;DATE&#39;]) df = df.set_index(&#39;DATE&#39;) # IMPORTANT!! daily precipitation data is in tenths of mm, divide by 10 to get it in mm. df[&#39;PRCP&#39;] = df[&#39;PRCP&#39;] / 10 . # %matplotlib notebook fig, ax = plt.subplots(figsize=(10,7)) ax.plot(df[&#39;PRCP&#39;]) ax.set_xlabel(&quot;date&quot;) ax.set_ylabel(&quot;daily rainfall (mm)&quot;) ax.set_title(&quot;Bilbao, Spain, 1947--2021&quot;) ax.annotate(&quot;26 August 1983&quot;, xy=(&#39;1983-08-26&#39;, 2500), xycoords=&#39;data&#39;, xytext=(0.7, 0.95), textcoords=&#39;axes fraction&#39;, fontsize=16, va=&quot;center&quot;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) plt.savefig(&quot;hydrology_figures/bilbao-1947-2021.png&quot;) . from matplotlib.dates import DateFormatter import matplotlib.dates as mdates import matplotlib fig, ax = plt.subplots(figsize=(10,7)) one_week = df.loc[&#39;1983-08-22&#39;:&#39;1983-08-28&#39;, &#39;PRCP&#39;] bars = ax.bar(one_week.index, one_week) ax.set_xlabel(&quot;date&quot;) ax.set_ylabel(&quot;daily rainfall (mm)&quot;) ax.set_title(&quot;Bilbao, Spain, August 1983&quot;) # write daily rainfall for i in range(len(one_week)): ax.text(one_week.index[i], one_week[i], f&quot;{one_week[i]:.0f}&quot;, ha=&quot;center&quot;, fontsize=16) ax.text(0.1, 0.8, f&quot;Total rainfall during this week: n{one_week.sum():.0f} mm&quot;, transform=ax.transAxes, fontsize=16) # Define the date format # https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior date_form = DateFormatter(&quot;%b-%d&quot;) ax.xaxis.set_major_formatter(date_form) # Ensure a major tick for each day using (interval=1) # https://matplotlib.org/stable/api/dates_api.html#date-tickers ax.xaxis.set_major_locator(mdates.DayLocator(interval=1)) plt.gcf().autofmt_xdate() plt.savefig(&quot;hydrology_figures/bilbao-august-1983.png&quot;) . import altair as alt alt.data_transformers.disable_max_rows() df_new = df.reset_index()#.replace({0.0:np.nan}) source = df_new[[&#39;DATE&#39;, &#39;PRCP&#39;]] brush = alt.selection(type=&#39;interval&#39;, encodings=[&#39;x&#39;]) base = alt.Chart(source).mark_line().encode( x = &#39;DATE:T&#39;, y = &#39;PRCP:Q&#39; ).properties( width=600, height=200 ) upper = base.encode( alt.X(&#39;DATE:T&#39;, scale=alt.Scale(domain=brush)), alt.Y(&#39;PRCP:Q&#39;, scale=alt.Scale(domain=(0,500))) ) lower = base.properties( height=60 ).add_selection(brush) alt.vconcat(upper, lower) . df_month = df[&#39;PRCP&#39;].resample(&#39;M&#39;).sum().to_frame() month_numbers = np.arange(1,13) monthly_mean = np.array([]) # empty array for m in month_numbers: # cycle over months (1, 2, 3, etc) this_month_mean = df_month[df_month.index.month == m].mean() # this is the monthly mean monthly_mean = np.append(monthly_mean, this_month_mean) # append # make new df and return it df_month = pd.DataFrame({&#39;monthly rainfall (mm)&#39;:monthly_mean, &#39;month number&#39;:month_numbers }) df_month . monthly rainfall (mm) month number . 0 130.254054 | 1 | . 1 103.468919 | 2 | . 2 95.350667 | 3 | . 3 107.420270 | 4 | . 4 84.037838 | 5 | . 5 64.563514 | 6 | . 6 50.702703 | 7 | . 7 69.922973 | 8 | . 8 85.070270 | 9 | . 9 117.237838 | 10 | . 10 153.709459 | 11 | . 11 136.832432 | 12 | . fig, ax = plt.subplots(figsize=(10,7)) ax.bar(df_month[&#39;month number&#39;], df_month[&#39;monthly rainfall (mm)&#39;]) ax.set(xlabel=&quot;month&quot;, ylabel=&quot;monthly rainfall (mm)&quot;, title=&quot;Monthly average, Bilbao, 1947--2021&quot;, xticks=np.arange(1,13)) plt.savefig(&quot;hydrology_figures/monthly_average_bilbao.png&quot;) . Let&#39;s rank the annual max . # resample daily data into yearly data (maximum yearly value) # hydrologic year starts in August max_annual = df[&#39;PRCP&#39;].resample(&#39;A-JUL&#39;).max().to_frame() # sort yearly max from highest to lowest max_annual = max_annual.sort_values(by=[&#39;PRCP&#39;], ascending=False) max_annual[&#39;rank&#39;] = np.arange(1, len(max_annual) + 1) # print(max_annual) max_annual . PRCP rank . DATE . 1984-07-31 252.6 | 1 | . 1954-07-31 172.6 | 2 | . 1964-07-31 143.5 | 3 | . 1960-07-31 137.2 | 4 | . 2010-07-31 108.1 | 5 | . ... ... | ... | . 2004-07-31 41.1 | 71 | . 2001-07-31 38.6 | 72 | . 2021-07-31 35.8 | 73 | . 2002-07-31 28.5 | 74 | . 2011-07-31 27.0 | 75 | . 75 rows √ó 2 columns . %matplotlib notebook fig, ax = plt.subplots(figsize=(10,6)) # plot annual max vs. rank ax.plot(max_annual[&#39;rank&#39;], max_annual[&#39;PRCP&#39;], &#39;-o&#39;) plt.gca().set(xlabel=&quot;rank&quot;, ylabel=&quot;annual max (mm)&quot;) . [Text(0.5, 0, &#39;rank&#39;), Text(0, 0.5, &#39;annual max (mm)&#39;)] . fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,8)) h=max_annual[&#39;PRCP&#39;].values ax1.hist(h, bins=np.arange(0,250,20)) ax2.hist(h, bins=np.arange(0,250,20), cumulative=1) ax1.set(ylabel=&quot;number of years&quot;) ax2.set(xlabel=&quot;annual max (mm)&quot;, ylabel=&quot;cumulative&quot;) plt.savefig(&quot;hydrology_figures/hist_count_cumulative_bilbao.png&quot;) . fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,8)) h=max_annual[&#39;PRCP&#39;].values ax1.hist(h, bins=np.arange(0,250,20), density=True) ax2.hist(h, bins=np.arange(0,250,20), cumulative=1, density=True) ax1.set(ylabel=&quot;pdf&quot;) ax2.set(xlabel=&quot;annual max (mm)&quot;, ylabel=&quot;cdf&quot;, ) ax1.text(0.5, 0.5, &quot;probability density function&quot;, transform=ax1.transAxes, fontsize=16, bbox=dict(boxstyle=&#39;round&#39;, facecolor=&#39;wheat&#39;, alpha=1)) ax2.text(0.5, 0.5, &quot;cumulative density function&quot;, transform=ax2.transAxes, fontsize=16, bbox=dict(boxstyle=&#39;round&#39;, facecolor=&#39;wheat&#39;, alpha=1)) plt.savefig(&quot;hydrology_figures/pdf_cdf_bilbao.png&quot;) . max_annual.shape[0] . 75 . Ward, Environmental Hydrology, pages 46-47, Brutsaert, Hydrology, pages 514-516 . $P_m=$ plotting position, or probability of occurence for each event $n=$ total number of events $m=$ rank of each event . Return period: . $$ text{Return period} = frac{1}{1-P_m} $$oldest, and intuitively the simplest: . $$ P_m = frac{m}{n} $$another option: . $$ P_m = frac{m-1}{n} $$Hazen: . $$ P_m = frac{m- frac{1}{2}}{n} $$Weibull: . $$ P_m = frac{m}{n+1} $$ Return Period . Brutsaert, &quot;Hydrology, an introduction&quot;, page 513 . $F(x)$ is the CDF of the PDF $f(x)$. $F(x)$ indicates the probability that a certain event above $x$ has not occurred (or that an event below $x$ has occurred, same thing). . $1-F(x)$ is the probability that a certain event above $x$ has occurred. It&#39;s reciprocal is the return period: $$ T_r(x) = frac{1}{1-F(x)} $$ . This return period is the expected number of observations required until $x$ is exceeded once. In our case, we can ask the question: how many years will pass (on average) until we see a rainfall event greater that that of 26 August 1983? . Let&#39;s call $p=F(x)$ the probability that we measured once and that an event greater than $x$ has not occurred. What is the probability that a rainfall above $x$ will occur only on year number $k$? . it hasn&#39;t occurred on year 1 (probability p) | it hasn&#39;t occurred on year 2 (probability p) | it hasn&#39;t occurred on year 3 (probability p) | ... | it has occurred on year k (probability 1-p) | . $P {k text{ trials until }X&gt;x } = p^{k-1}(1-p)$ . Every time the number $k$ will be different. What will be $k$ on average? . $$ bar{k} = displaystyle sum_{k=1}^{ infty} k P(k) = displaystyle sum_{k=1}^{ infty} k p^{k-1}(1-p)$$ . Let&#39;s open that up: . $$ begin{align} bar{k} &amp;= 1-p + 2p(1-p) + 3p^2(1-p) + 4p^3(1-p)+ cdots bar{k} &amp;= 1-p + 2p - 2p^2 + 3p^2 - 3p^4 + 4p^3 - 4p^4+ cdots bar{k} &amp;= 1 + p + p^2 + p^3 + p^4 + cdots end{align} $$For $p&lt;1$, the series converges to $$ 1 + p + p^2 + p^3 + p^4 + cdots = frac{1}{1-p}, $$ therefore $$ bar{k} = frac{1}{1-p}. $$ . from scipy.stats import gamma import scipy from matplotlib import rc rc(&#39;text&#39;, usetex=True) fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,6))#, sharex=True) a = 4 xmax = 12 b = np.arange(0,13,1) x = np.linspace(0, gamma.ppf(0.999, a), 100) # x = np.linspace(gamma.ppf(0.001, a), # gamma.ppf(0.999, a), 100) ax1.plot(x, gamma.pdf(x, a), &#39;r-&#39;, lw=5, alpha=0.6, label=&#39;gamma pdf&#39;) r = gamma.rvs(a, size=10000) ax1.hist(r, bins=b, density=True, histtype=&#39;stepfilled&#39;, alpha=0.2) ax2.hist(r, bins=b, density=True, histtype=&#39;stepfilled&#39;, alpha=0.2, cumulative=-1) ax2.plot(x, 1-scipy.special.gammainc(a, x), &#39;r-&#39;, lw=5, alpha=0.6, label=&#39;gamma cdf&#39;) quantile = lambda p: gamma.ppf(1-p, a) p = 0.05 q = quantile(p) xfill = np.linspace(q, xmax, 100) ax1.fill_between(xfill, gamma.pdf(xfill, a), color=&#39;None&#39;, hatch=&quot;//&quot;,edgecolor=&#39;k&#39;) ax2.plot([0, q, q], [p, p ,0], color=&quot;black&quot;, ls=&quot;:&quot;) ax1.annotate(r&quot; noindent probability that next occurrence is textbf{ underline{higher}} than $x^*$:&quot; + &quot; {:.0f} %&quot;.format(100*p), xy=(q+0.8, gamma.pdf(q+0.8, a)*0.6), xycoords=&#39;data&#39;, xytext=(0.7, 0.8), textcoords=&#39;axes fraction&#39;, fontsize=16, horizontalalignment=&quot;center&quot;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.0)) ax2.annotate(r&quot; noindent textbf{ underline{exceedance}} probability&quot;, xy=(q, 1-scipy.special.gammainc(a, q)), xycoords=&#39;data&#39;, xytext=(0.5, 0.9), textcoords=&#39;axes fraction&#39;, fontsize=16, horizontalalignment=&quot;center&quot;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.0)) ax2.text(q+0.1, 0, r&quot;$x^*$&quot;, ha=&quot;left&quot;, va=&quot;bottom&quot;, fontsize=16) ax2.text(0.5, p, r&quot;{:.0f} %&quot;.format(100*p), ha=&quot;left&quot;, va=&quot;bottom&quot;, fontsize=16) ax1.set_title(r&quot;we&#39;ll wait textbf{on average}&quot; + &quot; ({:.0f} %)&quot;.format(100*p) + r&quot;$^{-1}=$&quot; + r&quot; ({:.2f})&quot;.format(p) + r&quot;$^{-1}=$&quot; + &quot; {:.0f} years &quot;.format(1/p) + r&quot; for a yearly maximum above $x^*$&quot;, fontsize=16) ax1.set(xlim=[0, xmax], ylabel=&quot;probability density&quot;, xticks=[q], xticklabels=[r&quot;$x^*$&quot;]) ax2.set(xlim=[0, xmax], xlabel=r&quot;$x$&quot;, ylabel=&quot;cumulative&quot;) plt.savefig(&quot;hydrology_figures/return_prob_005.png&quot;) . # make an animated gif of all the figures import imageio files = [&#39;hydrology_figures/return_prob_050.png&#39;, &#39;hydrology_figures/return_prob_033.png&#39;, &#39;hydrology_figures/return_prob_020.png&#39;, &#39;hydrology_figures/return_prob_010.png&#39;, &#39;hydrology_figures/return_prob_005.png&#39;,] images = [imageio.imread(file) for file in files] imageio.mimwrite(&#39;hydrology_figures/movie.gif&#39;, images, fps=1) . from scipy.stats import gamma import scipy from matplotlib import rc rc(&#39;text&#39;, usetex=True) fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,6))#, sharex=True) a = 4 xmax = 12 b = np.arange(0,13,1) x = np.linspace(0, gamma.ppf(0.999, a), 100) # x = np.linspace(gamma.ppf(0.001, a), # gamma.ppf(0.999, a), 100) ax1.plot(x, gamma.pdf(x, a), &#39;r-&#39;, lw=5, alpha=0.6, label=&#39;gamma pdf&#39;) r = gamma.rvs(a, size=10000) ax1.hist(r, bins=b, density=True, histtype=&#39;stepfilled&#39;, alpha=0.2) ax2.hist(r, bins=b, density=True, histtype=&#39;stepfilled&#39;, alpha=0.2, cumulative=1) ax2.plot(x, scipy.special.gammainc(a, x), &#39;r-&#39;, lw=5, alpha=0.6, label=&#39;gamma cdf&#39;) quantile = lambda p: gamma.ppf(1-p, a) p = 0.50 q = quantile(p) xfill = np.linspace(0, q, 100) ax1.fill_between(xfill, gamma.pdf(xfill, a), color=&#39;None&#39;, hatch=&quot;//&quot;,edgecolor=&#39;k&#39;) ax2.plot([0, q, q], [p, p ,0], color=&quot;black&quot;, ls=&quot;:&quot;) ax1.annotate(r&quot; noindent probability that next occurrence is textbf{ underline{lower}} than $x^*$:&quot; + &quot; {:.0f} %&quot;.format(100*p), xy=(q-0.8, gamma.pdf(q+0.8, a)*0.6), xycoords=&#39;data&#39;, xytext=(0.7, 0.8), textcoords=&#39;axes fraction&#39;, fontsize=16, horizontalalignment=&quot;center&quot;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.0)) ax2.annotate(r&quot; noindent textbf{ underline{non-exceedance}} probability&quot;, xy=(q, 1-scipy.special.gammainc(a, q)), xycoords=&#39;data&#39;, xytext=(0.3, 0.9), textcoords=&#39;axes fraction&#39;, fontsize=16, horizontalalignment=&quot;center&quot;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.0)) ax2.text(q+0.1, 0, r&quot;$x^*$&quot;, ha=&quot;left&quot;, va=&quot;bottom&quot;, fontsize=16) ax2.text(0.5, p, r&quot;{:.0f} %&quot;.format(100*p), ha=&quot;left&quot;, va=&quot;bottom&quot;, fontsize=16) # ax1.set_title(r&quot;we&#39;ll wait textbf{on average}&quot; + # &quot; ({:.0f} %)&quot;.format(100*p) + # r&quot;$^{-1}=$&quot; + # r&quot; ({:.2f})&quot;.format(p) + # r&quot;$^{-1}=$&quot; + # &quot; {:.0f} years &quot;.format(1/p) + r&quot; for a yearly maximum below $x^*$&quot;, fontsize=16) ax2.text(0.6,0.5,r&quot;$ displaystyle F(x)= int_0^x ! !f(x) textrm{d}x$&quot;, transform=ax2.transAxes, fontsize=20) ax1.set_ylabel(r&quot;$f(x)$&quot;, rotation=&quot;horizontal&quot;, labelpad=20) ax2.set_ylabel(r&quot;$F(x)$&quot;, rotation=&quot;horizontal&quot;, labelpad=20) ax1.set(xlim=[0, xmax], xticks=[q], xticklabels=[r&quot;$x^*$&quot;]) ax2.set(xlim=[0, xmax], xlabel=r&quot;$x$&quot;) plt.savefig(&quot;hydrology_figures/return_prob_050_reversed.png&quot;) . fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 10)) # resample daily data into yearly data (maximum yearly value) max_annual = df[&#39;PRCP&#39;].resample(&#39;A-JUL&#39;).max().to_frame() # sort yearly max from highest to lowest max_annual = max_annual.sort_values(by=[&#39;PRCP&#39;], ascending=True) max_annual[&#39;rank&#39;] = np.arange(1, len(max_annual) + 1) print(max_annual) # plot annual max vs. rank ax1.plot(max_annual[&#39;rank&#39;], max_annual[&#39;PRCP&#39;], &#39;-o&#39;) ax1.set(xlabel=&quot;rank&quot;, ylabel=&quot;annual max (mm)&quot;) n = len(max_annual[&#39;rank&#39;]) m = max_annual[&#39;rank&#39;] Pm = m / (n+1) Tr = 1 / (1 - Pm) ax2.plot(max_annual[&#39;rank&#39;], Tr) ax3.plot(Tr, max_annual[&#39;PRCP&#39;]) ax2.set(xlabel=&quot;rank&quot;, ylabel=&quot;return period (y)&quot;) ax3.set(xlabel=&quot;return period (y)&quot;, ylabel=&quot;yearly maximum (mm/24h)&quot;) . PRCP rank DATE 2011-07-31 27.0 1 2002-07-31 28.5 2 2021-07-31 35.8 3 2001-07-31 38.6 4 2004-07-31 41.1 5 ... ... ... 2010-07-31 108.1 71 1960-07-31 137.2 72 1964-07-31 143.5 73 1954-07-31 172.6 74 1984-07-31 252.6 75 [75 rows x 2 columns] . [Text(0.5, 0, &#39;return period (y)&#39;), Text(0, 0.5, &#39;yearly maximum (mm/24h)&#39;)] . fig, ax = plt.subplots(figsize=(10, 7)) # resample daily data into yearly data (maximum yearly value) max_annual = df[&#39;PRCP&#39;].resample(&#39;A-JUL&#39;).max().to_frame() # sort yearly max from highest to lowest max_annual = max_annual.sort_values(by=[&#39;PRCP&#39;], ascending=True) max_annual[&#39;rank&#39;] = np.arange(1, len(max_annual) + 1) print(max_annual) # plot annual max vs. rank # ax1.plot(max_annual[&#39;rank&#39;], max_annual[&#39;PRCP&#39;], &#39;-o&#39;) # ax1.set(xlabel=&quot;rank&quot;, # ylabel=&quot;annual max (mm)&quot;) n = len(max_annual[&#39;rank&#39;]) m = max_annual[&#39;rank&#39;] Pm = m / (n+1) Tr = 1 / (1 - Pm) # ax2.plot(max_annual[&#39;rank&#39;], Tr) ax.plot(Tr, max_annual[&#39;PRCP&#39;]) # ax2.set(xlabel=&quot;rank&quot;, # ylabel=&quot;return period (y)&quot;) ax.set(xlabel=&quot;return period (y)&quot;, ylabel=&quot;annual maximum (mm/24h)&quot;) plt.savefig(&quot;hydrology_figures/annual_max_vs_return_period.png&quot;) . PRCP rank DATE 2011-07-31 27.0 1 2002-07-31 28.5 2 2021-07-31 35.8 3 2001-07-31 38.6 4 2004-07-31 41.1 5 ... ... ... 2010-07-31 108.1 71 1960-07-31 137.2 72 1964-07-31 143.5 73 1954-07-31 172.6 74 1984-07-31 252.6 75 [75 rows x 2 columns] . Tr . DATE 2011-07-31 1.013333 2002-07-31 1.027027 2021-07-31 1.041096 2001-07-31 1.055556 2004-07-31 1.070423 ... 2010-07-31 15.200000 1960-07-31 19.000000 1964-07-31 25.333333 1954-07-31 38.000000 1984-07-31 76.000000 Name: rank, Length: 75, dtype: float64 . fig, ax = plt.subplots(figsize=(10, 6)) ax.hist(h, bins=np.arange(0,250,20), cumulative=1, density=True, label=&quot;1947--2021 statistics for Bilbao&quot;) ax.plot(max_annual[&#39;PRCP&#39;], Pm, lw=3, label=&quot;Weibull plotting position&quot;) ax.set(xlabel=&quot;annual maximum (mm/24h)&quot;, ylabel=&quot;cumulative (non-exceedance) probability&quot;, xlim=[0, 250], ylim=[0, 1.2], yticks=np.arange(0,1.1,0.2)) ax.legend(loc=&quot;upper left&quot;, frameon=False) plt.savefig(&quot;hydrology_figures/weibull_plotting_position.png&quot;) . fig, ax = plt.subplots(figsize=(10, 6)) # ax.hist(h, bins=np.arange(0,2500,200), cumulative=1, density=True, label=&quot;1947--2021 statistics for Bilbao&quot;) ax.plot(max_annual[&#39;PRCP&#39;], Pm, lw=3, label=&quot;Weibull plotting position&quot;) ax.set(xlabel=&quot;annual maximum (mm/24h)&quot;, ylabel=&quot;cumulative (non-exceedance) probability&quot;)#, # xlim=[0, 2500], # ylim=[0, 1.2], # yticks=np.arange(0,1.1,0.2)) ax.set_xscale(&quot;linear&quot;) ax.legend(loc=&quot;upper left&quot;, frameon=False) . &lt;matplotlib.legend.Legend at 0x7f87d147cad0&gt; . import imageio files = [&#39;hydrology_figures/return_prob_050.png&#39;, &#39;hydrology_figures/return_prob_033.png&#39;, &#39;hydrology_figures/return_prob_020.png&#39;, &#39;hydrology_figures/return_prob_010.png&#39;, &#39;hydrology_figures/return_prob_005.png&#39;,] images = [imageio.imread(file) for file in files] imageio.mimwrite(&#39;hydrology_figures/movie.gif&#39;, images, fps=1) . from matplotlib.ticker import ScalarFormatter import matplotlib.ticker as mtick fig, ax = plt.subplots(figsize=(10, 6)) depth = max_annual[&#39;PRCP&#39;].values exc_prob = (1-Pm).values ax.plot(exc_prob, depth, lw=3, label=&quot;Weibull plotting position&quot;) ax.set(ylabel=&quot;annual maximum (mm/24h)&quot;, xlabel=&quot;exceedance probability&quot;) ax.set_xscale(&quot;log&quot;) ax.set_yscale(&quot;log&quot;) exclude = 40 depth_tofit = depth[exclude:] exc_prob_tofit = exc_prob[exclude:] # ax.plot(exc_prob_tofit, depth_tofit, &#39;o&#39;) exc_prob_tofit_log = np.log(exc_prob_tofit) depth_tofit_log = np.log(depth_tofit) slope, intercept = np.polyfit(exc_prob_tofit_log, depth_tofit_log, 1) prob = [1e-3,1-1e-3] def equation(p): return np.exp(slope*np.log(p) + intercept) # ax.plot(prob, equation(prob), lw=3, color=&quot;tab:red&quot;, alpha=0.4) one_in_x_years = np.array([5, 10, 50, 100])#, 200, 1000]) for y in one_in_x_years: ax.plot([1/y]*2, [0, equation(1/y)], color=&quot;black&quot;, ls=&quot;--&quot;) ax.text(1/y, 30, f&quot;{y}-yr event&quot;, rotation=90, ha=&quot;right&quot;, fontsize=16) ax.set_xticks([0.01, 0.02, 0.1, 0.2, 1.0]) ax.set_yticks([10, 20, 50, 100, 200, 500]) ax.yaxis.set_major_formatter(ScalarFormatter()) ax.xaxis.set_major_formatter(mtick.FuncFormatter(lambda y, _: &#39;{:.1f}%&#39;.format(y*100))) # ax.annotate(f&quot;{equation(1/5):.0f} mm&quot;, # xy=(1/5, equation(1/5)), xycoords=&#39;data&#39;, # xytext=(0.8, 0.6), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/10):.0f} mm&quot;, # xy=(1/10, equation(1/10)), xycoords=&#39;data&#39;, # xytext=(0.7, 0.7), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/50):.0f} mm&quot;, # xy=(1/50, equation(1/50)), xycoords=&#39;data&#39;, # xytext=(0.6, 0.8), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/100):.0f} mm&quot;, # xy=(1/100, equation(1/100)), xycoords=&#39;data&#39;, # xytext=(0.5, 0.9), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/200):.0f} mm&quot;, # xy=(1/200, equation(1/200)), xycoords=&#39;data&#39;, # xytext=(0.35, 0.9), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/1000):.0f} mm&quot;, # xy=(1/1000, equation(1/1000)), xycoords=&#39;data&#39;, # xytext=(0.2, 0.95), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) plt.savefig(&quot;hydrology_figures/extrapolation_exceedance1.png&quot;) . from matplotlib.ticker import ScalarFormatter import matplotlib.ticker as mtick fig, ax = plt.subplots(figsize=(10, 6)) depth = max_annual[&#39;PRCP&#39;].values exc_prob = (1-Pm).values ax.plot(exc_prob, depth, lw=3, label=&quot;Weibull plotting position&quot;) ax.set(ylabel=&quot;annual maximum (mm/24h)&quot;, xlabel=&quot;exceedance probability&quot;) ax.set_xscale(&quot;log&quot;) ax.set_yscale(&quot;log&quot;) exclude = 40 depth_tofit = depth[exclude:] exc_prob_tofit = exc_prob[exclude:] ax.plot(exc_prob_tofit, depth_tofit, &#39;o&#39;) exc_prob_tofit_log = np.log(exc_prob_tofit) depth_tofit_log = np.log(depth_tofit) slope, intercept = np.polyfit(exc_prob_tofit_log, depth_tofit_log, 1) prob = [1e-3,1-1e-3] def equation(p): return np.exp(slope*np.log(p) + intercept) # ax.plot(prob, equation(prob), lw=3, color=&quot;tab:red&quot;, alpha=0.4) one_in_x_years = np.array([5, 10, 50, 100])#, 200, 1000]) for y in one_in_x_years: ax.plot([1/y]*2, [0, equation(1/y)], color=&quot;black&quot;, ls=&quot;--&quot;) ax.text(1/y, 30, f&quot;{y}-yr event&quot;, rotation=90, ha=&quot;right&quot;, fontsize=16) ax.set_xticks([0.01, 0.02, 0.1, 0.2, 1.0]) ax.set_yticks([10, 20, 50, 100, 200, 500]) ax.yaxis.set_major_formatter(ScalarFormatter()) ax.xaxis.set_major_formatter(mtick.FuncFormatter(lambda y, _: &#39;{:.1f}%&#39;.format(y*100))) # ax.annotate(f&quot;{equation(1/5):.0f} mm&quot;, # xy=(1/5, equation(1/5)), xycoords=&#39;data&#39;, # xytext=(0.8, 0.6), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/10):.0f} mm&quot;, # xy=(1/10, equation(1/10)), xycoords=&#39;data&#39;, # xytext=(0.7, 0.7), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/50):.0f} mm&quot;, # xy=(1/50, equation(1/50)), xycoords=&#39;data&#39;, # xytext=(0.6, 0.8), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/100):.0f} mm&quot;, # xy=(1/100, equation(1/100)), xycoords=&#39;data&#39;, # xytext=(0.5, 0.9), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/200):.0f} mm&quot;, # xy=(1/200, equation(1/200)), xycoords=&#39;data&#39;, # xytext=(0.35, 0.9), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/1000):.0f} mm&quot;, # xy=(1/1000, equation(1/1000)), xycoords=&#39;data&#39;, # xytext=(0.2, 0.95), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) plt.savefig(&quot;hydrology_figures/extrapolation_exceedance2.png&quot;) . from matplotlib.ticker import ScalarFormatter import matplotlib.ticker as mtick fig, ax = plt.subplots(figsize=(10, 6)) depth = max_annual[&#39;PRCP&#39;].values exc_prob = (1-Pm).values ax.plot(exc_prob, depth, lw=3, label=&quot;Weibull plotting position&quot;) ax.set(ylabel=&quot;annual maximum (mm/24h)&quot;, xlabel=&quot;exceedance probability&quot;) ax.set_xscale(&quot;log&quot;) ax.set_yscale(&quot;log&quot;) exclude = 40 depth_tofit = depth[exclude:] exc_prob_tofit = exc_prob[exclude:] ax.plot(exc_prob_tofit, depth_tofit, &#39;o&#39;) exc_prob_tofit_log = np.log(exc_prob_tofit) depth_tofit_log = np.log(depth_tofit) slope, intercept = np.polyfit(exc_prob_tofit_log, depth_tofit_log, 1) prob = [1e-2,1-1e-3] def equation(p): return np.exp(slope*np.log(p) + intercept) ax.plot(prob, equation(prob), lw=3, color=&quot;tab:red&quot;, alpha=0.4) one_in_x_years = np.array([5, 10, 50, 100])#, 200, 1000]) for y in one_in_x_years: ax.plot([1/y]*2, [0, equation(1/y)], color=&quot;black&quot;, ls=&quot;--&quot;) ax.text(1/y, 30, f&quot;{y}-yr event&quot;, rotation=90, ha=&quot;right&quot;, fontsize=16) ax.set_xticks([0.01, 0.02, 0.1, 0.2, 1.0]) ax.set_yticks([10, 20, 50, 100, 200, 500]) ax.yaxis.set_major_formatter(ScalarFormatter()) ax.xaxis.set_major_formatter(mtick.FuncFormatter(lambda y, _: &#39;{:.1f}%&#39;.format(y*100))) # ax.annotate(f&quot;{equation(1/5):.0f} mm&quot;, # xy=(1/5, equation(1/5)), xycoords=&#39;data&#39;, # xytext=(0.8, 0.6), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/10):.0f} mm&quot;, # xy=(1/10, equation(1/10)), xycoords=&#39;data&#39;, # xytext=(0.7, 0.7), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/50):.0f} mm&quot;, # xy=(1/50, equation(1/50)), xycoords=&#39;data&#39;, # xytext=(0.6, 0.8), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/100):.0f} mm&quot;, # xy=(1/100, equation(1/100)), xycoords=&#39;data&#39;, # xytext=(0.5, 0.9), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/200):.0f} mm&quot;, # xy=(1/200, equation(1/200)), xycoords=&#39;data&#39;, # xytext=(0.35, 0.9), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) # ax.annotate(f&quot;{equation(1/1000):.0f} mm&quot;, # xy=(1/1000, equation(1/1000)), xycoords=&#39;data&#39;, # xytext=(0.2, 0.95), textcoords=&#39;axes fraction&#39;, # fontsize=16, horizontalalignment=&quot;center&quot;, # arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) plt.savefig(&quot;hydrology_figures/extrapolation_exceedance3.png&quot;) . from matplotlib.ticker import ScalarFormatter import matplotlib.ticker as mtick fig, ax = plt.subplots(figsize=(10, 6)) depth = max_annual[&#39;PRCP&#39;].values exc_prob = (1-Pm).values ax.plot(exc_prob, depth, lw=3, label=&quot;Weibull plotting position&quot;) ax.set(ylabel=&quot;annual maximum (mm/24h)&quot;, xlabel=&quot;exceedance probability&quot;) ax.set_xscale(&quot;log&quot;) ax.set_yscale(&quot;log&quot;) exclude = 40 depth_tofit = depth[exclude:] exc_prob_tofit = exc_prob[exclude:] ax.plot(exc_prob_tofit, depth_tofit, &#39;o&#39;) exc_prob_tofit_log = np.log(exc_prob_tofit) depth_tofit_log = np.log(depth_tofit) slope, intercept = np.polyfit(exc_prob_tofit_log, depth_tofit_log, 1) prob = [1e-3,1-1e-3] def equation(p): return np.exp(slope*np.log(p) + intercept) ax.plot(prob, equation(prob), lw=3, color=&quot;tab:red&quot;, alpha=0.4) one_in_x_years = np.array([5, 10, 50, 100, 200, 1000]) for y in one_in_x_years: ax.plot([1/y]*2, [0, equation(1/y)], color=&quot;black&quot;, ls=&quot;--&quot;) ax.text(1/y, 30, f&quot;{y}-yr event&quot;, rotation=90, ha=&quot;right&quot;, fontsize=16) ax.set_xticks([0.001, 0.005, 0.01, 0.02, 0.1, 0.2, 1.0]) ax.set_yticks([10, 20, 50, 100, 200, 500]) ax.yaxis.set_major_formatter(ScalarFormatter()) ax.xaxis.set_major_formatter(mtick.FuncFormatter(lambda y, _: &#39;{:.1f}%&#39;.format(y*100))) ax.annotate(f&quot;{equation(1/5):.0f} mm&quot;, xy=(1/5, equation(1/5)), xycoords=&#39;data&#39;, xytext=(0.8, 0.6), textcoords=&#39;axes fraction&#39;, fontsize=16, horizontalalignment=&quot;center&quot;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) ax.annotate(f&quot;{equation(1/10):.0f} mm&quot;, xy=(1/10, equation(1/10)), xycoords=&#39;data&#39;, xytext=(0.7, 0.7), textcoords=&#39;axes fraction&#39;, fontsize=16, horizontalalignment=&quot;center&quot;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) ax.annotate(f&quot;{equation(1/50):.0f} mm&quot;, xy=(1/50, equation(1/50)), xycoords=&#39;data&#39;, xytext=(0.6, 0.8), textcoords=&#39;axes fraction&#39;, fontsize=16, horizontalalignment=&quot;center&quot;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) ax.annotate(f&quot;{equation(1/100):.0f} mm&quot;, xy=(1/100, equation(1/100)), xycoords=&#39;data&#39;, xytext=(0.5, 0.9), textcoords=&#39;axes fraction&#39;, fontsize=16, horizontalalignment=&quot;center&quot;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) ax.annotate(f&quot;{equation(1/200):.0f} mm&quot;, xy=(1/200, equation(1/200)), xycoords=&#39;data&#39;, xytext=(0.35, 0.9), textcoords=&#39;axes fraction&#39;, fontsize=16, horizontalalignment=&quot;center&quot;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) ax.annotate(f&quot;{equation(1/1000):.0f} mm&quot;, xy=(1/1000, equation(1/1000)), xycoords=&#39;data&#39;, xytext=(0.2, 0.95), textcoords=&#39;axes fraction&#39;, fontsize=16, horizontalalignment=&quot;center&quot;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05)) plt.savefig(&quot;hydrology_figures/extrapolation_exceedance4.png&quot;) . df7days = (df[&#39;PRCP&#39;].rolling(3) .sum() .dropna() ) annual_7days = (df7days.resample(&#39;A-JUL&#39;) .max() .to_frame() ) annual_7days = annual_7days.reset_index() annual_7days = (annual_7days.iloc[1:-1] .sort_values(by=[&#39;PRCP&#39;], ascending=True) ) annual_7days[&#39;rank&#39;] = np.arange(1, len(annual_7days) + 1) n = len(annual_7days[&#39;rank&#39;]) m = annual_7days[&#39;rank&#39;] Pm = m / (n+1) fig, ax = plt.subplots(figsize=(10, 6)) depth = annual_7days[&#39;PRCP&#39;].values exc_prob = (1-Pm).values ax.plot(exc_prob, depth, &#39;-o&#39;, lw=3, label=&quot;Weibull plotting position&quot;) ax.set(ylabel=&quot;annual maximum (mm/week)&quot;, xlabel=&quot;exceedance probability&quot;) ax.set_xscale(&quot;log&quot;) ax.set_yscale(&quot;log&quot;) exclude = 40 depth_tofit = depth[exclude:-1] exc_prob_tofit = exc_prob[exclude:-1] ax.plot(exc_prob_tofit, depth_tofit, &#39;o&#39;) exc_prob_tofit_log = np.log(exc_prob_tofit) depth_tofit_log = np.log(depth_tofit) slope, intercept = np.polyfit(exc_prob_tofit_log, depth_tofit_log, 1) prob = [1e-3,1-1e-3] def equation(p): return np.exp(slope*np.log(p) + intercept) ax.plot(prob, equation(prob), lw=3, color=&quot;tab:red&quot;, alpha=0.4) ax.set_xticks([0.001, 0.005, 0.01, 0.02, 0.1, 0.2, 1.0]) ax.set_yticks([50, 100, 200, 500]) ax.yaxis.set_major_formatter(mtick.FuncFormatter(lambda y, _: &#39;{:.0f}&#39;.format(y))) ax.xaxis.set_major_formatter(mtick.FuncFormatter(lambda x, _: &#39;{:.1f}%&#39;.format(x*100))) . df7days = (df[&#39;PRCP&#39;].rolling(3) .sum() .dropna() ) annual_7days = (df7days.resample(&#39;A-JUL&#39;) .max() .to_frame() ) annual_7days = annual_7days.reset_index() annual_7days = (annual_7days.iloc[1:-1] .sort_values(by=[&#39;PRCP&#39;], ascending=True) ) annual_7days[&#39;rank&#39;] = np.arange(1, len(annual_7days) + 1) n = len(annual_7days[&#39;rank&#39;]) m = annual_7days[&#39;rank&#39;] Pm = m / (n+1) fig, ax = plt.subplots(figsize=(10, 6)) depth = annual_7days[&#39;PRCP&#39;].values exc_prob = (1-Pm).values ret_time = 1 / exc_prob ax.plot(ret_time, depth, &#39;-o&#39;, lw=3, label=&quot;Weibull plotting position&quot;) ax.set(ylabel=&quot;annual maximum (mm/week)&quot;, xlabel=&quot;return time (year)&quot;) ax.set_xscale(&quot;log&quot;) ax.set_yscale(&quot;log&quot;) # exclude = 40 # depth_tofit = depth[exclude:-1] # exc_prob_tofit = exc_prob[exclude:-1] # ax.plot(exc_prob_tofit, depth_tofit, &#39;o&#39;) # exc_prob_tofit_log = np.log(exc_prob_tofit) # depth_tofit_log = np.log(depth_tofit) # slope, intercept = np.polyfit(exc_prob_tofit_log, depth_tofit_log, 1) # prob = [1e-3,1-1e-3] # def equation(p): # return np.exp(slope*np.log(p) + intercept) # ax.plot(prob, equation(prob), lw=3, color=&quot;tab:red&quot;, alpha=0.4) # ax.set_xticks([0.001, 0.005, 0.01, 0.02, 0.1, 0.2, 1.0]) # ax.set_yticks([500, 1000, 2000, 5000]) # ax.yaxis.set_major_formatter(mtick.FuncFormatter(lambda y, _: &#39;{:.0f}&#39;.format(y))) # ax.xaxis.set_major_formatter(mtick.FuncFormatter(lambda x, _: &#39;{:.1f} %&#39;.format(x*100))) .",
            "url": "https://yairmau.github.io/jupyter/2020/02/02/return-period-code.html",
            "relUrl": "/jupyter/2020/02/02/return-period-code.html",
            "date": " ‚Ä¢ Feb 2, 2020"
        }
        
    
  
    
        ,"post25": {
            "title": "Intra-annual variability of precipitation",
            "content": ". Let&#39;s shift the months according the the hydrological year: . . Seasonality Index, Walsh and Lawler (1981) . http://leddris.aegean.gr/ses-parameters/293-rainfall-seasonality.html#:~:text=Rainfall%20seasonality%20index%20is%20a,in%20relation%20to%20water%20availability . $R=$ mean annual precipitation $m_i$ precipitation mean for month $i$ . $$ SI = displaystyle frac{1}{R} sum_{n=1}^{n=12} left| m_i - frac{R}{12} right| $$ . $SI$ Precipitation Regime . &lt;0.19 | Precipitation spread throughout the year | . 0.20-0.39 | Precipitation spread throughout the year, but with a definite wetter season | . 0.40-0.59 | Rather seasonal with a short dry season | . 0.60-0.79 | Seasonal | . 0.80-0.99 | Marked seasonal with a long dry season | . 1.00-1.19 | Most precipitation in &lt;3 months | . #collapse-hide # import packages import numpy as np import pandas as pd from calendar import month_abbr # load data month_numbers = np.arange(1,13) month_names = [month_abbr[i] for i in month_numbers] def monthly_mean(station_name, freq): # import daily data df = pd.read_csv(station_name + &#39;_&#39; + freq + &#39;.csv&#39;, sep=&quot;,&quot;) # make &#39;DATE&#39; the dataframe index df[&#39;DATE&#39;] = pd.to_datetime(df[&#39;DATE&#39;]) df = df.set_index(&#39;DATE&#39;) # print(df.index[0], df.index[-1]) if freq == &#39;daily&#39;: # resample data by month df_month = df[&#39;PRCP&#39;].resample(&#39;M&#39;).sum() # sum is labeled at the last day of the month df_month = df_month/10 # PRCP is given in tens of mm (see readme) if freq == &#39;monthly&#39;: df_month = df[&#39;PRCP&#39;] # calculate monthly mean monthly_mean = np.array([]) # empty array for m in month_numbers: # cycle over months (1, 2, 3, etc) this_month_all_indices = (df_month.index.month == m) # indices in df_month belonging to month m this_month_mean = df_month[this_month_all_indices].mean() # this is the monthly mean monthly_mean = np.append(monthly_mean, this_month_mean) # append # make new df and return it df_return = pd.DataFrame({&#39;monthly rainfall (mm)&#39;:monthly_mean, &#39;month names&#39;:month_names, &#39;month number&#39;:month_numbers }) return df_return # load monthly mean df_london = monthly_mean(&quot;LONDON HEATHROW&quot;, &#39;monthly&#39;) df_telaviv = monthly_mean(&quot;TEL AVIV READING&quot;, &#39;monthly&#39;) #collapse-hide def walsh_index(df): m = df[&quot;monthly rainfall (mm)&quot;] R = df[&quot;monthly rainfall (mm)&quot;].sum() SI = np.sum(np.abs(m-R/12)) / R return SI london_index = walsh_index(df_london) telaviv_index = walsh_index(df_telaviv) print(&quot;Seasonality index (Walsh and Lawler, 1981)&quot;) print(f&quot;London: {london_index:.2f}&quot;) print(f&quot;Tel Aviv: {telaviv_index:.2f}&quot;) . . Seasonality index (Walsh and Lawler, 1981) London: 0.13 Tel Aviv: 1.00 . .",
            "url": "https://yairmau.github.io/jupyter/2020/02/02/intra-annual-variability-of-precipitation-seasonality-lecture.html",
            "relUrl": "/jupyter/2020/02/02/intra-annual-variability-of-precipitation-seasonality-lecture.html",
            "date": " ‚Ä¢ Feb 2, 2020"
        }
        
    
  
    
        ,"post26": {
            "title": "Intra-annual variability of precipitation - code",
            "content": "import matplotlib.pyplot as plt import numpy as np import pandas as pd from calendar import month_abbr import seaborn as sns sns.set(style=&quot;ticks&quot;, font_scale=1.5) import urllib.request . import data . Go to NOAA&#39;s National Centers for Environmental Information (NCEI) Climate Data Online: Dataset Discovery . Find station codes in this map . def download_data(station_name, station_code): url_daily = &#39;https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access/&#39; url_monthly = &#39;https://www.ncei.noaa.gov/data/gsom/access/&#39; # download daily data urllib.request.urlretrieve(url_daily + station_code + &#39;.csv&#39;, station_name + &#39;_daily.csv&#39;) # download monthly data urllib.request.urlretrieve(url_monthly + station_code + &#39;.csv&#39;, station_name + &#39;_monthly.csv&#39;) . calculate monthly averages . month_numbers = np.arange(1,13) month_names = [month_abbr[i] for i in month_numbers] def monthly_mean(station_name, freq): # import daily data df = pd.read_csv(station_name + &#39;_&#39; + freq + &#39;.csv&#39;, sep=&quot;,&quot;) # make &#39;DATE&#39; the dataframe index df[&#39;DATE&#39;] = pd.to_datetime(df[&#39;DATE&#39;]) df = df.set_index(&#39;DATE&#39;) print(df.index[0], df.index[-1]) if freq == &#39;daily&#39;: # resample data by month df_month = df[&#39;PRCP&#39;].resample(&#39;M&#39;).sum() # sum is labeled at the last day of the month df_month = df_month/10 # PRCP is given in tens of mm (see readme) if freq == &#39;monthly&#39;: df_month = df[&#39;PRCP&#39;] # calculate monthly mean monthly_mean = np.array([]) # empty array for m in month_numbers: # cycle over months (1, 2, 3, etc) this_month_all_indices = (df_month.index.month == m) # indices in df_month belonging to month m this_month_mean = df_month[this_month_all_indices].mean() # this is the monthly mean monthly_mean = np.append(monthly_mean, this_month_mean) # append # make new df and return it df_return = pd.DataFrame({&#39;monthly rainfall (mm)&#39;:monthly_mean, &#39;month names&#39;:month_names, &#39;month number&#39;:month_numbers }) return df_return . # download_data(&#39;LONDON HEATHROW&#39;, &#39;UKM00003772&#39;) # download_data(&#39;TEL AVIV READING&#39;, &#39;IS000002011&#39;) # download_data(&#39;SAO PAULO&#39;, &#39;BR00E3-0520&#39;) . # load monthly mean df_london = monthly_mean(&quot;LONDON HEATHROW&quot;, &#39;monthly&#39;) df_telaviv = monthly_mean(&quot;TEL AVIV READING&quot;, &#39;monthly&#39;) df_saopaulo = monthly_mean(&quot;SAO PAULO&quot;, &#39;monthly&#39;) total_london = df_london[&#39;monthly rainfall (mm)&#39;].sum() total_telaviv = df_telaviv[&#39;monthly rainfall (mm)&#39;].sum() . 1973-01-01 00:00:00 2021-02-01 00:00:00 1939-11-01 00:00:00 1999-11-01 00:00:00 1940-04-01 00:00:00 2021-02-01 00:00:00 . fig, ax = plt.subplots(figsize=(10,7)) # bar plots ax.bar(df_london[&#39;month number&#39;], df_london[&#39;monthly rainfall (mm)&#39;], alpha=0.5, color=&quot;blue&quot;, label=f&quot;London ({total_london:.0f} mm per year)&quot;) ax.bar(df_telaviv[&#39;month number&#39;], df_telaviv[&#39;monthly rainfall (mm)&#39;], alpha=0.5, color=&quot;red&quot;, width=0.5, label=f&quot;Tel Aviv ({total_telaviv:.0f} mm per year)&quot;) # axes labels and figure title ax.set_xlabel(&#39;month&#39;) ax.set_ylabel(&#39;monthly rainfall average (mm)&#39;) ax.set_title(&#39;seasonality of two cities with similar yearly rainfall&#39;) # some ticks adjustments ax.set_xticks(month_numbers) plt.legend(loc=&#39;upper center&#39;) # save figure plt.savefig(&quot;hydrology_figures/monthly_tel_aviv_london_bars.png&quot;) . fig, ax = plt.subplots(figsize=(10,7)) # hydrological year starts in October, roll data by 9 months months = df_london[&#39;month number&#39;] months_to_roll = -9 # minus sign = roll data to the left london_rolled = np.roll(df_london[&#39;monthly rainfall (mm)&#39;], months_to_roll) telaviv_rolled = np.roll(df_telaviv[&#39;monthly rainfall (mm)&#39;], months_to_roll) months_rolled = np.roll(months, months_to_roll) months_rolled_str = [str(x) for x in months_rolled] # bar plots ax.bar(months, london_rolled, alpha=0.5, color=&quot;blue&quot;, label=f&quot;London ({total_london:.0f} mm per year)&quot;) ax.bar(months, telaviv_rolled, alpha=0.5, color=&quot;red&quot;, width=0.5, label=f&quot;Tel Aviv ({total_telaviv:.0f} mm per year)&quot;) # axes labels and figure title ax.set_xlabel(&#39;month&#39;) ax.set_ylabel(&#39;monthly rainfall average (mm)&#39;) ax.set_title(&#39;seasonality of two cities with similar yearly rainfall&#39;) # some ticks adjustments # ax.set_xticks(months) ax.set_xticklabels(months_rolled_str) plt.legend(loc=&#39;upper right&#39;) # save figure plt.savefig(&quot;hydrology_figures/monthly_tel_aviv_london_bars_hydrological_year.png&quot;) . fig = plt.figure(figsize=(10,10)) # radar chart ax = fig.add_subplot(111, polar=True) # make polar plot ax.set_theta_zero_location(&quot;N&quot;) # January on top (&quot;N&quot;orth) ax.set_theta_direction(-1) # clockwise direction ax.set_rlabel_position(90) # radial labels on the right ax.set_rticks([50,100]) # two radial ticks is enough ax.set_rlim(0,150) # limits of r axis angles=np.linspace(0, 2*np.pi, 12, endpoint=False) # divide circle into 12 slices angles=np.append(angles, angles[0]) # close loop, otherwise lines will be open ax.set_thetagrids(angles[:-1] * 180/np.pi, month_names) # relabel angles with month names # plot london data stats_london = np.array(df_london[&#39;monthly rainfall (mm)&#39;]) # get london data stats_london = np.append(stats_london, stats_london[0]) # close loop ax.plot(angles, stats_london, &quot;o-&quot;, color=&#39;blue&#39;, label=&quot;london&quot;) # plot line ax.fill(angles, stats_london, alpha=0.25, color=&#39;blue&#39;) # fill # plot tel aviv data stats_telaviv = np.array(df_telaviv[&#39;monthly rainfall (mm)&#39;]) # get tel aviv data stats_telaviv = np.append(stats_telaviv, stats_telaviv[0]) # close loop ax.plot(angles, stats_telaviv, &quot;o-&quot;, color=&#39;red&#39;, label=&quot;tel aviv&quot;) # plot line ax.fill(angles, stats_telaviv, alpha=0.25, color=&#39;red&#39;) # fill # plot sao paulo data # stats_saopaulo = np.array(df_saopaulo[&#39;monthly rainfall (mm)&#39;]) # get tel aviv data # stats_saopaulo = np.append(stats_saopaulo, stats_saopaulo[0]) # close loop # ax.plot(angles, stats_saopaulo, &quot;o-&quot;, color=&#39;green&#39;, label=&quot;sao paulo&quot;) # plot line # ax.fill(angles, stats_saopaulo, alpha=0.25, color=&#39;green&#39;) # fill ax.set_title(&quot;Monthly rainfall averages&quot;) ax.legend(loc=(-0.1,0.9)) # legend at x=-0.2 so it doesn&#39;t overlap with graph # save figure plt.savefig(&quot;hydrology_figures/radar_chart_tel_aviv_london.png&quot;) . station_name = &quot;LONDON HEATHROW&quot; freq = &#39;daily&#39; df_day = pd.read_csv(station_name + &#39;_&#39; + freq + &#39;.csv&#39;, sep=&quot;,&quot;) # make &#39;DATE&#39; the dataframe index df_day[&#39;DATE&#39;] = pd.to_datetime(df_day[&#39;DATE&#39;]) df_day = df_day.set_index(&#39;DATE&#39;) # resample data by month df_month1 = df_day[&#39;PRCP&#39;].resample(&#39;MS&#39;).sum() # sum is labeled at the last day of the month df_month1 = df_month1/10 # PRCP is given in tens of mm (see readme) freq = &#39;monthly&#39; df_mon = pd.read_csv(station_name + &#39;_&#39; + freq + &#39;.csv&#39;, sep=&quot;,&quot;) # make &#39;DATE&#39; the dataframe index df_mon[&#39;DATE&#39;] = pd.to_datetime(df_mon[&#39;DATE&#39;]) df_mon = df_mon.set_index(&#39;DATE&#39;) # resample data by month df_month2 = df_mon[&#39;PRCP&#39;] . %matplotlib notebook plt.plot(df_month1, label=&#39;from daily data&#39;) plt.plot(df_month2, label=&#39;from monthly data&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f93c9228cd0&gt; . %matplotlib notebook df = pd.read_csv(&quot;TEL AVIV READING_monthly.csv&quot;, sep=&quot;,&quot;) df[&#39;DATE&#39;] = pd.to_datetime(df[&#39;DATE&#39;]) df = df.set_index(&#39;DATE&#39;) df_by_year = df.groupby([df.index.year]) for year_number, df_year in df_by_year: year_precipitation = df_year[&#39;PRCP&#39;].values year_precipitation = np.roll(year_precipitation, 6) month_numbers = np.arange(1,len(year_precipitation)+1) month_numbers_roll = np.roll(month_numbers, 6) plt.plot(month_numbers, year_precipitation, color=&quot;gray&quot;, alpha=0.2) month_numbers = np.arange(1,12+1) month_numbers_roll = np.roll(month_numbers, 6) plt.xticks(month_numbers, month_numbers_roll) . ([&lt;matplotlib.axis.XTick at 0x7f83097830d0&gt;, &lt;matplotlib.axis.XTick at 0x7f82f83d3f10&gt;, &lt;matplotlib.axis.XTick at 0x7f82f83d3390&gt;, &lt;matplotlib.axis.XTick at 0x7f82e86780d0&gt;, &lt;matplotlib.axis.XTick at 0x7f835ad1a610&gt;, &lt;matplotlib.axis.XTick at 0x7f835acf4610&gt;, &lt;matplotlib.axis.XTick at 0x7f835acf4ed0&gt;, &lt;matplotlib.axis.XTick at 0x7f835acf42d0&gt;, &lt;matplotlib.axis.XTick at 0x7f835acf4510&gt;, &lt;matplotlib.axis.XTick at 0x7f835ad0f590&gt;, &lt;matplotlib.axis.XTick at 0x7f835ad0fa90&gt;, &lt;matplotlib.axis.XTick at 0x7f83096f7090&gt;], [Text(1, 0, &#39;7&#39;), Text(2, 0, &#39;8&#39;), Text(3, 0, &#39;9&#39;), Text(4, 0, &#39;10&#39;), Text(5, 0, &#39;11&#39;), Text(6, 0, &#39;12&#39;), Text(7, 0, &#39;1&#39;), Text(8, 0, &#39;2&#39;), Text(9, 0, &#39;3&#39;), Text(10, 0, &#39;4&#39;), Text(11, 0, &#39;5&#39;), Text(12, 0, &#39;6&#39;)]) . np.roll(np.arange(12),6) . array([ 6, 7, 8, 9, 10, 11, 0, 1, 2, 3, 4, 5]) . Seasonality Index, Walsh and Lawler (1981) . http://leddris.aegean.gr/ses-parameters/293-rainfall-seasonality.html#:~:text=Rainfall%20seasonality%20index%20is%20a,in%20relation%20to%20water%20availability . SI Precipitation Regime . &lt;0.19 | Precipitation spread throughout the year | . 0.20-0.39 | Precipitation spread throughout the year, but with a definite wetter season | . 0.40-0.59 | Rather seasonal with a short dry season | . 0.60-0.79 | Seasonal | . 0.80-0.99 | Marked seasonal with a long dry season | . 1.00-1.19 | Most precipitation in &lt;3 months | . def walsh_index(df): X = df[&quot;monthly rainfall (mm)&quot;] Ri = df[&quot;monthly rainfall (mm)&quot;].sum() SI = np.sum(np.abs(X-Ri/12)) / Ri return SI london_index = walsh_index(df_london) telaviv_index = walsh_index(df_telaviv) print(&quot;london seasonality: t&quot;, london_index) print(&quot;tel aviv seasonality: t&quot;, telaviv_index) print(&quot;ratio: t&quot;, telaviv_index/london_index) . london seasonality: 0.114678755577802 tel aviv seasonality: 1.0004295512696868 ratio: 8.723756603645398 . fig, ax = plt.subplots(figsize=(10,7)) plt.rcParams[&#39;hatch.linewidth&#39;] = 3 # hydrological year starts in October, roll data by 9 months months = df_london[&#39;month number&#39;] months_to_roll = -9 # minus sign = roll data to the left # london_rolled = np.roll(df_london[&#39;monthly rainfall (mm)&#39;], months_to_roll) telaviv_rolled = np.roll(df_telaviv[&#39;monthly rainfall (mm)&#39;], months_to_roll) months_rolled = np.roll(months, months_to_roll) months_rolled_str = [str(x) for x in months_rolled] # bar plots # ax.bar(months, london_rolled, # alpha=0.5, color=&quot;blue&quot;, # label=f&quot;London ({total_london:.0f} mm per year)&quot;) xlim = [1, 13] ax.plot(xlim, [total_telaviv/12]*2, color=&quot;tab:blue&quot;, linewidth=3) ax.set_xlim(xlim) shaded = telaviv_rolled - total_telaviv/12 ax.bar(months, shaded, alpha=0.9, color=&quot;None&quot;, width=1, hatch=&quot;//&quot;, edgecolor=&#39;k&#39;, align=&#39;edge&#39;, bottom=total_telaviv/12, label=f&quot;absolute difference&quot;) ax.bar(months, telaviv_rolled, alpha=0.5, color=&quot;red&quot;, width=1, align=&#39;edge&#39;, label=f&quot;total rainfall&quot;, zorder=0) ax.text(5.3, 86.5, r&quot;SI$=1.00=$&quot;, fontsize=24) ax.text(xlim[-1], total_telaviv/12, &quot; mean&quot;, va=&quot;center&quot;) ax.plot([8.2, 12.8], [89.5]*2, color=&quot;black&quot;, lw=2) # # axes labels and figure title ax.set_xlabel(&#39;month&#39;) ax.set_ylabel(&#39;monthly rainfall average (mm)&#39;) ax.set_title(&#39;Walsh and Lawler (1981) Seasonality Index; Tel Aviv&#39;) ax.set_xticks(np.arange(1.5,12.6,1)) ax.set_xticklabels(months_rolled_str) # ax.set_title(&#39;seasonality of two cities with similar yearly rainfall&#39;) # # some ticks adjustments # # # ax.set_xticks(months) # ax.set_xticklabels(months_rolled_str) plt.legend(loc=&#39;upper right&#39;, frameon=False, bbox_to_anchor=(1, 0.7), fontsize=18) # save figure plt.savefig(&quot;hydrology_figures/si_walsh_telaviv.png&quot;) . fig, ax = plt.subplots(figsize=(10,7)) plt.rcParams[&#39;hatch.linewidth&#39;] = 3 # hydrological year starts in October, roll data by 9 months months = df_london[&#39;month number&#39;] months_to_roll = -9 # minus sign = roll data to the left london_rolled = np.roll(df_london[&#39;monthly rainfall (mm)&#39;], months_to_roll) # telaviv_rolled = np.roll(df_telaviv[&#39;monthly rainfall (mm)&#39;], months_to_roll) months_rolled = np.roll(months, months_to_roll) months_rolled_str = [str(x) for x in months_rolled] xlim = [1, 13] ax.plot(xlim, [total_london/12]*2, color=&quot;tab:blue&quot;, linewidth=3) ax.set_xlim(xlim) shaded = london_rolled - total_london/12 ax.bar(months, shaded, alpha=0.9, color=&quot;None&quot;, width=1, hatch=&quot;//&quot;, edgecolor=&#39;k&#39;, align=&#39;edge&#39;, bottom=total_london/12, label=f&quot;absolute difference&quot;) ax.bar(months, london_rolled, alpha=0.5, color=&quot;red&quot;, width=1, align=&#39;edge&#39;, label=f&quot;total rainfall&quot;, zorder=0) ax.text(5.3, 73.5, r&quot;SI$=0.13=$&quot;, fontsize=24) ax.text(xlim[-1], total_london/12, &quot; mean&quot;, va=&quot;center&quot;) ax.plot([8.2, 12.8], [75]*2, color=&quot;black&quot;, lw=2) # # axes labels and figure title ax.set_xlabel(&#39;month&#39;) ax.set_ylabel(&#39;monthly rainfall average (mm)&#39;) ax.set_title(&#39;Walsh and Lawler (1981) Seasonality Index; London&#39;) ax.set_xticks(np.arange(1.5,12.6,1)) ax.set_xticklabels(months_rolled_str) ax.set_ylim([0, 83]) # ax.set_title(&#39;seasonality of two cities with similar yearly rainfall&#39;) # # some ticks adjustments # # # ax.set_xticks(months) # ax.set_xticklabels(months_rolled_str) plt.legend(loc=&#39;upper right&#39;, frameon=False, bbox_to_anchor=(1, 1.005), fontsize=18) # save figure plt.savefig(&quot;hydrology_figures/si_walsh_london.png&quot;) .",
            "url": "https://yairmau.github.io/jupyter/2020/02/02/intra-annual-variability-of-precipitation-code.html",
            "relUrl": "/jupyter/2020/02/02/intra-annual-variability-of-precipitation-code.html",
            "date": " ‚Ä¢ Feb 2, 2020"
        }
        
    
  
    
        ,"post27": {
            "title": "Interannual variability of precipitation",
            "content": ". hydrological year . A time period of 12 months for which precipitation totals are measured. The hydrological year is designated by the calendar year in which it ends. Let&#39;s define the hydrological year for Tel Aviv from 1 October to 30 September. . ◊î◊ê◊ù ◊ê◊ß◊ú◊ô◊ù ◊î◊í◊©◊ù ◊©◊ú◊†◊ï ◊û◊©◊™◊†◊î . . coefficient of variation . $ langle{P} rangle=$ average precipitation $ sigma=$ standard deviation . $$CV = frac{ sigma}{ langle{P} rangle}$$ . Assuming that the inter-annual distribution is a gaussian: 67% of the time, rainfall will vary +/- 30% from its long term average in Tel Aviv. . Precipitation averages are usually calculated for time intervals of 30 years. . . . #collapse-hide import altair as alt import pandas as pd df = pd.read_csv(&quot;TEL AVIV READING_monthly.csv&quot;, sep=&quot;,&quot;) # make &#39;DATE&#39; the dataframe index df[&#39;DATE&#39;] = pd.to_datetime(df[&#39;DATE&#39;]) df = df.set_index(&#39;DATE&#39;) df_year_all = df[&#39;PRCP&#39;].resample(&#39;A-SEP&#39;).sum().to_frame() # annual frequency, anchored end of September df_year_all.columns = [&#39;rain (mm)&#39;] # rename &#39;PRCP&#39; column to &#39;rain (mm)&#39; df_year = df_year_all.iloc[:-1] # exclude last row # Altair only recognizes column data; it ignores index values. # You can plot the index data by first resetting the index # I know that I&#39;ve just made &#39;DATE&#39; the index, but I want to have this here nonetheless so I can refer to this in the future source = df_year.reset_index() brush = alt.selection(type=&#39;interval&#39;, encodings=[&#39;x&#39;]) # T: temporal, a time or date value # Q: quantitative, a continuous real-valued quantity # https://altair-viz.github.io/user_guide/encoding.html#encoding-data-types bars = alt.Chart().mark_bar().encode( x=alt.X(&#39;DATE:T&#39;, axis=alt.Axis(title=&#39;date&#39;)), y=alt.Y(&#39;rain (mm):Q&#39;, axis=alt.Axis(title=&#39;annual precipitation (mm) and average&#39;)), opacity=alt.condition(brush, alt.OpacityValue(1), alt.OpacityValue(0.2)), ).add_selection( brush ).properties( title=&#39;Select year range and drag for rolling average of annual precipitation in Tel Aviv&#39; ).properties( width=600, height=400 ) line = alt.Chart().mark_rule(color=&#39;orange&#39;).encode( y=&#39;mean(rain (mm)):Q&#39;, size=alt.SizeValue(3) ).transform_filter( brush ) alt.layer(bars, line, data=source) . . .",
            "url": "https://yairmau.github.io/jupyter/2020/02/02/interannual-variability-of-precipitation-lecture.html",
            "relUrl": "/jupyter/2020/02/02/interannual-variability-of-precipitation-lecture.html",
            "date": " ‚Ä¢ Feb 2, 2020"
        }
        
    
  
    
        ,"post28": {
            "title": "Interannual variability of precipitation - code",
            "content": "import matplotlib.pyplot as plt import matplotlib import numpy as np import pandas as pd from pandas.plotting import register_matplotlib_converters register_matplotlib_converters() # datetime converter for a matplotlib from calendar import month_abbr import seaborn as sns sns.set(style=&quot;ticks&quot;, font_scale=1.5) import urllib.request . def download_data(station_name, station_code): url_daily = &#39;https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access/&#39; url_monthly = &#39;https://www.ncei.noaa.gov/data/gsom/access/&#39; # download daily data urllib.request.urlretrieve(url_daily + station_code + &#39;.csv&#39;, station_name + &#39;_daily.csv&#39;) # download monthly data urllib.request.urlretrieve(url_monthly + station_code + &#39;.csv&#39;, station_name + &#39;_monthly.csv&#39;) . df = pd.read_csv(&quot;TEL AVIV READING_monthly.csv&quot;, sep=&quot;,&quot;) # make &#39;DATE&#39; the dataframe index df[&#39;DATE&#39;] = pd.to_datetime(df[&#39;DATE&#39;]) df = df.set_index(&#39;DATE&#39;) df . STATION LATITUDE LONGITUDE ELEVATION NAME CDSD CDSD_ATTRIBUTES CLDD CLDD_ATTRIBUTES DP01 ... HTDD HTDD_ATTRIBUTES PRCP PRCP_ATTRIBUTES TAVG TAVG_ATTRIBUTES TMAX TMAX_ATTRIBUTES TMIN TMIN_ATTRIBUTES . DATE . 1939-11-01 IS000002011 | 32.1 | 34.7831 | 3.0 | TEL AVIV READING, IS | NaN | NaN | NaN | NaN | 7.0 | ... | NaN | NaN | 106.5 | ,,,E | NaN | NaN | NaN | NaN | NaN | NaN | . 1939-12-01 IS000002011 | 32.1 | 34.7831 | 3.0 | TEL AVIV READING, IS | NaN | NaN | NaN | NaN | 7.0 | ... | NaN | NaN | 100.1 | ,,,E | NaN | NaN | NaN | NaN | NaN | NaN | . 1940-01-01 IS000002011 | 32.1 | 34.7831 | 3.0 | TEL AVIV READING, IS | NaN | NaN | NaN | NaN | 11.0 | ... | NaN | NaN | 181.5 | ,,,E | NaN | NaN | NaN | NaN | NaN | NaN | . 1940-02-01 IS000002011 | 32.1 | 34.7831 | 3.0 | TEL AVIV READING, IS | NaN | NaN | NaN | NaN | 6.0 | ... | NaN | NaN | 57.7 | ,,,E | NaN | NaN | NaN | NaN | NaN | NaN | . 1940-03-01 IS000002011 | 32.1 | 34.7831 | 3.0 | TEL AVIV READING, IS | NaN | NaN | NaN | NaN | 7.0 | ... | NaN | NaN | 27.2 | ,,,E | NaN | NaN | NaN | NaN | NaN | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1999-05-01 IS000002011 | 32.1 | 34.7831 | 3.0 | TEL AVIV READING, IS | 165.9 | NaN | 125.0 | ,I | NaN | ... | 0.0 | ,I | NaN | NaN | 22.37 | ,I | 26.15 | ,,,I | 18.58 | ,,,I | . 1999-06-01 IS000002011 | 32.1 | 34.7831 | 3.0 | TEL AVIV READING, IS | 367.1 | NaN | 201.2 | 1,I | NaN | ... | 0.0 | 1,I | NaN | NaN | 25.29 | 1,I | 28.93 | 1,,,I | 21.65 | ,,,I | . 1999-07-01 IS000002011 | 32.1 | 34.7831 | 3.0 | TEL AVIV READING, IS | 612.0 | NaN | 244.9 | 4,I | NaN | ... | 0.0 | 4,I | NaN | NaN | 27.44 | 4,I | 31.09 | 4,,,I | 23.80 | ,,,I | . 1999-08-01 IS000002011 | 32.1 | 34.7831 | 3.0 | TEL AVIV READING, IS | 921.4 | NaN | 309.4 | ,I | NaN | ... | 0.0 | ,I | NaN | NaN | 28.31 | ,I | 31.54 | ,,,I | 25.09 | ,,,I | . 1999-11-01 IS000002011 | 32.1 | 34.7831 | 3.0 | TEL AVIV READING, IS | NaN | NaN | 62.1 | ,I | NaN | ... | 13.3 | ,I | NaN | NaN | 19.96 | ,I | 24.41 | ,,,I | 15.51 | ,,,I | . 719 rows √ó 43 columns . fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,7)) # plot precipitation ax1.fill_between(df.index, df[&#39;PRCP&#39;], 0, color=&#39;tab:blue&#39;) df_1990_1992 = df.loc[&#39;1990-07-01&#39;:&#39;1992-07-01&#39;] ax2.bar(df_1990_1992.index, df_1990_1992[&#39;PRCP&#39;], width=30) # adjust labels, ticks, title, etc ax1.set_title(&quot;Monthly precipitation in Tel Aviv&quot;) ax2.tick_params(axis=&#39;x&#39;, rotation=45) ax2.set_xlabel(&quot;date&quot;) # dirty trick to get common y label between the two panels: # make a large invisible axes, give it a ylabel ax0 = fig.add_subplot(111, frame_on=False) ax0.tick_params(labelcolor=&quot;none&quot;, bottom=False, left=False) ax0.set_ylabel(&quot;monthly precipitation (mm)&quot;, labelpad=20) # write yearly rainfall rain_1990_1991 = df.loc[&#39;1990-07-01&#39;:&#39;1991-07-01&#39;,&#39;PRCP&#39;].sum() rain_1991_1992 = df.loc[&#39;1991-07-01&#39;:&#39;1992-07-01&#39;,&#39;PRCP&#39;].sum() ax2.text(&#39;1991-01-01&#39;, 300, &quot;{:.0f} mm&quot;.format(rain_1990_1991)) ax2.text(&#39;1992-01-01&#39;, 300, &quot;{:.0f} mm&quot;.format(rain_1991_1992)) # save figure plt.savefig(&quot;monthly_tel_aviv_1940-1999.png&quot;) . hydrological year . A time period of 12 months for which precipitation totals are measured. The water year is designated by the calendar year in which it ends. Let&#39;s define the hydrological year for Tel Aviv from 1 October to 30 September. . # read more about resampling options # https://pandas.pydata.org/pandas-docs/version/0.12.0/timeseries.html#offset-aliases # also, annual resampling can be anchored to the end of specific months: # https://pandas.pydata.org/pandas-docs/version/0.12.0/timeseries.html#anchored-offsets df_year_all = df[&#39;PRCP&#39;].resample(&#39;A-SEP&#39;).sum().to_frame() # annual frequency, anchored end of September df_year_all.columns = [&#39;rain (mm)&#39;] # rename &#39;PRCP&#39; column to &#39;rain (mm)&#39; df_year_all . rain (mm) . DATE . 1940-09-30 474.9 | . 1941-09-30 447.8 | . 1942-09-30 372.9 | . 1943-09-30 618.2 | . 1944-09-30 440.5 | . ... ... | . 1996-09-30 488.2 | . 1997-09-30 619.1 | . 1998-09-30 489.6 | . 1999-09-30 226.5 | . 2000-09-30 0.0 | . 61 rows √ó 1 columns . # the last year is the sum of only on month (November), let&#39;s take it out df_year = df_year_all.iloc[:-1] # exclude last row df_year.tail() # show &quot;tail&quot; of the dataframe to see that year 2000 was excluded . rain (mm) . DATE . 1995-09-30 804.7 | . 1996-09-30 488.2 | . 1997-09-30 619.1 | . 1998-09-30 489.6 | . 1999-09-30 226.5 | . fig, ax = plt.subplots(figsize=(10,7)) # plot YEARLY precipitation ax.bar(df_year.index, df_year[&#39;rain (mm)&#39;], width=365, align=&#39;edge&#39;, color=&quot;tab:blue&quot;) # plot mean rain_mean = df_year[&#39;rain (mm)&#39;].mean() ax.plot(df_year*0 + rain_mean, linewidth=3, color=&quot;tab:orange&quot;) # adjust labels, ticks, title, etc ax.set_title(&quot;Annual precipitation in Tel Aviv, 1940‚Äì1999&quot;) ax.set_xlabel(&quot;date&quot;) ax.set_ylabel(&quot;annual precipitation (mm)&quot;) ax.set_xlim([df_year.index[0], df_year.index[-1]]) # write mean on the right ax.text(df_year.index[-1], rain_mean, &quot; mean n {:.0f} mm&quot;.format(rain_mean), horizontalalignment=&quot;left&quot;, verticalalignment=&quot;center&quot;) # save figure plt.savefig(&quot;annual_tel_aviv_with_mean.png&quot;) . fig, ax = plt.subplots(figsize=(10,7)) # calculate mean and standard deviation rain_mean = df_year[&#39;rain (mm)&#39;].mean() rain_std = df_year[&#39;rain (mm)&#39;].std() # plot histogram b = np.arange(0, 1101, 100) # bins from 0 to 55, width = 5 ax.hist(df_year, bins=b) # plot vertical lines with mean, std, etc ylim = np.array(ax.get_ylim()) ylim[1] = ylim[1]*1.1 ax.plot([rain_mean]*2, ylim, linewidth=3, color=&quot;tab:orange&quot;) ax.plot([rain_mean+rain_std]*2, ylim, linewidth=3, linestyle=&quot;--&quot;, color=&quot;tab:olive&quot;) ax.plot([rain_mean-rain_std]*2, ylim, linewidth=3, linestyle=&quot;--&quot;, color=&quot;tab:olive&quot;) ax.set_ylim(ylim) # write mean, std, etc ax.text(rain_mean, ylim[1]*0.99, &quot;mean&quot;, horizontalalignment=&quot;center&quot;, verticalalignment=&quot;top&quot;, bbox=dict(facecolor=&#39;white&#39;, edgecolor=&#39;none&#39;, pad=0.0)) ax.text(rain_mean+rain_std, ylim[1]*0.99, &quot;mean+std&quot;, horizontalalignment=&quot;center&quot;, verticalalignment=&quot;top&quot;, bbox=dict(facecolor=&#39;white&#39;, edgecolor=&#39;none&#39;, pad=0.0)) ax.text(rain_mean-rain_std, ylim[1]*0.99, &quot;mean-std&quot;, horizontalalignment=&quot;center&quot;, verticalalignment=&quot;top&quot;, bbox=dict(facecolor=&#39;white&#39;, edgecolor=&#39;none&#39;, pad=0.0)) # adjust labels, ticks, title, limits, etc ax.set_title(&quot;Histogram of annual precipitation in Tel Aviv, 1940‚Äì1999&quot;) ax.set_xlabel(&quot;annual rainfall (mm)&quot;) ax.set_ylabel(&quot;number of years&quot;) # save figure plt.savefig(&quot;histogram_tel_aviv_with_mean_and_std.png&quot;) . coefficient of variation . $ langle{P} rangle=$ average precipitation $ sigma=$ standard deviation . $$CV = frac{ sigma}{ langle{P} rangle}$$ . Assuming that the inter-annual distribution is a gaussian: 67% of the time, rainfall will vary +/- 30% from its long term average in Tel Aviv. . CV = rain_std / rain_mean print(f&quot;CV = {CV:.2f}&quot;) # rain_mean . CV = 0.30 . fig, ax = plt.subplots(figsize=(10,7)) # windows of length 30 years windows = [[1940,1969], [1970,1999]] for window in windows: start_date = f&quot;{window[0]:d}-09-30&quot; end_date = f&quot;{window[1]:d}-09-30&quot; window_mean = df_year[&#39;rain (mm)&#39;][start_date:end_date].mean() ax.plot(df_year[start_date:end_date]*0+window_mean, color=&quot;purple&quot;, linewidth=3) ax.text(start_date, window_mean+0.5, f&quot;{window[0]} to {window[1]}: {window_mean:.0f} mm&quot;) # plot mean rain_mean = df_year[&#39;rain (mm)&#39;].mean() ax.plot(df_year*0 + rain_mean, linewidth=3, color=&quot;tab:orange&quot;, alpha=0.5) ax.text(df_year.index[-1], rain_mean, &quot; mean&quot;.format(rain_mean), horizontalalignment=&quot;left&quot;, verticalalignment=&quot;center&quot;) # adjust labels, ticks, title, limits, etc ax.set_title(&quot;Annual precipitation averages in Tel Aviv, 1940‚Äì1999&quot;) ax.set_xlabel(&quot;date&quot;) ax.set_ylabel(&quot;annual precipitation (mm)&quot;) ax.set_xlim([df_year.index[0], df_year.index[-1]]) ax.set_ylim([500, 560]) # save figure plt.savefig(&quot;mean_tel_aviv_2_windows.png&quot;) . fig, ax = plt.subplots(figsize=(10,7)) # windows of length 30 years windows = [[x,x+29] for x in [1940,1950,1960,1970]] for window in windows: start_date = f&quot;{window[0]:d}-09-30&quot; end_date = f&quot;{window[1]:d}-09-30&quot; window_mean = df_year[&#39;rain (mm)&#39;][start_date:end_date].mean() ax.plot(df_year[start_date:end_date]*0+window_mean, color=&quot;purple&quot;, linewidth=3) ax.text(start_date, window_mean+0.5, f&quot;{window[0]} to {window[1]}: {window_mean:.0f} mm&quot;) # plot mean ax.plot(df_year*0 + rain_mean, linewidth=3, color=&quot;tab:orange&quot;, alpha=0.5) ax.text(df_year.index[-1], rain_mean, &quot; mean&quot;.format(rain_mean), horizontalalignment=&quot;left&quot;, verticalalignment=&quot;center&quot;) # adjust labels, ticks, title, limits, etc ax.set_title(&quot;Annual precipitation averages in Tel Aviv&quot;) ax.set_xlabel(&quot;date&quot;) ax.set_ylabel(&quot;annual precipitation (mm)&quot;) ax.set_xlim([df_year.index[0], df_year.index[-1]]) ax.set_ylim([500, 560]) # save figure plt.savefig(&quot;mean_tel_aviv_4_windows.png&quot;) . import altair as alt from vega_datasets import data # Altair only recognizes column data; it ignores index values. You can plot the index data by first resetting the index source = df_year.reset_index() brush = alt.selection(type=&#39;interval&#39;, encodings=[&#39;x&#39;]) # T: temporal, a time or date value # Q: quantitative, a continuous real-valued quantity # https://altair-viz.github.io/user_guide/encoding.html#encoding-data-types bars = alt.Chart().mark_bar().encode( x=alt.X(&#39;DATE:T&#39;, axis=alt.Axis(title=&#39;date&#39;)), y=alt.Y(&#39;rain (mm):Q&#39;, axis=alt.Axis(title=&#39;annual precipitation (mm) and average&#39;)), opacity=alt.condition(brush, alt.OpacityValue(1), alt.OpacityValue(0.2)), ).add_selection( brush ).properties( title=&#39;Select year range and drag for rolling average of annual precipitation in Tel Aviv&#39; ).properties( width=600, height=400 ) line = alt.Chart().mark_rule(color=&#39;orange&#39;).encode( y=&#39;mean(rain (mm)):Q&#39;, size=alt.SizeValue(3) ).transform_filter( brush ) alt.layer(bars, line, data=source) . fig, ax = plt.subplots(figsize=(10,7)) rolling_mean = df_year.rolling(30, center=True).mean() ax.plot(rolling_mean, linewidth=3, color=&quot;tab:red&quot;, zorder=5) ax.set_title(&quot;30-year rolling average, Tel Aviv&quot;) ax.set_xlabel(&quot;date&quot;) ax.set_ylabel(&quot;annual precipitation (mm)&quot;) # windows of length 30 years windows = [[x,x+29] for x in [1940,1950,1960,1970]] for window in windows: start_date = f&quot;{window[0]:d}-09-30&quot; end_date = f&quot;{window[1]:d}-09-30&quot; window_mean = df_year[&#39;rain (mm)&#39;][start_date:end_date].mean() ax.plot(df_year[start_date:end_date]*0+window_mean, color=&quot;purple&quot;, linewidth=3, alpha=0.5) ax.text(start_date, window_mean+0.5, f&quot;{window[0]} to {window[1]}: {window_mean:.0f} mm&quot;, alpha=0.5) ax.set_ylim([480, 560]) # plot mean ax.plot(df_year*0 + rain_mean, linewidth=3, color=&quot;tab:orange&quot;, alpha=0.5) ax.text(df_year.index[-1], rain_mean, &quot; mean&quot;.format(rain_mean), horizontalalignment=&quot;left&quot;, verticalalignment=&quot;center&quot;) ax.set_xlim([df_year.index[0], df_year.index[-1]]) # save figure plt.savefig(&quot;rolling_average_tel_aviv.png&quot;) . rolling_mean . rain (mm) . DATE . 1940-09-30 NaN | . 1941-09-30 NaN | . 1942-09-30 NaN | . 1943-09-30 NaN | . 1944-09-30 NaN | . 1945-09-30 NaN | . 1946-09-30 NaN | . 1947-09-30 NaN | . 1948-09-30 NaN | . 1949-09-30 NaN | . 1950-09-30 NaN | . 1951-09-30 NaN | . 1952-09-30 NaN | . 1953-09-30 NaN | . 1954-09-30 NaN | . 1955-09-30 543.683333 | . 1956-09-30 541.690000 | . 1957-09-30 539.496667 | . 1958-09-30 547.746667 | . 1959-09-30 541.823333 | . 1960-09-30 550.390000 | . 1961-09-30 542.760000 | . 1962-09-30 536.510000 | . 1963-09-30 545.810000 | . 1964-09-30 545.803333 | . 1965-09-30 537.756667 | . 1966-09-30 530.206667 | . 1967-09-30 535.093333 | . 1968-09-30 527.663333 | . 1969-09-30 528.926667 | . 1970-09-30 516.710000 | . 1971-09-30 510.670000 | . 1972-09-30 495.516667 | . 1973-09-30 495.690000 | . 1974-09-30 502.350000 | . 1975-09-30 506.033333 | . 1976-09-30 517.593333 | . 1977-09-30 513.926667 | . 1978-09-30 527.450000 | . 1979-09-30 533.643333 | . 1980-09-30 524.583333 | . 1981-09-30 526.223333 | . 1982-09-30 531.463333 | . 1983-09-30 529.980000 | . 1984-09-30 532.553333 | . 1985-09-30 517.790000 | . 1986-09-30 NaN | . 1987-09-30 NaN | . 1988-09-30 NaN | . 1989-09-30 NaN | . 1990-09-30 NaN | . 1991-09-30 NaN | . 1992-09-30 NaN | . 1993-09-30 NaN | . 1994-09-30 NaN | . 1995-09-30 NaN | . 1996-09-30 NaN | . 1997-09-30 NaN | . 1998-09-30 NaN | . 1999-09-30 NaN | .",
            "url": "https://yairmau.github.io/jupyter/hydrology/2020/02/02/interannual-variability-of-precipitation-code.html",
            "relUrl": "/jupyter/hydrology/2020/02/02/interannual-variability-of-precipitation-code.html",
            "date": " ‚Ä¢ Feb 2, 2020"
        }
        
    
  
    
        ,"post29": {
            "title": "Assignment 2 - Evapotranspiration",
            "content": "&#128210; instructions . This is where learning happens, not during a lecture. You&#39;ll learn a ton of things by doing them yourself. Much success! üòÑ . Create a Jupyter Notebook called assignment-02-IDNUMBER, where IDNUMBER is your 9-digit ID. This is the file only file we will check. . &#128204; locations and data . Choose two stations with different climates. . Go to NOAA&#39;s Climate Reference Network Data website. The sub-hourly (5-min) data contains information on . air temperature, | precipitation, | global solar radiation, | surface infrared temperature, | relative humidity, | soil moisture and temperature, | wetness, and | 1.5 meter wind speed. There is no data on air pressure, so one needs to use the stations coordinates (lat, lon) to find its height above sea level, and from that infer the air pressure. You can use Google Earth or any other means to find the station&#39;s height. | . In the Data Access link, choose a year and a station you would like to analyze. If you are not sure where the stations are, find them using the 2-letter state abbreviation and the station name. . Download the following files: . One full year of data for each station. Make sure important data we need to calculate Penman&#39;s ET estimation is available. | The headers file | The documentation file | Make sure you understand what are the units provided for each measurement (see documentation). . &#128736; tasks . Produce potential ET estimates using Thornthwaite&#39;s equation and Penman&#39;s equation. Produce plots of ET as a function of time for each station, comparing the two methods you used. Also, using Penman&#39;s ET estimates, compare the two stations and discuss about their differences/similarities. . You might find interesting things in the data, such as periods of unusually high/low temperatures, radiation, etc. Discuss how these factors might have affected the ET estimates that you calculated. . You will have two weeks to deliver your assignment. You should not hand in a dry document with only figures and code, I&#39;m expecting text before and after each code/graph cell, explaining what you did, why you did it, and how it fits the story you are telling. Don&#39;t forget to put labels on your plot axes, title, legend, etc. . Your Jupyter Notebook should be fully functional: if we press Kernel &gt; Restart &amp; Run All, all the code must work without any errors. . &#127749; presentation . All the assignment must be in one single Jupyter Notebook. Use markdown cells to discuss the analysis and results, and in code cells show all the code you used to produce the figures and data analysis. Leave only the code necessary for your analysis, delete unnecessary lines your wrote while analyzing your data. Don&#39;t forget to comment your code, just like we did during exercise sessions. . You can write in English or in Hebrew, but the text in the figures must be in English. If you choose to write the discussion in Hebrew, be aware that Jupyter Notebooks don&#39;t have native right-to-left language support: . ◊†◊ô◊™◊ü ◊ú◊õ◊™◊ï◊ë ◊ë◊¢◊ë◊®◊ô◊™, ◊ú◊û◊®◊ï◊™ ◊©◊ñ◊î ◊ú◊ê ◊†◊®◊ê◊î ◊õ◊¥◊õ ◊ò◊ï◊ë... . You can use some HTML code to achieve best results in Hebrew. Type the following . &lt;p dir=&quot;rtl&quot; style=&quot;text-align: right;&quot;&gt; ◊¢◊õ◊©◊ô◊ï ◊î◊®◊ë◊î ◊ô◊ï◊™◊® ◊ò◊ï◊ë! &lt;/p&gt; . to get . ◊¢◊õ◊©◊ô◊ï ◊î◊®◊ë◊î ◊ô◊ï◊™◊® ◊ò◊ï◊ë! . If you have many paragraphs in hebrew, do the following: . ◊§◊°◊ß◊î ◊û◊°◊§◊® 1. . ◊§◊°◊ß◊î ◊û◊°◊§◊® 2. . ◊ê◊ù ◊ô◊© ◊ú◊õ◊ù ◊õ◊û◊î ◊§◊°◊ß◊ê◊ï◊™, ◊õ◊ú ◊ê◊ó◊™ ◊û◊î◊ü ◊™◊î◊ô◊î ◊ë◊™◊ï◊ö &quot;dir&quot; ◊û◊©◊ú◊î . In my opinion it is too complicated to write in Hebrew in Jupyter Notebooks, just write in English, your grade will not be affected by typos nor less-than-perfect English proficiency. . &#128175; evaluation . Your assignment will be evaluated according to the following criteria: . 40% Presentation. How the graphs look, labels, general organization, markdown, clean code. | 30% Discussion. This is where you explain what you did, what you found out, etc. | 15% Depth of analysis. You can analyze/explore the data with different levels of complexity, this is where we take that into consideration. | 10% Replicability: Your code runs flawlessly. | 5%: Code commenting. Explain in your code what you are doing, this is good for everyone, especially for yourself! | . &#128666; importing the data . Below you can find an example of how to import the data file provided by NOAA&#39;s Climate Reference Network Data website. You might have to make some adjustments to it. . data_file = &quot;CRNS0101-05-2020-CO_Boulder_14_W.txt&quot; df = pd.read_csv(data_file, header=None, # no headers needed, we&#39;ll do that later delim_whitespace=True, # blank spaces separate between columns na_values=[&quot;-99.000&quot;, &quot;-9999.0&quot;] # substitute these values for missing (NaN) values ) headers = pd.read_csv(&quot;HEADERS_sub_hourly.txt&quot;, # load headers file header=1, # skip the first [0] line delim_whitespace=True ) df.columns = headers.columns # rename df columns with headers columns # LST = local standard time df[&quot;LST_TIME&quot;] = [f&quot;{x:04d}&quot; for x in df[&quot;LST_TIME&quot;]] # time needs padding of zeros, then convert to string df[&#39;LST_DATE&#39;] = df[&#39;LST_DATE&#39;].astype(str) # convert date into string df[&#39;datetime&#39;] = df[&#39;LST_DATE&#39;] + &#39; &#39; + df[&#39;LST_TIME&#39;] # combine date+time into datetime df[&#39;datetime&#39;] = pd.to_datetime(df[&#39;datetime&#39;]) # interpret datetime df = df.set_index(&#39;datetime&#39;) # make datetime the index df .",
            "url": "https://yairmau.github.io/jupyter/2020/02/02/assignment-02-ET.html",
            "relUrl": "/jupyter/2020/02/02/assignment-02-ET.html",
            "date": " ‚Ä¢ Feb 2, 2020"
        }
        
    
  
    
        ,"post30": {
            "title": "let's have fun plotting some data üòÄ",
            "content": "download the data . Go to the Faculty of Agriculture&#39;s weather station. | Click on ◊û◊©◊ô◊õ◊™ ◊†◊™◊ï◊†◊ô◊ù and download data for 1 September to 28 February, with a 24h interval. Call it data-sep2020-feb2021. You can download an example file here. | Open the .csv file with Excel, see how it looks like | import packages . We need to import this data into python. First we import useful packages. Type (don&#39;t copy and paste) the following lines in the code cell below: . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns.set(style=&quot;ticks&quot;, font_scale=1.5) . import data with pandas . Import data from csv and put it in a pandas dataframe (a table). Make line 5 the header (column names) . df = pd.read_csv(&quot;data-sep2020-feb2021.csv&quot;, header=[4]) df . rename columns . rename the columns to: date, tmax, tmin, wind, rain24h, rain_cumulative . df.columns = [&#39;date&#39;, &#39;tmax&#39;, &#39;tmin&#39;, &#39;wind&#39;, &#39;rain24h&#39;, &#39;rain_cumulative&#39;] df . a first plot! . plot the minimum temperature: . plt.plot(df[&#39;tmin&#39;]) . how to deal with dates . We want the dates to appear on the horizontal axis. Interpret &#39;date&#39; column as a pandas datetime, see how it looks different from before before: 01/09/20 after: 2020-09-01 . df[&#39;date&#39;] = pd.to_datetime(df[&#39;date&#39;], dayfirst=True) df . date as dataframe index . Make &#39;date&#39; the dataframe&#39;s index . df = df.set_index(&#39;date&#39;) df . plot again, now with dates . Plot minimum temperature, now we have dates on the horizontal axis . plt.plot(df[&#39;tmin&#39;]) . we&#39;re getting there! the graph could look better . Let&#39;s make the graph look better: labels, title, slanted dates, etc . %matplotlib notebook # creates figure (the canvas) and the axis (rectangle where the plot sits) fig, ax = plt.subplots(1, figsize=(10,7)) # two line plots ax.plot(df[&#39;tmin&#39;], color=&quot;red&quot;, label=&quot;Temp (min)&quot;) ax.plot(df[&#39;tmax&#39;], color=&quot;blue&quot;, label=&quot;Temp (max)&quot;) # axes labels and figure title ax.set_xlabel(&#39;date&#39;) ax.set_ylabel(&#39;temperature (¬∞C)&#39;) ax.set_title(&#39;maximum and minimum temperatures&#39;) # some ticks adjustments ax.set_yticks([10,15,20,25]) # we can choose where to put ticks ax.grid(axis=&#39;y&#39;) # makes horizontal lines plt.gcf().autofmt_xdate() # makes slated dates # legend ax.legend(loc=&#39;upper right&#39;) # save png figure plt.savefig(&quot;temp_max_min.png&quot;) . make the following figure . Use the following function to plot bars for daily rainfall . ax.bar(x_array, y_array) . Can you write yourself some lines of code that calculate the cumulative rainfall from the daily rainfall? . . double click this markdown cell to reveal the code I used to produce the figure. Don&#39;t do this right away, try to go as far as you can! | . make another figure . In order to choose just a part of the time series, you can use the following: . start_date = &#39;2021-01-01&#39; end_date = &#39;2021-01-31&#39; january = df[start_date:end_date] . . double click this markdown cell to reveal the code I used to produce the figure. Don&#39;t do this right away, try to go as far as you can! | . one last figure for today . Use the following code to create histograms with user-defined bins: . b = np.arange(0, 56, 5) # bins from 0 to 55, width = 5 ax.hist(df[&#39;wind&#39;], bins=b, density=True) . Play with the bins, see what happens. What does density=True do? . . double click this markdown cell to reveal the code I used to produce the figure. Don&#39;t do this right away, try to go as far as you can! | . homework . Go back to the weather station website, download one year of data from 01.01.2020 to 31.12.2020 (24h data). Make the following graph: . daily tmax and tmin | smoothed data for tmax and tmin | . In order to smooth the data with a 30 day window, use the following function: df[&#39;tmin&#39;].rolling(30, center=True).mean() This means that you will take the mean of 30 days, and put the result in the center of this 30-day window. . Play with this function, see what you can do with it. What happens when you change the size of the window? Why is the smoothed data shorter than the original data? See the documentation for rolling to find more options. . . double click this markdown cell to reveal the code I used to produce the figure. Don&#39;t do this right away, try to go as far as you can! | .",
            "url": "https://yairmau.github.io/jupyter/2020/02/01/python-intro.html",
            "relUrl": "/jupyter/2020/02/01/python-intro.html",
            "date": " ‚Ä¢ Feb 1, 2020"
        }
        
    
  
    
        ,"post31": {
            "title": "Hydrology --- introduction",
            "content": "How much water is there? Where? . . . The water cycle . Global water distribution . Water source Volume (km$^3$) % of freshwater % of total water . Oceans, Seas, &amp; Bays | 1,338,000,000 | -- | 96.54 | . Ice caps, Glaciers,&amp; Permanent Snow | 24,064,000 | 68.7 | 1.74 | . Groundwater | 23,400,000 | -- | 1.69 | . $ quad$Fresh | 10,530,000 | 30.1 | 0.76 | . $ quad$Saline | 12,870,000 | -- | 0.93 | . Soil Moisture | 16,500 | 0.05 | 0.001 | . Ground Ice&amp; Permafrost | 300,000 | 0.86 | 0.022 | . Lakes | 176,400 | -- | 0.013 | . $ quad$Fresh | 91,000 | 0.26 | 0.007 | . $ quad$Saline | 85,400 | -- | 0.006 | . Atmosphere | 12,900 | 0.04 | 0.001 | . Swamp Water | 11,470 | 0.03 | 0.0008 | . Rivers | 2,120 | 0.006 | 0.0002 | . Biological Water | 1,120 | 0.003 | 0.0001 | . * (Percents are rounded, so will not add to 100) https://www.usgs.gov/special-topic/water-science-school/science/fundamentals-water-cycle . Energy drives the hydrologic cycle . A key aspect of the hydrologic cycle is the fact that it is driven by energy inputs (primarily from the sun). At the global scale, the system is essentially closed with respect to water; negligible water is entering or leaving the system. In other words, there is no external forcing in terms of a water flux. Systems with no external forcing will generally eventually come to an equilibrium state. So what makes the hydrologic cycle so dynamic? The solar radiative energy input, which is external to the system, drives the hydrologic cycle. Averaged over the globe, 342 W m$^{-2}$ of solar radiative energy is being continuously input to the system at the top of the atmosphere. This energy input must be dissipated, and this is done, to a large extent, via the hydrologic cycle. Due to this fact, the study of hydrology is not isolated to the study of water storage and movement, but also must often include study of energy storage and movements. . Margulis, 2017, &quot;Introduction to Hydrology&quot; . Components of the water cycle . Water storage in oceans . Evaporation / Sublimation . Evaporation $ longrightarrow$ cooling . . . . . Evapotranspiration . Water storage in the atmosphere . Cumulonimbus cloud over Africa . Picture of cumulonimbus taken from the International Space Station, over western Africa near the Senegal-Mali border. . If all of the water in the atmosphere rained down at once, it would only cover the globe to a depth of 2.5 centimeters. $$ begin{align} text{amount of water in the atmosphere} &amp; qquad V = 12 , 900 , text{km}^3 text{surface of Earth} &amp; qquad S = 4 pi R^2; quad R=6371 , text{km} &amp; qquad V = S times h text{height} &amp; qquad h = frac{V}{S} simeq 2.5 , text{cm} end{align} $$ . Try to calculate this yourself, and click on the button below to check how to do it. . #collapse-hide # amount of water in the atmosphere V = 12900 # km^3 # Earth&#39;s radius R = 6371 # km # surface of Earth = 4 pi RÀÜ2 S = 4 * 3.141592 * R**2 # Volume: V = S * h, therefore # height h = V / S # in km h_cm = h * 1e5 # in cm print(f&quot;The height would be ~ {h_cm:.1f} cm&quot;) . . The height would be ~ 2.5 cm . Condensation . Precipitation . Intensity (cm/h) Median diameter (mm) Velocity of fall (m/s) Drops s$^{-1}$ m$^{-2}$ . Fog | 0.013 | 0.01 | 0.003 | 67,425,000 | . Mist | 0.005 | 0.1 | 0.21 | 27,000 | . Drizzle | 0.025 | 0.96 | 4.1 | 151 | . Light rain | 0.10 | 1.24 | 4.8 | 280 | . Moderate rain | 0.38 | 1.60 | 5.7 | 495 | . Heavy rain | 1.52 | 2.05 | 6.7 | 495 | . Excessive rain | 4.06 | 2.40 | 7.3 | 818 | . Cloudburst | 10.2 | 2.85 | 7.9 | 1,220 | . Source: https://www.usgs.gov/special-topic/water-science-school/science/precipitation-and-water-cycle . Water storage in ice and snow . . Snowmelt runoff to streams . Surface runoff . . Streamflow . The Mississippi river basin is very large . The Amazon river basin is Huge . . Lakes and rivers . Lake Malawi . . Infiltration . Groundwater storage . . . Center Pivot irrigation in Nebraska taps the Ogallala Aquifer. . Groundwater flow and discharge . . . Spring . Ein Gedi . Thousand Springs, Idaho .",
            "url": "https://yairmau.github.io/jupyter/2020/02/01/introduction.html",
            "relUrl": "/jupyter/2020/02/01/introduction.html",
            "date": " ‚Ä¢ Feb 1, 2020"
        }
        
    
  
    
        ,"post32": {
            "title": "Hydrology --- introduction",
            "content": "How much water is there? Where? . . . The water cycle . Global water distribution . Water source Volume (km$^3$) % of freshwater % of total water . Oceans, Seas, &amp; Bays | 1,338,000,000 | -- | 96.54 | . Ice caps, Glaciers,&amp; Permanent Snow | 24,064,000 | 68.7 | 1.74 | . Groundwater | 23,400,000 | -- | 1.69 | . $ quad$Fresh | 10,530,000 | 30.1 | 0.76 | . $ quad$Saline | 12,870,000 | -- | 0.93 | . Soil Moisture | 16,500 | 0.05 | 0.001 | . Ground Ice&amp; Permafrost | 300,000 | 0.86 | 0.022 | . Lakes | 176,400 | -- | 0.013 | . $ quad$Fresh | 91,000 | 0.26 | 0.007 | . $ quad$Saline | 85,400 | -- | 0.006 | . Atmosphere | 12,900 | 0.04 | 0.001 | . Swamp Water | 11,470 | 0.03 | 0.0008 | . Rivers | 2,120 | 0.006 | 0.0002 | . Biological Water | 1,120 | 0.003 | 0.0001 | . * (Percents are rounded, so will not add to 100) https://www.usgs.gov/special-topic/water-science-school/science/fundamentals-water-cycle . Energy drives the hydrologic cycle . A key aspect of the hydrologic cycle is the fact that it is driven by energy inputs (primarily from the sun). At the global scale, the system is essentially closed with respect to water; negligible water is entering or leaving the system. In other words, there is no external forcing in terms of a water flux. Systems with no external forcing will generally eventually come to an equilibrium state. So what makes the hydrologic cycle so dynamic? The solar radiative energy input, which is external to the system, drives the hydrologic cycle. Averaged over the globe, 342 W m$^{-2}$ of solar radiative energy is being continuously input to the system at the top of the atmosphere. This energy input must be dissipated, and this is done, to a large extent, via the hydrologic cycle. Due to this fact, the study of hydrology is not isolated to the study of water storage and movement, but also must often include study of energy storage and movements. . Margulis, 2017, &quot;Introduction to Hydrology&quot; . Components of the water cycle . Water storage in oceans . Evaporation / Sublimation . Evaporation $ longrightarrow$ cooling . . . . . Evapotranspiration . Water storage in the atmosphere . Cumulonimbus cloud over Africa . Picture of cumulonimbus taken from the International Space Station, over western Africa near the Senegal-Mali border. . If all of the water in the atmosphere rained down at once, it would only cover the globe to a depth of 2.5 centimeters. $$ begin{align} text{amount of water in the atmosphere} &amp; qquad V = 12 , 900 , text{km}^3 text{surface of Earth} &amp; qquad S = 4 pi R^2; quad R=6371 , text{km} &amp; qquad V = S times h text{height} &amp; qquad h = frac{V}{S} simeq 2.5 , text{cm} end{align} $$ . Try to calculate this yourself, and click on the button below to check how to do it. . #collapse-hide # amount of water in the atmosphere V = 12900 # km^3 # Earth&#39;s radius R = 6371 # km # surface of Earth = 4 pi RÀÜ2 S = 4 * 3.141592 * R**2 # Volume: V = S * h, therefore # height h = V / S # in km h_cm = h * 1e5 # in cm print(f&quot;The height would be ~ {h_cm:.1f} cm&quot;) . . The height would be ~ 2.5 cm . Condensation . Precipitation . Intensity (cm/h) Median diameter (mm) Velocity of fall (m/s) Drops s$^{-1}$ m$^{-2}$ . Fog | 0.013 | 0.01 | 0.003 | 67,425,000 | . Mist | 0.005 | 0.1 | 0.21 | 27,000 | . Drizzle | 0.025 | 0.96 | 4.1 | 151 | . Light rain | 0.10 | 1.24 | 4.8 | 280 | . Moderate rain | 0.38 | 1.60 | 5.7 | 495 | . Heavy rain | 1.52 | 2.05 | 6.7 | 495 | . Excessive rain | 4.06 | 2.40 | 7.3 | 818 | . Cloudburst | 10.2 | 2.85 | 7.9 | 1,220 | . Source: https://www.usgs.gov/special-topic/water-science-school/science/precipitation-and-water-cycle . Water storage in ice and snow . . Snowmelt runoff to streams . Surface runoff . . Streamflow . The Mississippi river basin is very large . The Amazon river basin is Huge . . Lakes and rivers . Lake Malawi . . Infiltration . Groundwater storage . . . Center Pivot irrigation in Nebraska taps the Ogallala Aquifer. . Groundwater flow and discharge . . . Spring . Ein Gedi . Thousand Springs, Idaho .",
            "url": "https://yairmau.github.io/jupyter/2020/02/01/introduction-lecture.html",
            "relUrl": "/jupyter/2020/02/01/introduction-lecture.html",
            "date": " ‚Ä¢ Feb 1, 2020"
        }
        
    
  
    
        ,"post33": {
            "title": "Introduction - some first exercises in python",
            "content": "let&#39;s have fun plotting some data &#128512; . download the data . Go to the Faculty of Agriculture&#39;s weather station. | Click on ◊û◊©◊ô◊õ◊™ ◊†◊™◊ï◊†◊ô◊ù and download data for 1 September to 28 February, with a 24h interval. Call it data-sep2020-feb2021 | Open the .csv file with Excel, see how it looks like | If you can&#39;t download the data, just click here. | import packages . We need to import this data into python. First we import useful packages. Type (don&#39;t copy and paste) the following lines in the code cell below. . #collapse-hide import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns sns.set(style=&quot;ticks&quot;, font_scale=1.5) . . import data with pandas . Import data from csv and put it in a pandas dataframe (a table). Make line 5 the header (column names) . #collapse-hide df = pd.read_csv(&quot;data-sep2020-feb2021.csv&quot;, header=[4]) df . . Unnamed: 0 ÔøΩC ÔøΩC.1 km/h mm mm.1 . 0 01/09/20 | 32.8 | 25.3 | 29.7 | 0.0 | 0.0 | . 1 02/09/20 | 33.0 | 24.0 | 28.8 | 0.0 | 0.0 | . 2 03/09/20 | 34.2 | 23.8 | 31.6 | 0.0 | 0.0 | . 3 04/09/20 | 36.3 | 27.3 | 24.2 | 0.0 | 0.0 | . 4 05/09/20 | 34.2 | 26.3 | 22.4 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | . 176 24/02/21 | 20.6 | 9.9 | 28.8 | 0.0 | 481.7 | . 177 25/02/21 | 19.4 | 9.3 | 23.3 | 0.0 | 481.7 | . 178 26/02/21 | 21.3 | 8.0 | 24.2 | 0.1 | 481.8 | . 179 27/02/21 | 23.4 | 9.2 | 30.6 | 0.0 | 481.8 | . 180 28/02/21 | 19.7 | 9.2 | 22.4 | 0.0 | 481.8 | . 181 rows √ó 6 columns . rename columns . rename the columns to: date, tmax, tmin, wind, rain24h, rain_cumulative . #collapse-hide df.columns = [&#39;date&#39;, &#39;tmax&#39;, &#39;tmin&#39;, &#39;wind&#39;, &#39;rain24h&#39;, &#39;rain_cumulative&#39;] df . . date tmax tmin wind rain24h rain_cumulative . 0 01/09/20 | 32.8 | 25.3 | 29.7 | 0.0 | 0.0 | . 1 02/09/20 | 33.0 | 24.0 | 28.8 | 0.0 | 0.0 | . 2 03/09/20 | 34.2 | 23.8 | 31.6 | 0.0 | 0.0 | . 3 04/09/20 | 36.3 | 27.3 | 24.2 | 0.0 | 0.0 | . 4 05/09/20 | 34.2 | 26.3 | 22.4 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | . 176 24/02/21 | 20.6 | 9.9 | 28.8 | 0.0 | 481.7 | . 177 25/02/21 | 19.4 | 9.3 | 23.3 | 0.0 | 481.7 | . 178 26/02/21 | 21.3 | 8.0 | 24.2 | 0.1 | 481.8 | . 179 27/02/21 | 23.4 | 9.2 | 30.6 | 0.0 | 481.8 | . 180 28/02/21 | 19.7 | 9.2 | 22.4 | 0.0 | 481.8 | . 181 rows √ó 6 columns . a first plot! . plot the minimum temperature: . #collapse-hide plt.plot(df[&#39;tmin&#39;]) . . [&lt;matplotlib.lines.Line2D at 0x7fd02954bbd0&gt;] . how to deal with dates . We want the dates to appear on the horizontal axis. Interpret &#39;date&#39; column as a pandas datetime, see how it looks different from before before: 01/09/20 after: 2020-09-01 . #collapse-hide df[&#39;date&#39;] = pd.to_datetime(df[&#39;date&#39;], dayfirst=True) df . . date tmax tmin wind rain24h rain_cumulative . 0 2020-09-01 | 32.8 | 25.3 | 29.7 | 0.0 | 0.0 | . 1 2020-09-02 | 33.0 | 24.0 | 28.8 | 0.0 | 0.0 | . 2 2020-09-03 | 34.2 | 23.8 | 31.6 | 0.0 | 0.0 | . 3 2020-09-04 | 36.3 | 27.3 | 24.2 | 0.0 | 0.0 | . 4 2020-09-05 | 34.2 | 26.3 | 22.4 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | . 176 2021-02-24 | 20.6 | 9.9 | 28.8 | 0.0 | 481.7 | . 177 2021-02-25 | 19.4 | 9.3 | 23.3 | 0.0 | 481.7 | . 178 2021-02-26 | 21.3 | 8.0 | 24.2 | 0.1 | 481.8 | . 179 2021-02-27 | 23.4 | 9.2 | 30.6 | 0.0 | 481.8 | . 180 2021-02-28 | 19.7 | 9.2 | 22.4 | 0.0 | 481.8 | . 181 rows √ó 6 columns . date as dataframe index . Make &#39;date&#39; the dataframe&#39;s index (leftmost column, but not really a column!) . #collapse-hide df = df.set_index(&#39;date&#39;) df . . tmax tmin wind rain24h rain_cumulative . date . 2020-09-01 32.8 | 25.3 | 29.7 | 0.0 | 0.0 | . 2020-09-02 33.0 | 24.0 | 28.8 | 0.0 | 0.0 | . 2020-09-03 34.2 | 23.8 | 31.6 | 0.0 | 0.0 | . 2020-09-04 36.3 | 27.3 | 24.2 | 0.0 | 0.0 | . 2020-09-05 34.2 | 26.3 | 22.4 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | . 2021-02-24 20.6 | 9.9 | 28.8 | 0.0 | 481.7 | . 2021-02-25 19.4 | 9.3 | 23.3 | 0.0 | 481.7 | . 2021-02-26 21.3 | 8.0 | 24.2 | 0.1 | 481.8 | . 2021-02-27 23.4 | 9.2 | 30.6 | 0.0 | 481.8 | . 2021-02-28 19.7 | 9.2 | 22.4 | 0.0 | 481.8 | . 181 rows √ó 5 columns . plot again, now with dates . Plot minimum temperature, now we have dates on the horizontal axis . #collapse-hide plt.plot(df[&#39;tmin&#39;]) . . [&lt;matplotlib.lines.Line2D at 0x7fd0795aa190&gt;] . we&#39;re getting there! the graph could look better . Let&#39;s make the graph look better: labels, title, slanted dates, etc . #collapse-hide # creates figure (the canvas) and the axis (rectangle where the plot sits) fig, ax = plt.subplots(1, figsize=(10,7)) # two line plots ax.plot(df[&#39;tmin&#39;], color=&quot;red&quot;, label=&quot;Temp (min)&quot;) ax.plot(df[&#39;tmax&#39;], color=&quot;blue&quot;, label=&quot;Temp (max)&quot;) # axes labels and figure title ax.set_xlabel(&#39;date&#39;) ax.set_ylabel(&#39;temperature (¬∞C)&#39;) ax.set_title(&#39;maximum and minimum temperatures&#39;) # some ticks adjustments ax.set_yticks([10,15,20,25]) # we can choose where to put ticks ax.grid(axis=&#39;y&#39;) # makes horizontal lines plt.gcf().autofmt_xdate() # makes slated dates # legend ax.legend(loc=&#39;upper right&#39;) # save png figure plt.savefig(&quot;temp_max_min.png&quot;) . . make the following figure . Use the following function to plot bars for daily rainfall . ax.bar(x_array, y_array) . Can you write yourself some lines of code that calculate the cumulative rainfall from the daily rainfall? . #collapse-hide # creates figure (the canvas) and the axis (rectangle where the plot sits) fig, ax = plt.subplots(1, figsize=(10,7)) # line and bar plots ax.bar(df.index, df[&#39;rain24h&#39;], color=&quot;blue&quot;, label=&quot;daily rainfall&quot;) # there are many ways of calculating the cumulative rain # method 1, use a for loop: # rain = df[&#39;rain24h&#39;].to_numpy() # cumulative = rain * 0 # for i in range(len(rain)): # cumulative[i] = np.sum(rain[:i]) # df[&#39;cumulative1&#39;] = cumulative # method 2, use list comprehension: # rain = df[&#39;rain24h&#39;].to_numpy() # cumulative = [np.sum(rain[:i]) for i in range(len(rain))] # df[&#39;cumulative2&#39;] = cumulative # method 3, use existing functions: df[&#39;cumulative3&#39;] = np.cumsum(df[&#39;rain24h&#39;]) ax.plot(df[&#39;cumulative3&#39;], color=&quot;red&quot;, label=&quot;cumulative rainfall&quot;) # compare our cumulative rainfall with the downloaded data # ax.plot(df[&#39;rain_cumulative&#39;], &#39;x&#39;) # axes labels and figure title ax.set_xlabel(&#39;date&#39;) ax.set_ylabel(&#39;rainfall (mm)&#39;) ax.set_title(&#39;daily and cumulative rainfall&#39;) ax.set_xlim([&#39;2020-11-01&#39;,&#39;2021-02-28&#39;]) # some ticks adjustments plt.gcf().autofmt_xdate() # makes slated dates # legend ax.legend(loc=&#39;upper left&#39;) # save png figure plt.savefig(&quot;cumulative_rainfall.png&quot;) . . make another figure . In order to choose just a part of the time series, you can use the following: . start_date = &#39;2021-01-01&#39; end_date = &#39;2021-01-31&#39; january = df[start_date:end_date] . #collapse-hide # creates figure (the canvas) and the axis (rectangle where the plot sits) fig, ax = plt.subplots(1, figsize=(10,7)) # define date range start_date = &#39;2021-01-01&#39; end_date = &#39;2021-01-31&#39; january = df[start_date:end_date][&#39;tmax&#39;] # plots ax.plot(january, color=&quot;red&quot;, label=&quot;daily max&quot;) ax.plot(january*0 + january.mean(), color=&quot;purple&quot;, linestyle=&quot;--&quot;, label=&quot;average daily max&quot;) # axes labels and figure title ax.set_xlabel(&#39;date&#39;) ax.set_ylabel(&#39;temperature (¬∞C)&#39;) ax.set_title(&#39;average daily maximum temperature for January 2021&#39;) # some ticks adjustments plt.gcf().autofmt_xdate() # makes slated dates # legend ax.legend(loc=&#39;lower left&#39;) # save png figure plt.savefig(&quot;average_max_temp.png&quot;) . . one last figure for today . Use the following code to create histograms with user-defined bins: . b = np.arange(0, 56, 5) # bins from 0 to 55, width = 5 ax.hist(df[&#39;wind&#39;], bins=b, density=True) . Play with the bins, see what happens. What does density=True do? . #collapse-hide # creates figure (the canvas) and the axis (rectangle where the plot sits) fig, ax = plt.subplots(1, figsize=(10,7)) # histogram b = np.arange(0, 56, 5) # bins from 0 to 55, width = 5 ax.hist(df[&#39;wind&#39;], bins=b, density=True) # axes labels and figure title ax.set_xlabel(&#39;max wind speed (km/h)&#39;) ax.set_ylabel(&#39;frequency&#39;) ax.set_title(&#39;frequency of maximum wind speed&#39;) # save png figure plt.savefig(&quot;wind-histogram.png&quot;) . . homework . Go back to the weather station website, download one year of data from 01.01.2020 to 31.12.2020 (24h data). If you can&#39;t download the data, just click here. Make the following graph: . daily tmax and tmin | smoothed data for tmax and tmin | . In order to smooth the data with a 30 day window, use the following function: df[&#39;tmin&#39;].rolling(30, center=True).mean() This means that you will take the mean of 30 days, and put the result in the center of this 30-day window. . Play with this function, see what you can do with it. What happens when you change the size of the window? Why is the smoothed data shorter than the original data? See the documentation for rolling to find more options. . #collapse-hide fig, ax = plt.subplots(figsize=(10,7)) df2 = pd.read_csv(&quot;1year.csv&quot;, header=[4]) df2[&#39;date&#39;] = pd.to_datetime(df2[&#39;date&#39;], dayfirst=True) df2 = df2.set_index(&#39;date&#39;) plt.plot(df2[&#39;tmax&#39;], label=&#39;tmax&#39;, color=&quot;tab:red&quot;) plt.plot(df2[&#39;tmin&#39;], label=&#39;tmin&#39;, color=&quot;tab:blue&quot;) tmin_smooth = df2[&#39;tmin&#39;].rolling(30, center=True).mean() tmax_smooth = df2[&#39;tmax&#39;].rolling(30, center=True).mean() plt.plot(tmax_smooth, label=&#39;tmax smoothed&#39;, color=&quot;tab:pink&quot;, linestyle=&quot;--&quot;, linewidth=3) plt.plot(tmin_smooth, label=&#39;tmin smoothed&#39;, color=&quot;tab:cyan&quot;, linestyle=&quot;--&quot;, linewidth=3) plt.legend() plt.savefig(&quot;t_smoothed.png&quot;) . .",
            "url": "https://yairmau.github.io/jupyter/2020/02/01/introduction-exercises.html",
            "relUrl": "/jupyter/2020/02/01/introduction-exercises.html",
            "date": " ‚Ä¢ Feb 1, 2020"
        }
        
    
  
    
        ,"post34": {
            "title": "Final Assignment",
            "content": "&#128210; instructions . This is where learning happens, not during a lecture. You&#39;ll learn a ton of things by doing them yourself. Much success! üòÑ . Create two Jupyter Notebooks called . assignment-FINAL-CODE-IDNUMBER, and | assignment-FINAL-REPORT-IDNUMBER, where IDNUMBER is your 9-digit ID. These are the only files we will check. | &#128204; locations and data . Choose one location in the US. . Download relevant data from NOAA&#39;s Global Summary of the Month, NOAA&#39;s Climate Reference Network Data, and from the USGS&#39;s National Water Information System. . Try to find locations with many years of data, the more the better. Take some time to choose your station, plan well. Choose a location you have not worked with in past assignments. . &#128736; tasks . In this final project, we will integrate the various topics we learned throughout the semester. You will tell a story about the location you chose, and describe the changes it experienced in the past many decades. You can focus on any kind of changes that would influence the hydrological fluxes we learned about. Here are a few examples of changes that you might work on: . severe droughts in part of the studied period, or an increasing trend in drought severity. | same as above for rainfall/floods, high temperatures, low temperatures, etc. | significant changes in land use, such as urbanization, deforestation, agricultural practices, etc. | . The list above is not comprehensive, you can choose other factors. Consult with me in case of doubt. . Try to find on the media and in scientific papers evidence for the change you are focusing on. Cite these sources: at least one peer-reviewed scientific paper, and at least 3 other sources, such as a government website, official weather sites, books, reputable news websites, etc. . Can you see the same when analyzing data for the location you chose? Do your findings corroborate the expectation you had when you started this project? If they don&#39;t, can you explain why? Did you reach interesting or surprising conclusions in your analysis? . Analyze your location&#39;s history with respect to the following: . Precipitation: seasonality, inter-annual variability, extreme precipitation events and return periods. | Potential evapotranspiration: Calculate PET using Penman&#39;s equation for at least three different years of interest (not necessarily contiguous years). Calculate Thornthwaite&#39;s PET for the whole length of the available data (comment about the suitability of Thornthwaite&#39;s PET to the location you chose). | Analyze streamflow statistics in a similar manner as for precipitation: extreme discharge events and return periods. | Use Budyko&#39;s framework to calculate where the location you chose falls on the $(ET/P,PET/P)$ space for at least three different years of interest. | . Try to connect the dots: how do your different findings fit together? Discuss what you are trying to show, tell your story with the help of the data and your analyses. If you find things that go contrary to your expectations, can you raise hypotheses of why you see what you see? . You will have one month to hand in your project. Much success! üòÅ . &#127749; presentation . All the assignment must be in two Jupyter Notebooks. . The notebook called CODE will contain all the code for the analyses you made. It must be fully functional, i.e., we must be able to Run All and not get any errors. Explain what you are doing in each step. Comment your code. Use markdown cells to split the notebook into subsections, one for each analysis (e.g.: ## Precipitation Analysis, ### Inter-annual variability, etc). . The notebook called Report will contain graphs and relevant data from the CODE notebook. It is here where you will introduce the location you chose, what you are trying to see. Here you will write all the results and discussion, as supported by the graphs and results you produced. Divide this notebook into sections: Introduction, Results and Discussion, Conclusion. Subdivide the sections into subsections when needed. In this file there should be no code at all. . You can write in English or in Hebrew, but the text in the figures must be in English. If you choose to write the discussion in Hebrew, be aware that Jupyter Notebooks don&#39;t have native right-to-left language support: . ◊†◊ô◊™◊ü ◊ú◊õ◊™◊ï◊ë ◊ë◊¢◊ë◊®◊ô◊™, ◊ú◊û◊®◊ï◊™ ◊©◊ñ◊î ◊ú◊ê ◊†◊®◊ê◊î ◊õ◊¥◊õ ◊ò◊ï◊ë... . You can use some HTML code to achieve best results in Hebrew. Type the following . &lt;p dir=&quot;rtl&quot; style=&quot;text-align: right;&quot;&gt; ◊¢◊õ◊©◊ô◊ï ◊î◊®◊ë◊î ◊ô◊ï◊™◊® ◊ò◊ï◊ë! &lt;/p&gt; . to get . ◊¢◊õ◊©◊ô◊ï ◊î◊®◊ë◊î ◊ô◊ï◊™◊® ◊ò◊ï◊ë! . If you have many paragraphs in hebrew, do the following: . ◊§◊°◊ß◊î ◊û◊°◊§◊® 1. . ◊§◊°◊ß◊î ◊û◊°◊§◊® 2. . ◊ê◊ù ◊ô◊© ◊ú◊õ◊ù ◊õ◊û◊î ◊§◊°◊ß◊ê◊ï◊™, ◊õ◊ú ◊ê◊ó◊™ ◊û◊î◊ü ◊™◊î◊ô◊î ◊ë◊™◊ï◊ö &quot;dir&quot; ◊û◊©◊ú◊î . In my opinion it is too complicated to write in Hebrew in Jupyter Notebooks, just write in English, your grade will not be affected by typos nor less-than-perfect English proficiency. . &#128175; evaluation . Your assignment will be evaluated according to the following criteria: . 40% Presentation. How the graphs look, labels, general organization, markdown, clean code. | 30% Discussion. This is where you explain what you did, what you found out, etc. | 15% Depth of analysis. You can analyze/explore the data with different levels of complexity, this is where we take that into consideration. | 10% Replicability: Your code runs flawlessly. | 5%: Code commenting. Explain in your code what you are doing, this is good for everyone, especially for yourself! | . &#128666; importing the data . You can use the code from previous assignments and from the exercise lectures. .",
            "url": "https://yairmau.github.io/jupyter/2020/02/01/assignment-FINAL.html",
            "relUrl": "/jupyter/2020/02/01/assignment-FINAL.html",
            "date": " ‚Ä¢ Feb 1, 2020"
        }
        
    
  
    
        ,"post35": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://yairmau.github.io/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  
    
        ,"post36": {
            "title": "This website's logo",
            "content": ". Introduction . This will only work if you have Mathematica installed in your machine. Python can interface with Wolfram Mathematica, taking advantage of its awesome power. . The curve inside the square is a parabola: $$ y = ax^2 + bx + c $$ . This parabola passes through the points $(0.3, 0)$ and $(1, 1)$, and it&#39;s derivative at $(0.3, 0)$ is zero. We use Mathematica to figure out what are the parameters $a,b,c$. Finally, we transpose the line when plotting, i.e., $x$ is in the vertical axis, and $y$ is in the horizontal axis. . The code . %matplotlib inline import matplotlib.pyplot as plt import numpy as np from wolframclient.language import wl from wolframclient.evaluation import WolframLanguageSession from wolframclient.language import wl, wlexpr w = 20 plt.rc(&#39;axes&#39;, linewidth=w) fig=plt.figure(1, (5, 5)) fig.subplots_adjust(left=0.0, right=1.0, top=1.0, bottom=0.0, hspace=0, wspace=0) ax = plt.Axes(fig, [0., 0., 1., 1.]) fig.add_axes(ax) session = WolframLanguageSession() session.evaluate(wlexpr(&#39;y[x_] := a x^2 + b x + c&#39;)) session.evaluate(wlexpr(&#39;p1 = {0.3, 0}&#39;)) session.evaluate(wlexpr(&#39;p2 = {1, 1}&#39;)) session.evaluate(wlexpr(&#39;sol1 = Solve[ {y[p1[[1]]] == p1[[2]], y[p2[[1]]] == p2[[2]]}, {a, b, c}][[1]]&#39;)) session.evaluate(wlexpr(&#39;sol2 = Solve[(D[(y[x] /. sol1), x] /. x -&gt; 0.3) == 0, a][[1]]&#39;)) par = list(session.evaluate(wlexpr(&#39;{a, b, c} /. sol1 /. sol2&#39;))) x0 = 0.3 x=np.linspace(x0, 1, 1001) a, b, c = par# [2.08163, -1.24898, 0.167347] y = lambda x: a*x**2 + b*x + c c1 = &#39;white&#39; # bottom right c2 = &#39;white&#39; # top left # c1 = &#39;#6c7053&#39; # bottom right # c2 = &#39;#6e0014&#39; # top left # ax.fill_between(y(x), x, y2=0, facecolor=c1, # edgecolor=&#39;black&#39;) # bottom right # ax.fill_between(y(x), x, y2=1, facecolor=c2, # edgecolor=&quot;black&quot;, linewidth=10) # top left ax.plot(y(x), x, color=&quot;black&quot;, lw=w) ax.set_xlim([0,1]) ax.set_ylim([0,1]) ax.set_xticks([]) ax.set_yticks([]) fig.savefig(&quot;./python_figures/site-logo.png&quot;, resolution=600, transparent=True, bbox_inches=&quot;tight&quot;) fig.savefig(&quot;./python_figures/site-logo.svg&quot;) plt.show() . Equations may not give solutions for all &#34;solve&#34; variables. Equations may not give solutions for all &#34;solve&#34; variables. . session = WolframLanguageSession() session.evaluate(wlexpr(&#39;y[x_] := a x^2 + b x + c&#39;)) session.evaluate(wlexpr(&#39;p1 = {0.3, -0.02}&#39;)) session.evaluate(wlexpr(&#39;p2 = {1, 1}&#39;)) session.evaluate(wlexpr(&#39;sol1 = Solve[ {y[p1[[1]]] == p1[[2]], y[p2[[1]]] == p2[[2]]}, {a, b, c}][[1]]&#39;)) session.evaluate(wlexpr(&#39;sol2 = Solve[(D[(y[x] /. sol1), x] /. x -&gt; 0.3) == 0, a][[1]]&#39;)) par = list(session.evaluate(wlexpr(&#39;{a, b, c} /. sol1 /. sol2&#39;))) . Equations may not give solutions for all &#34;solve&#34; variables. Equations may not give solutions for all &#34;solve&#34; variables. . par . [2.0816326530612246, -1.2489795918367346, 0.16734693877551027] .",
            "url": "https://yairmau.github.io/jupyter/2020/01/01/websitelogo.html",
            "relUrl": "/jupyter/2020/01/01/websitelogo.html",
            "date": " ‚Ä¢ Jan 1, 2020"
        }
        
    
  
    
        ,"post37": {
            "title": "The time-dependent Ginzburg-Landau equation",
            "content": ". Introduction . Simulation of the Time-Dependent Ginzburg-Landau Equation $$ frac{ text{d}u}{ text{d}t}= u - u^3 +D nabla^2 u,$$ in 1 and 2 spatial dimensions. This is the simplest example of numerical integration through Finite Differences: . Euler method to advance time | Five-point stencil to compute the laplacian, periodic boundary conditions are assumed. See an example of the output here: https://www.youtube.com/watch?v=JgE9Px7zsQE | . Code . import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.axes_grid1 import make_axes_locatable import time as tm . # grid size n = 128 # for 1d simulation write N=(n,) N=(n,n) # diffusion coefficient D = 1.0 # spatial dimensions L = 100.0 dx = L / n x = np.arange(0,L,dx) # time t = 0.0 total_time = 3.0 # beware of the Von Neumann stability analysis # https://en.wikipedia.org/wiki/Von_Neumann_stability_analysis dt = 0.2 * 0.5 * dx**2 / D . define functions . def periodic_lap_1d(u,dx=1.0): return (+1*np.roll(u,+1) +1*np.roll(u,-1) -2*u) / dx**2 def periodic_lap_2d(u,dx=1.0): return (+1*np.roll(u,+1,axis=0) +1*np.roll(u,-1,axis=0) +1*np.roll(u,+1,axis=1) +1*np.roll(u,-1,axis=1) -4*u) / dx**2 f = lambda u: u - u**3 . initialize and start plotting . plt.ion() fig = plt.figure(1,figsize=(7,6)) plt.clf() ax = fig.add_subplot(111) # random initial condition u = 2*np.random.random(N)-1.0 if len(N) == 1: lap = periodic_lap_1d p, = ax.plot(x,u) ax.axis([x[0],x[-1],-1.1,1.1]) if len(N) == 2: lap = periodic_lap_2d p = ax.imshow(u,cmap=&quot;RdGy&quot;, vmin=-1.0, vmax=1.0,extent=[0,L,0,L]) # create an axes on the right side of ax. The width of cax will be 5% # of ax and the padding between cax and ax will be fixed at 0.15 inch. divider = make_axes_locatable(ax) colorbar_ax = divider.append_axes(&quot;right&quot;, size=&quot;5%&quot;, pad=0.15) cbar = fig.colorbar(p, cax=colorbar_ax, ticks=[-1,-0.5,0,0.5,1]) ax.set_title(&quot;time={:5.1f}&quot;.format(0.0)) . Text(0.5, 1.0, &#39;time= 0.0&#39;) . start simulation . while t&lt;total_time: t += dt u = u + dt * (f(u) + D * lap(u,dx) ) # we don&#39;t need to plot again, just to update the data of the plot if len(N) == 1: p.set_data(x,u) if len(N) == 2: p.set_data(u) ax.set_title(&quot;time={:5.1f}&quot;.format(t)) fig.canvas.draw() .",
            "url": "https://yairmau.github.io/jupyter/2020/01/01/tdgle.html",
            "relUrl": "/jupyter/2020/01/01/tdgle.html",
            "date": " ‚Ä¢ Jan 1, 2020"
        }
        
    
  
    
        ,"post38": {
            "title": "Fancy subplot grid",
            "content": ". Introduction . With GridSpec you can create any combination of panels . The code . import numpy as np import matplotlib import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec from matplotlib.ticker import FuncFormatter . # http://wiki.scipy.org/Cookbook/Matplotlib/LaTeX_Examples # this is a latex constant, don&#39;t change it. pts_per_inch = 72.27 # write &quot; the textwidth&quot; (or &quot; showthe columnwidth&quot; for a 2 collumn text) text_width_in_pts = 246.0 # inside a figure environment in latex, the result will be on the # dvi/pdf next to the figure. See url above. text_width_in_inches = text_width_in_pts / pts_per_inch # make rectangles with a nice proportion golden_ratio = 0.618 # figure.png or figure.eps will be intentionally larger, because it is prettier inverse_latex_scale = 2 # when compiling latex code, use # includegraphics[scale=(1/inverse_latex_scale)]{figure} # we want the figure to occupy 2/3 (for example) of the text width fig_proportion = (3.0 / 3.0) csize = inverse_latex_scale * fig_proportion * text_width_in_inches # always 1.0 on the first argument fig_size = (1.0 * csize, 1.1 * csize) # find out the fontsize of your latex text, and put it here text_size = inverse_latex_scale * 12 tick_size = inverse_latex_scale * 8 # learn how to configure: # http://matplotlib.sourceforge.net/users/customizing.html params = {&#39;backend&#39;: &#39;ps&#39;, &#39;axes.labelsize&#39;: text_size, &#39;legend.fontsize&#39;: tick_size, &#39;legend.handlelength&#39;: 2.5, &#39;legend.borderaxespad&#39;: 0, &#39;xtick.labelsize&#39;: tick_size, &#39;ytick.labelsize&#39;: tick_size, &#39;font.family&#39;: &#39;serif&#39;, &#39;font.size&#39;: text_size, # Times, Palatino, New Century Schoolbook, # Bookman, Computer Modern Roman &#39;font.serif&#39;: [&#39;Times&#39;], &#39;ps.usedistiller&#39;: &#39;xpdf&#39;, &#39;text.usetex&#39;: True, &#39;figure.figsize&#39;: fig_size, # include here any neede package for latex &#39;text.latex.preamble&#39;: [r&#39; usepackage{amsmath}&#39;, ], } plt.rcParams.update(params) plt.ioff() plt.clf() # figsize accepts only inches. fig = plt.figure(1, figsize=fig_size) gs = gridspec.GridSpec(3, 2, width_ratios=[1,0.5], height_ratios=[1,0.7,0.3]) gs.update(left=0.16, right=0.86,top=0.92, bottom=0.08, hspace=0.05, wspace=0.05) . subplot a . ax0 = plt.subplot(gs[0, :]) heaviside = lambda x: 0.5 * (np.sign(x) + 1) x = np.arange(0, 10.01, 0.01) ax0.plot(x, heaviside(x - 2), color=&#39;purple&#39;, lw=3) ax0.text(2.5, 1.1, r&quot;$ longleftarrow$ heaviside&quot;) # y ticks as a percentage ax0.set_yticks(np.arange(-0.5, 2.0, 0.5)) def to_percent(y, position): # Ignore the passed in position. This has the effect of scaling the default # tick locations. s = &quot;{:+.0f}&quot;.format(y * 100) # str(100 * y) # The percent symbol needs escaping in latex if matplotlib.rcParams[&#39;text.usetex&#39;] is True: return s + r&#39;$ %$&#39; else: return s + &#39;%&#39; # Create the formatter using the function to_percent. This multiplies all the # default labels by 100, making them all percentages formatter = FuncFormatter(to_percent) # Set the formatter ax0.yaxis.set_major_formatter(formatter) ax0.set_ylabel(&quot;heaviside, percentage&quot;) # x ticks on top ax0.axis([x.min(), x.max(), -0.5, 1.5]) ax0.xaxis.tick_top() ax0.set_xlabel(r&quot;x labels on top&quot;) ax0.xaxis.set_label_position(&quot;top&quot;) # transAxes makes position relative to axes ax0.text(0.97, 0.97, r&quot; textbf{a}&quot;, transform=ax0.transAxes, horizontalalignment=&#39;right&#39;, verticalalignment=&#39;top&#39;) # copy window with same x axis (y will be different) ax0b = ax0.twinx() ax0b.plot(x, np.tanh(x - 5), color=&quot;green&quot;, linewidth=3) ax0b.axis([x.min(), x.max(), -1.1, 2.5]) ax0b.text(5.5, 0, r&quot;tanh $ longrightarrow$&quot;) ax0b.set_ylabel(r&#39;tanh, offset label&#39;) ax0b.yaxis.set_label_coords(1.1, 0.70) . subplot b . ax10 = plt.subplot(gs[1, 0]) x = np.arange(-5, 5, 0.01) y = np.exp(-x) ax10.plot(x, y, color=&quot;orange&quot;, lw=3) ax10.set_yscale(&#39;log&#39;, basey=2) ax10.set_yticks(2.0 ** np.arange(-7, 7, 3)) ax10.text(1.0, 1, r&quot;$y=e^{-x}$&quot;) ax10.set_xticks(np.arange(-5, 6, 2)) ax10.set_xticklabels(np.arange(-5, 6, 2), y=0.15) ax10.get_yaxis().set_tick_params(direction=&#39;out&#39;) ax10.set_ylabel(&quot;log scale base 2&quot;, labelpad=15) ax10.text(0.97, 0.97, r&quot; textbf{b}&quot;, transform=ax10.transAxes, horizontalalignment=&#39;right&#39;, verticalalignment=&#39;top&#39;) . Text(0.97, 0.97, &#39; textbf{b}&#39;) . subplot c . ax11 = plt.subplot(gs[1, 1]) x = np.arange(1.0, np.e ** 4, 0.01) y = x ** (-0.8) ax11.plot(x, y, color=&quot;cyan&quot;, lw=3) ax11.text(2, 1, r&quot;$y=x^{-0.8}$&quot;, fontsize=tick_size) ax11.loglog(x, y, basex=np.e, basey=np.e) xt = np.exp(np.arange(1, 4, 1)) yt = np.pi ** (np.arange(-3, 2, 1)) ax11.set_xticks(xt) ax11.set_xticklabels(xt, y=0.15) ax11.set_yticks(yt) def ticks_e(y, pos): # base e return r&#39;$e^{:.0f}$&#39;.format(np.log(y)) def ticks_pi(y, pos): # base pi, why not? return r&#39;$ pi^{%+.0f}$&#39;%(np.log(y)/np.log(np.pi)) ax11.xaxis.set_major_formatter(FuncFormatter(ticks_e)) ax11.yaxis.set_major_formatter(FuncFormatter(ticks_pi)) ax11.yaxis.tick_right() ax11.yaxis.set_label_position(&quot;right&quot;) ax11.set_ylabel(&quot;right side&quot;, labelpad=10) ax11.text(0.97, 0.97, r&quot; textbf{c}&quot;, transform=ax11.transAxes, horizontalalignment=&#39;right&#39;, verticalalignment=&#39;top&#39;) . Text(0.97, 0.97, &#39; textbf{c}&#39;) . subplot d . ax20 = plt.subplot(gs[2, 0]) ax20.axis([0, 1, 0, 1]) ax20.set_xticks(np.arange(0, 1.1, 0.2)) ax20.set_xticklabels([&quot;January&quot;, &quot;February&quot;, &quot;March&quot;, &quot;April&quot;, &quot;May&quot;, &quot;June&quot;], rotation=30, horizontalalignment=&quot;right&quot;) ax20.set_yticks([]) ax20.text(0.97, 0.97, r&quot; textbf{d}&quot;, transform=ax20.transAxes, horizontalalignment=&#39;right&#39;, verticalalignment=&#39;top&#39;) . Text(0.97, 0.97, &#39; textbf{d}&#39;) . subplot e . ax21 = plt.subplot(gs[2, 1]) ax21.set_xticks([]) ax21.set_yticks([]) ax21.axis([0, 1, 0, 1]) ax21.text(0.97, 0.97, r&quot; textbf{e}&quot;, transform=ax21.transAxes, horizontalalignment=&#39;right&#39;, verticalalignment=&#39;top&#39;) . Text(0.97, 0.97, &#39; textbf{e}&#39;) . %matplotlib notebook fig.savefig(&quot;./python_figures/subplot-grid.png&quot;, dpi=300) fig .",
            "url": "https://yairmau.github.io/jupyter/2020/01/01/subplotgrid.html",
            "relUrl": "/jupyter/2020/01/01/subplotgrid.html",
            "date": " ‚Ä¢ Jan 1, 2020"
        }
        
    
  
    
        ,"post39": {
            "title": "Streamplot",
            "content": ". Introduction . Streamplot of a two-dimensional linear system, with eigenvectors and nullclines. Python shows LaTeX equations beautifully. Main features: meshgrid, streamplot, contour, legend, LaTeX . The code . %matplotlib notebook import matplotlib import matplotlib.pyplot as plt import numpy as np . make graph look pretty . # http://wiki.scipy.org/Cookbook/Matplotlib/LaTeX_Examples # this is a latex constant, don&#39;t change it. pts_per_inch = 72.27 # write &quot; the textwidth&quot; (or &quot; showthe columnwidth&quot; for a 2 collumn text) text_width_in_pts = 300.0 # inside a figure environment in latex, the result will be on the # dvi/pdf next to the figure. See url above. text_width_in_inches = text_width_in_pts / pts_per_inch # make rectangles with a nice proportion golden_ratio = 0.618 # figure.png or figure.eps will be intentionally larger, because it is prettier inverse_latex_scale = 2 # when compiling latex code, use # includegraphics[scale=(1/inverse_latex_scale)]{figure} # we want the figure to occupy 2/3 (for example) of the text width fig_proportion = (3.0 / 3.0) csize = inverse_latex_scale * fig_proportion * text_width_in_inches # always 1.0 on the first argument fig_size = (1.0 * csize, 0.8 * csize) # find out the fontsize of your latex text, and put it here text_size = inverse_latex_scale * 12 tick_size = inverse_latex_scale * 8 # learn how to configure: # http://matplotlib.sourceforge.net/users/customizing.html params = {&#39;backend&#39;: &#39;ps&#39;, &#39;axes.labelsize&#39;: text_size, &#39;legend.fontsize&#39;: tick_size, &#39;legend.handlelength&#39;: 2.5, &#39;legend.borderaxespad&#39;: 0, &#39;xtick.labelsize&#39;: tick_size, &#39;ytick.labelsize&#39;: tick_size, &#39;font.family&#39;: &#39;serif&#39;, &#39;font.size&#39;: text_size, # Times, Palatino, New Century Schoolbook, # Bookman, Computer Modern Roman # &#39;font.serif&#39;: [&#39;Times&#39;], &#39;ps.usedistiller&#39;: &#39;xpdf&#39;, &#39;text.usetex&#39;: True, &#39;figure.figsize&#39;: fig_size, # include here any neede package for latex &#39;text.latex.preamble&#39;: [r&#39; usepackage{amsmath}&#39;, ], } plt.rcParams.update(params) plt.ioff() # figsize accepts only inches. fig = plt.figure(1, figsize=fig_size) fig.subplots_adjust(left=0.10, right=0.97, top=0.82, bottom=0.10, hspace=0.02, wspace=0.02) ax = fig.add_subplot(111) . define parameters, system of equations, and equation for eigenvectors . # parameters as a dictionary p = {&#39;a&#39;: -1.0, &#39;b&#39;: +0.2, &#39;c&#39;: +1.2, &#39;d&#39;: -1.5} # the equations def system_equations(x,y): return [p[&#39;a&#39;] * x + p[&#39;b&#39;] * y, p[&#39;c&#39;] * x + p[&#39;d&#39;] * y, ] # eigenvectors eigen_vec = 100 * np.array([ [(p[&#39;a&#39;] - p[&#39;d&#39;] - np.sqrt((p[&#39;a&#39;] - p[&#39;d&#39;]) ** 2 + 4.0 * p[&#39;b&#39;] * p[&#39;c&#39;])) / (2.0 * p[&#39;c&#39;]), 1.0], [(p[&#39;a&#39;] - p[&#39;d&#39;] + np.sqrt((p[&#39;a&#39;] - p[&#39;d&#39;]) ** 2 + 4.0 * p[&#39;b&#39;] * p[&#39;c&#39;])) / (2.0 * p[&#39;c&#39;]), 1.0], ]) . there are two equivalent ways to build a mesh, choose the one that makes more sense to you... . min_x, max_x = [-1, 1] min_y, max_y = [-4, 4] divJ = 50j div = 50 # 1st way # Y, X = np.mgrid[min_y:max_y:div,min_x:max_x:div] # 2nd way X, Y = np.meshgrid(np.linspace(min_x, max_x, div), np.linspace(min_y, max_y, div)) # streamplot density = 2 * [0.80] minlength = 0.2 arrow_color = 3 * [0.5] ax.streamplot(X, Y, system_equations(X, Y)[0], system_equations(X, Y)[1], density=density, color=arrow_color, arrowsize=2, linewidth=2, minlength=minlength) . &lt;matplotlib.streamplot.StreamplotSet at 0x7ff7c5afdd50&gt; . nullclines . null_0 = ax.contour(X, Y, system_equations(X, Y)[0], levels=[0], colors=&#39;black&#39;, linewidths=3) null_1 = ax.contour(X, Y,system_equations(X, Y)[1], levels=[0], colors=&#39;blue&#39;, linewidths=3) n0 = null_0.collections[0] n1 = null_1.collections[0] . eigenvectors . eigen_0, = ax.plot([eigen_vec[0, 0],-eigen_vec[0, 0]], [eigen_vec[0, 1],-eigen_vec[0, 1]], color=&#39;red&#39;, lw=2, ls=&quot;--&quot;) eigen_1, = ax.plot([eigen_vec[1, 0],-eigen_vec[1, 0]], [eigen_vec[1, 1],-eigen_vec[1, 1]], color=&#39;orange&#39;, lw=2, ls=&quot;--&quot;) dash = (15, 10, 15, 10) eigen_0.set_dashes(dash) eigen_1.set_dashes(dash) . some labels, legend, and text . ax.set_ylabel(r&quot;$y$&quot;, rotation=&#39;horizontal&#39;) ax.set_xlabel(r&quot;$x$&quot;, labelpad=5) ax.legend([n0, n1, eigen_0, eigen_1], [r&#39;$dx/dt=0$&#39;, r&#39;$dy/dt=0$&#39;, &quot;eigenvector 1&quot;, &quot;eigenvector 2&quot;], loc=&quot;lower right&quot;, frameon=True, fancybox=False, shadow=False, ncol=2, borderpad=0.5, labelspacing=0.5, handlelength=3, handletextpad=0.1, borderaxespad=0.3, columnspacing=2) ax.text(-1.0, 4.3, (r&quot;$ frac{d}{dt} begin{pmatrix}x y end{pmatrix}=$&quot; r&quot;$ begin{pmatrix}a&amp;b c&amp;d end{pmatrix} cdot$&quot; r&quot;$ begin{pmatrix}x y end{pmatrix}$&quot;)) ax.text(0.1, 5.0, r&quot;$a={:.1f} qquad b={:.1f}$ &quot;.format(p[&#39;a&#39;], p[&#39;b&#39;])) ax.text(0.1, 4.3, r&quot;$c={:.1f} qquad d={:.1f}$ &quot;.format(p[&#39;c&#39;], p[&#39;d&#39;])) ax.axis([min_x, max_x, min_y, max_y]) fig.savefig(&quot;python_figures/streamplot.png&quot;, resolution=300) plt.draw() fig .",
            "url": "https://yairmau.github.io/jupyter/2020/01/01/streamplot.html",
            "relUrl": "/jupyter/2020/01/01/streamplot.html",
            "date": " ‚Ä¢ Jan 1, 2020"
        }
        
    
  
    
        ,"post40": {
            "title": "Least Squares",
            "content": ". Introduction . This code produces the figure above. It&#39;s main tool is the curve_fit method, that allows us to fit any function to data, and get optimal parameter values. . The code . %matplotlib notebook import matplotlib.pyplot as plt import numpy as np import matplotlib.gridspec as gridspec import scipy.special from scipy.optimize import curve_fit import matplotlib.patches as patches . Make graph look pretty . %%capture out %matplotlib notebook # http://wiki.scipy.org/Cookbook/Matplotlib/LaTeX_Examples # this is a latex constant, don&#39;t change it. pts_per_inch = 72.27 # write &quot; the textwidth&quot; (or &quot; showthe columnwidth&quot; for a 2 collumn text) text_width_in_pts = 300.0 # inside a figure environment in latex, the result will be on the # dvi/pdf next to the figure. See url above. text_width_in_inches = text_width_in_pts / pts_per_inch # make rectangles with a nice proportion golden_ratio = 0.618 # figure.png or figure.eps will be intentionally larger, because it is prettier inverse_latex_scale = 2 # when compiling latex code, use # includegraphics[scale=(1/inverse_latex_scale)]{figure} # we want the figure to occupy 2/3 (for example) of the text width fig_proportion = (3.0 / 3.0) csize = inverse_latex_scale * fig_proportion * text_width_in_inches # always 1.0 on the first argument fig_size = (1.0 * csize, 0.85 * csize) # find out the fontsize of your latex text, and put it here text_size = inverse_latex_scale * 12 label_size = inverse_latex_scale * 10 tick_size = inverse_latex_scale * 8 # learn how to configure: # http://matplotlib.sourceforge.net/users/customizing.html params = {#&#39;backend&#39;: &#39;ps&#39;, &#39;axes.labelsize&#39;: 16, &#39;legend.fontsize&#39;: tick_size, &#39;legend.handlelength&#39;: 2.5, &#39;legend.borderaxespad&#39;: 0, &#39;axes.labelsize&#39;: label_size, &#39;xtick.labelsize&#39;: tick_size, &#39;ytick.labelsize&#39;: tick_size, &#39;font.family&#39;: &#39;serif&#39;, &#39;font.size&#39;: text_size, # &#39;font.serif&#39;: [&#39;Computer Modern Roman&#39;], &#39;ps.usedistiller&#39;: &#39;xpdf&#39;, &#39;text.usetex&#39;: True, &#39;figure.figsize&#39;: fig_size, # include here any neede package for latex &#39;text.latex.preamble&#39;: [r&#39; usepackage{amsmath}&#39;], } plt.rcParams.update(params) plt.clf() fig = plt.figure(1, figsize=fig_size) # figsize accepts only inches. fig.subplots_adjust(left=0.04, right=0.98, top=0.93, bottom=0.15, hspace=0.05, wspace=0.02) plt.ioff() . Configure axes and some function definitions . x = np.arange(0, 12, 0.4) ax1 = fig.add_subplot(211, aspect=&#39;equal&#39;) ax2 = fig.add_subplot(212, aspect=&#39;equal&#39;) ax1.set_xlim((x.min(), x.max())) ax2.set_xlim((x.min(), x.max())) ax1.set_ylim(-1, 3.5) ax2.set_ylim(-1, 3.5) ax1.set_xticklabels([]) ax1.set_yticks(np.arange(-1, 4)) ax2.set_yticks(np.arange(-1, 4)) def func(x, par0, par1, par2): return par0 + np.cos(par1 * x + par2) def add_rec(ax, c, v, col): ax.add_patch( patches.Rectangle( c, # (x,y) np.abs(v), # width v, # height alpha=0.4, color=col ) ) . Now let&#39;s plot some stuff . %matplotlib notebook # the parameter values par = (1, 2, 1) # generating data with noise y = func(x, *par) + (np.random.random(len(x)) - 0.5) ax1.plot(x, y, marker=&#39;o&#39;, ls=&#39;None&#39;, markerfacecolor=&quot;blue&quot;, markeredgecolor=&quot;black&quot;) ax2.plot(x, y, marker=&#39;o&#39;, ls=&#39;None&#39;, markerfacecolor=&quot;red&quot;, markeredgecolor=&quot;black&quot;) # best fit popt, pcov = curve_fit(func, x, y, p0=(1.5, 1.5, 2.5)) # p0 = initial guess p0, p1, p2 = popt # The total sum of squares (proportional to the variance of the data) SStot = ((y - y.mean()) ** 2).sum() # The sum of squares of residuals SSres = ((y - func(x, p0, p1, p2)) ** 2).sum() Rsquared = 1 - SSres / SStot # plot best fit h = np.linspace(x.min(), x.max(), 1001) fit, = ax1.plot(h, func(h, p0, p1, p2), color=&#39;black&#39;, linewidth=2) ax1.legend([fit], [&quot;Best fit&quot;], loc=&quot;upper right&quot;, frameon=False, handlelength=4) # plot mean mean, = ax2.plot(h, h * 0 + np.mean(y), ls=&#39;--&#39;, color=&#39;black&#39;, linewidth=2) ax2.legend([mean], [&quot;Mean&quot;], loc=&quot;upper right&quot;, frameon=False, handlelength=4) # plot blue and red squares for ind in np.arange(len(x)): x0 = x[ind] y0 = y[ind] # print(x0,y0) v1 = y0 - func(x0, p0, p1, p2) v2 = y0 - y.mean() add_rec(ax1, (x0, y0), -v1, &quot;blue&quot;) add_rec(ax2, (x0, y0), -v2, &quot;red&quot;) ax2.text(0.5, 2.7, r&quot;Total sum of squares: {:.1f}&quot;.format(SStot)) ax1.text(0.5, 2.7, r&quot;Sum of squares of residuals: {:.1f}&quot;.format(SSres)) ax2.set_xlabel( r&quot;R-squared = $1 - displaystyle frac{ text{blue area}}{ text{red area}}$ = &quot; + &quot;{:.2f}&quot;.format(Rsquared)) ax1.set_xlabel( r&quot;Data: $f(x) = p_0 + cos(p_1 x + p_2)+ $ noise &quot;) ax1.xaxis.set_label_position(&quot;top&quot;) fig.savefig(&quot;./python_figures/least-squares.png&quot;,dpi=300) fig .",
            "url": "https://yairmau.github.io/jupyter/2020/01/01/leastsquares.html",
            "relUrl": "/jupyter/2020/01/01/leastsquares.html",
            "date": " ‚Ä¢ Jan 1, 2020"
        }
        
    
  
    
        ,"post41": {
            "title": "A hysteresis mechanism",
            "content": ". Introduction . Hysteresis mechanism created by bistability of states. . Energy function: $$f = u^4 - 2u^2 + hu$$ . The code . # comment these lines if you want interactive mode, # i.e., if you want to see the animation in real time. import matplotlib matplotlib.use(&#39;Agg&#39;) . import matplotlib.pyplot as plt import numpy as np import os import sympy from scipy.integrate import ode # learn how to configure: http://matplotlib.sourceforge.net/users/customizing.html params = {#&#39;backend&#39;: &#39;GTKAgg&#39;, &#39;legend.handlelength&#39;: 2.5, &#39;legend.borderaxespad&#39;: 0, &#39;font.family&#39;:&#39;serif&#39;, &#39;font.size&#39;: 18, &#39;font.serif&#39;:[&#39;Times&#39;], # Times, Palatino, New Century Schoolbook, Bookman, Computer Modern Roman &#39;ps.usedistiller&#39;: &#39;xpdf&#39;, &#39;text.usetex&#39;: True, } plt.rcParams.update(params) fig=plt.figure(1,figsize=(9.6,5.4),dpi=100) # 1920x1080 # figsize accepts only inches. if you rather think in cm, change the code yourself. fig.clf() fig.subplots_adjust(left=0.07, right=0.93,top=0.90, bottom=0.12,hspace=0.02,wspace=0.10) Hlim=2.5 # parameter range from -Hlim to Hlim ax1=fig.add_subplot(121) ax1.set_xticks([]) ax1.set_yticks([]) ax1.set_xlabel(r&#39;System response&#39;,labelpad=12) ax1.set_ylabel(&#39;Energy&#39;,labelpad=12) ax1.axis([-Hlim,Hlim,-5,5]) ax2=fig.add_subplot(122) ax2.set_xticks([]) ax2.set_yticks([]) ax2.set_xlabel(r&#39;Parameter&#39;,labelpad=12) ax2.set_ylabel(r&#39;System response&#39;,labelpad=12) ax2.yaxis.set_label_position(&quot;right&quot;) ax2.axis([-Hlim*1.2,Hlim*1.2,-2,2]) frame_names = [] frame_index = 0 make_movie=True plt.ion() . # energy function and its derivative f = lambda u,h: u**4-2*u**2+h*u fprime = lambda u,h: sympy.diff(f(u,h),u) Hinit=Hlim ulim=2.5 # system response axis, from -ulim to ulim u = np.linspace(-ulim,ulim,101) x = sympy.Symbol(&#39;x&#39;) def res(h): &quot;&quot;&quot;System response is one of the real roots of the energy function derivative &quot;&quot;&quot; # derivative roots, complex resp = sympy.solvers.solve(fprime(x,h),x) # numerical evaluation resp = map(sympy.N,resp) # let&#39;s check which roots are real isreal = len(resp)*[False] for i in range(len(resp)): # negligible imaginary component if np.abs(sympy.functions.im(resp[i]))&lt;1e-15: resp[i]=sympy.functions.re(resp[i]) isreal[i]=True resp = np.array(resp) # return only real roots return resp[np.array(isreal)] # let&#39;s plot stuff, and make a nice movie #### left plot, ax1 #### line_func, = ax1.plot(u,f(u,Hinit),lw=2,color=&#39;black&#39;) # ball color ball_color = &quot;blue&quot; # minimum = the smallest root, the leftmost root mini = np.min(res(Hinit)) # calculated for initial parameter value boost = 0.22 # so that ball sits on top of the curve # plot ball ball_u, = ax1.plot([mini],[f(mini,Hinit)+boost],&#39;o&#39;, markersize=12, markerfacecolor=ball_color) #### right plot, ax2 #### # build empty hysteresis array, we will add values # as simulation progresses deetype = np.dtype([(&#39;h&#39;, &#39;float64&#39;), (&#39;u&#39;, &#39;float64&#39;)]) hysteresis = np.array([(Hinit,mini)],dtype=deetype) line_hyst, = ax2.plot(hysteresis[&#39;h&#39;],hysteresis[&#39;u&#39;], lw=2,color=&#39;black&#39;) ballH, = ax2.plot([hysteresis[&#39;h&#39;][-1]],[hysteresis[&#39;u&#39;][-1]],&#39;o&#39;, markersize=12, markerfacecolor=ball_color) plt.show() . # time to simulate Total_time = 15 # seconds fps = 24 # frames per second # divided by 2 because we ramp down then up param_vec = np.linspace(Hlim,-Hlim,Total_time*fps/2) # ramp down for H in param_vec: line_func.set_data(u,f(u,H)) # update line on the left mini = np.min(res(H)) # calculate new minimum ball_u.set_data([mini],[f(mini,H)+boost]) # update ball on the left new_line = np.array([(H,mini)],dtype=deetype) # create new line # append new line to hysteresis array hysteresis = np.concatenate((hysteresis,new_line)) line_hyst.set_data(hysteresis[&#39;h&#39;],hysteresis[&#39;u&#39;]) # update line ballH.set_data([hysteresis[&#39;h&#39;][-1]],[hysteresis[&#39;u&#39;][-1]]) # update ball on the right fig.canvas.draw() if make_movie: fname = &quot;_tmp{:05d}.png&quot;.format(frame_index) frame_names.append(fname) fig.savefig(fname,dpi=200) frame_index+=1 # ramp up for H in param_vec[::-1]: # just reverse parameter array line_func.set_data(u,f(u,H)) maxi = np.max(res(H)) # everything is the same, but now with maximum ball_u.set_data([maxi],[f(maxi,H)+boost]) new_line = np.array([(H,maxi)],dtype=deetype) hysteresis = np.concatenate((hysteresis,new_line)) line_hyst.set_data(hysteresis[&#39;h&#39;],hysteresis[&#39;u&#39;]) ballH.set_data([hysteresis[&#39;h&#39;][-1]],[hysteresis[&#39;u&#39;][-1]]) fig.canvas.draw() if make_movie: fname = &quot;_tmp{:05d}.png&quot;.format(frame_index) frame_names.append(fname) fig.savefig(fname,dpi=200) frame_index+=1 if make_movie: frames = &quot;_tmp%5d.png&quot; movie_command = &quot;ffmpeg -y -r {:} -i {:} ball.mp4&quot;.format(fps,frames) os.system(movie_command) for fname in frame_names: # pass os.remove(fname) .",
            "url": "https://yairmau.github.io/jupyter/2020/01/01/hysteresis.html",
            "relUrl": "/jupyter/2020/01/01/hysteresis.html",
            "date": " ‚Ä¢ Jan 1, 2020"
        }
        
    
  
    
        ,"post42": {
            "title": "Fun with histograms",
            "content": ". Introduction . This code produces the figure above. I tried to showcase a few things one can do with 1d and 2d histograms. . The code . import matplotlib.pyplot as plt import numpy as np import matplotlib.gridspec as gridspec import scipy.special from scipy.optimize import curve_fit . make graph look pretty . # http://wiki.scipy.org/Cookbook/Matplotlib/LaTeX_Examples # this is a latex constant, don&#39;t change it. pts_per_inch = 72.27 # write &quot; the textwidth&quot; (or &quot; showthe columnwidth&quot; for a 2 collumn text) text_width_in_pts = 450.0 # inside a figure environment in latex, the result will be on the # dvi/pdf next to the figure. See url above. text_width_in_inches = text_width_in_pts / pts_per_inch # make rectangles with a nice proportion golden_ratio = 0.618 # figure.png or figure.eps will be intentionally larger, because it is prettier inverse_latex_scale = 2 # when compiling latex code, use # includegraphics[scale=(1/inverse_latex_scale)]{figure} # we want the figure to occupy 2/3 (for example) of the text width fig_proportion = (3.0 / 3.0) csize = inverse_latex_scale * fig_proportion * text_width_in_inches # always 1.0 on the first argument fig_size = (1.0 * csize, 0.5 * csize) # find out the fontsize of your latex text, and put it here text_size = inverse_latex_scale * 12 label_size = inverse_latex_scale * 10 tick_size = inverse_latex_scale * 8 # learn how to configure: # http://matplotlib.sourceforge.net/users/customizing.html params = {&#39;backend&#39;: &#39;ps&#39;, &#39;axes.labelsize&#39;: 16, &#39;legend.fontsize&#39;: tick_size, &#39;legend.handlelength&#39;: 2.5, &#39;legend.borderaxespad&#39;: 0, &#39;axes.labelsize&#39;: label_size, &#39;xtick.labelsize&#39;: tick_size, &#39;ytick.labelsize&#39;: tick_size, &#39;font.family&#39;: &#39;serif&#39;, &#39;font.size&#39;: text_size, # &#39;font.serif&#39;: [&#39;Computer Modern Roman&#39;], &#39;ps.usedistiller&#39;: &#39;xpdf&#39;, &#39;text.usetex&#39;: True, &#39;figure.figsize&#39;: fig_size, } plt.rcParams.update(params) plt.ioff() fig = plt.figure(1, figsize=fig_size) # figsize accepts only inches. . Panels on the left of the figure . gs = gridspec.GridSpec(2, 2, width_ratios=[1, 0.2], height_ratios=[0.2, 1]) gs.update(left=0.05, right=0.50, top=0.95, bottom=0.10, hspace=0.02, wspace=0.02) sigma = 1.0 # standard deviation (spread) mu = 0.0 # mean (center) of the distribution x = np.random.normal(loc=mu, scale=sigma, size=5000) k = 2.0 # shape theta = 1.0 # scale y = np.random.gamma(shape=k, scale=theta, size=5000) # bottom left panel ax10 = plt.subplot(gs[1, 0]) counts, xedges, yedges, image = ax10.hist2d(x, y, bins=40, cmap=&quot;YlOrRd&quot;, density=True) dx = xedges[1] - xedges[0] dy = yedges[1] - yedges[0] xvec = xedges[:-1] + dx / 2 yvec = yedges[:-1] + dy / 2 ax10.set_xlabel(r&quot;$x$&quot;) ax10.set_ylabel(r&quot;$y$&quot;, rotation=&quot;horizontal&quot;) ax10.text(-2, 8, r&quot;$p(x,y)$&quot;) ax10.set_xlim([xedges.min(), xedges.max()]) ax10.set_ylim([yedges.min(), yedges.max()]) # top left panel ax00 = plt.subplot(gs[0, 0]) gaussian = (1.0 / np.sqrt(2.0 * np.pi * sigma ** 2)) * np.exp(-((xvec - mu) ** 2) / (2.0 * sigma ** 2)) xdist = counts.sum(axis=1) * dy ax00.bar(xvec, xdist, width=dx, fill=False, edgecolor=&#39;black&#39;, alpha=0.8) ax00.plot(xvec, gaussian, color=&#39;black&#39;) ax00.set_xlim([xedges.min(), xedges.max()]) ax00.set_xticklabels([]) ax00.set_yticks([]) ax00.set_xlabel(&quot;Normal distribution&quot;, fontsize=16) ax00.xaxis.set_label_position(&quot;top&quot;) ax00.set_ylabel(r&quot;$p(x)$&quot;, rotation=&quot;horizontal&quot;, labelpad=20) # bottom right panel ax11 = plt.subplot(gs[1, 1]) gamma_dist = yvec ** (k - 1.0) * np.exp(-yvec / theta) / (theta ** k * scipy.special.gamma(k)) ydist = counts.sum(axis=0) * dx ax11.barh(yvec, ydist, height=dy, fill=False, edgecolor=&#39;black&#39;, alpha=0.8) ax11.plot(gamma_dist, yvec, color=&#39;black&#39;) ax11.set_ylim([yedges.min(), yedges.max()]) ax11.set_xticks([]) ax11.set_yticklabels([]) ax11.set_ylabel(&quot;Gamma distribution&quot;, fontsize=16) ax11.yaxis.set_label_position(&quot;right&quot;) ax11.set_xlabel(r&quot;$p(y)$&quot;) ax11.xaxis.set_label_position(&quot;top&quot;) . Panels on the right of the figure . gs2 = gridspec.GridSpec(2, 1, width_ratios=[1], height_ratios=[1, 1]) gs2.update(left=0.60, right=0.98, top=0.95, bottom=0.10, hspace=0.02, wspace=0.05) x = np.random.normal(loc=0, scale=1, size=1000) y = np.random.gamma(shape=2, size=1000) bx10 = plt.subplot(gs2[1, 0]) bx00 = plt.subplot(gs2[0, 0]) N = 100 a = np.random.gamma(shape=5, size=N) my_bins = np.arange(0,15,1.5) n1, bins1, patches1 = bx00.hist(a, bins=my_bins, density=True, histtype=&#39;stepfilled&#39;, alpha=0.2, hatch=&#39;/&#39;) bx00.set_xlim([0, 15]) bx00.set_ylim([0, 0.28]) bx00.set_xticklabels([]) bx00.set_xlabel(r&quot; texttt{plt.hist}&quot;) bx00.xaxis.set_label_position(&quot;top&quot;) # the following way is equivalent to plt.hist, but it gives # the user more flexibility when plotting and analysing the results n2, bins2 = np.histogram(a, bins=my_bins, density=True) wid = bins2[1] - bins2[0] red, = bx10.plot(bins2[:-1]+wid/2, n2, marker=&#39;o&#39;, color=&#39;red&#39;) bx10.bar(bins2[:-1], n2, width=wid, fill=False, edgecolor=&#39;black&#39;, linewidth=3, alpha=0.8, align=&quot;edge&quot;) bx10.set_xlim([0, 15]) bx10.set_ylim([0, 0.28]) bx10.set_xlabel(r&quot; texttt{np.histogram}; quad texttt{plt.bar}&quot;) . Text(0.5, 0, &#39; texttt{np.histogram}; quad texttt{plt.bar}&#39;) . best fit . xdata = my_bins[:-1] + wid/2 ydata = n2 def func(x, p1, p2): return x ** (p1 - 1.0) * np.exp(-x / p2) / (p2 ** p1 * scipy.special.gamma(p1)) popt, pcov = curve_fit(func, xdata, ydata, p0=(1.5, 1.5)) # p0 = initial guess p1, p2 = popt SStot = ((ydata - ydata.mean()) ** 2).sum() SSres = ((ydata - func(xdata, p1, p2)) ** 1).sum() Rsquared = 1 - SSres / SStot h = np.linspace(0,15,101) bx00.plot(h, func(h, p1, p2), color=&#39;blue&#39;, linewidth=2) # dummy plot, just so we can have a legend on the bottom panel blue, = ax10.plot([100],[100], color=&#39;blue&#39;, linewidth=2, label=&quot;Best fit&quot;) bx10.legend([red,blue],[r&#39;Data&#39;,r&#39;Best fit, $r^2=${:.2f}&#39;.format(Rsquared)], loc=&#39;upper right&#39;, frameon=False, handlelength=4, markerfirst=False, numpoints=3) . /Users/yairmau/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in double_scalars after removing the cwd from sys.path. . &lt;matplotlib.legend.Legend at 0x7fde860d7ed0&gt; . fig.savefig(&quot;./python_figures/histograms.png&quot;,dpi=300) fig .",
            "url": "https://yairmau.github.io/jupyter/2020/01/01/histograms.html",
            "relUrl": "/jupyter/2020/01/01/histograms.html",
            "date": " ‚Ä¢ Jan 1, 2020"
        }
        
    
  
    
        ,"post43": {
            "title": "Hilbert Curve",
            "content": ". Introduction . The Hilbert curve is calculated iteratively. Check out this: Hilbert_curve: Representation_as_Lindenmayer_system . The code . # comment this to enable interactive mode import matplotlib matplotlib.use(&#39;AGG&#39;) . import numpy as np import matplotlib.pyplot as plt import os . def apply_rules(s): &quot;&quot;&quot; Hilbert Curve as a Lindenmayer system (L-system) https://en.wikipedia.org/wiki/Hilbert_curve#Representation_as_Lindenmayer_system&quot;&quot;&quot; s=s.replace(&quot;a&quot;,&quot;-Bf+AfA+fB-&quot;) # capital letters &quot;A&quot; and &quot;B&quot; so that the second operation s=s.replace(&quot;b&quot;,&quot;+Af-BfB-fA+&quot;) # doesn&#39;t apply to the changes already made return s.lower() # make everyone lowercase axiom = &quot;a&quot; n=3 # number of iterations # displacements, ordered in a counter-clockwise direction dxdy = np.array([[ 1, 0], # right [ 0, 1], # up [-1, 0], # left [ 0,-1] ]) # down # displacement is of size 1, so the higher n is, the greater the domain length = 2**n-1; margin = 0.05*length domain = [0-margin,length+margin,0-margin,length+margin] # a 5% margin around the curve s = axiom # string to iterate upon for i in np.arange(n): s = apply_rules(s) . make_movie=True plt.ion() # interactive mode disabled if &quot;matplotlib.use(&#39;AGG&#39;)&quot; fig = plt.figure(figsize=(6,6)) ax = fig.add_subplot(111) ax.axis(&#39;off&#39;) # no frame ax.axis(domain) # domain size ax.set_aspect(&#39;equal&#39;) # square look ax.set_xticks([]); ax.set_yticks([]) # no ticks ax.set_title(r&quot;$n = {:d}$&quot;.format(n)) plt.show() # &quot;a&quot; and &quot;b&quot; can be erased now s=s.replace(&quot;a&quot;,&quot;&quot;) s=s.replace(&quot;b&quot;,&quot;&quot;) frame_names = [] # these two are only relevant if make_movie==True frame_counter=0 p = np.array([[0.0,0.0]]) # this is the starting point (0,0) p_plot, = plt.plot(p[:,0],p[:,1],color=&quot;black&quot;) # iterate on the string s for i,c in enumerate(s): # uncomment to see how fast things are going # print(&quot;{:d}/{:d}&quot;.format(i,len(s))) # rotations &quot;+&quot; and &quot;-&quot; change the displacement array dxdy # &quot;+&quot; means clockwise rotation if c == &#39;+&#39;: dxdy = np.roll(dxdy,+1,axis=0) # &quot;-&quot; means counter-clockwise rotation if c == &#39;-&#39;: dxdy = np.roll(dxdy,-1,axis=0) # forward &quot;f&quot; if c == &#39;f&#39;: # add one more point to array p p = np.vstack([p, [p[-1,0]+dxdy[0,0],p[-1,1]+dxdy[0,1]] ]) # update p_plot data, this is MUCH faster that plotting # several line segments separately p_plot.set_data(p[:,0],p[:,1]) fig.canvas.draw() if make_movie: fname = &quot;_tmp{:05d}.png&quot;.format(frame_counter) frame_names.append(fname) fig.savefig(fname,bbox_inches=&#39;tight&#39;,resolution=300) frame_counter += 1 . if make_movie: frames = &quot;_tmp%5d.png&quot; # movie_command = &quot;mencoder mf://*.png -mf fps=24:type=png --ovc lavc -lavcopts vcodec=mpeg4:mbd=2:trell -oac copy -o hil{:d}.avi&quot;.format(n) # we might have other .png figures in the directory # in this case, use the code below f = open(&quot;png_list.txt&quot;, &quot;w&quot;) for i in frame_names: f.write(i+&quot; n&quot;) f.close() movie_command = &quot;mencoder mf://@png_list.txt -mf fps=24:type=png -ovc lavc -lavcopts vcodec=mpeg4:mbd=2:trell -oac copy -o hil{:d}.avi&quot;.format(n) err=os.system(movie_command) if err!=0: raise RuntimeError(&quot;Couldn&#39;t run mencoder. Data in tmp*.png files&quot;) for fname in frame_names: os.remove(fname) # we now have one video ready. # if you want to join several videos, use this: # sudo apt-get install gpac # MP4Box -cat part1.avi -cat part2.avi -new joinedfile.avi .",
            "url": "https://yairmau.github.io/jupyter/2020/01/01/hilbertcurve.html",
            "relUrl": "/jupyter/2020/01/01/hilbertcurve.html",
            "date": " ‚Ä¢ Jan 1, 2020"
        }
        
    
  
    
        ,"post44": {
            "title": "Conway's Game of Life",
            "content": ". Introduction . This is a (slightly) modified version of Glowing Python&#39;s code. I make it available here because it features a few nice things: . how to make a movie using matplotlib.animation | how to write a generator (function with yield) | how to plot a sparce array (spy) | . The code . import numpy as np from matplotlib import pyplot as plt import matplotlib.animation as manimation def life(X, steps): &quot;&quot;&quot; Conway&#39;s Game of Life. - X, matrix with the initial state of the game. - steps, number of generations. &quot;&quot;&quot; def roll_it(x, y): # rolls the matrix X in a given direction # x=1, y=0 left; x=-1, y=0 right; return np.roll(np.roll(X, y, axis=0), x, axis=1) for _ in range(steps): # count the number of neighbours # the universe is considered toroidal Y = roll_it(1, 0) + roll_it(0, 1) + roll_it(-1, 0) + roll_it(0, -1) + roll_it(1, 1) + roll_it(-1, -1) + roll_it(1, -1) + roll_it(-1, 1) # game of life rules X = np.logical_or(np.logical_and(X, Y == 2), Y == 3) X = X.astype(int) yield X . dimensions = (90, 160) # height, width X = np.zeros(dimensions) # Y by X dead cells middle_y = dimensions[0] / 2 middle_x = dimensions[1] / 2 N_iterations = 600 # acorn initial condition # http://www.conwaylife.com/w/index.php?title=Acorn X[middle_y, middle_x:middle_x+2] = 1 X[middle_y, middle_x+4:middle_x+7] = 1 X[middle_y+1, middle_x+3] = 1 X[middle_y+2, middle_x+1] = 1 . FFMpegWriter = manimation.writers[&#39;ffmpeg&#39;] metadata = dict(title=&#39;Game of life&#39;, artist=&#39;Acorn initial condition&#39;) writer = FFMpegWriter(fps=10, metadata=metadata) fig = plt.figure() fig.patch.set_facecolor(&#39;black&#39;) with writer.saving(fig, &quot;game_of_life.mp4&quot;, 300): # last argument: dpi plt.spy(X, origin=&#39;lower&#39;) plt.axis(&#39;off&#39;) writer.grab_frame() plt.clf() for i, x in enumerate(life(X, N_iterations)): plt.title(&quot;iteration: {:03d}&quot;.format(i + 1)) plt.spy(x, origin=&#39;lower&#39;) plt.axis(&#39;off&#39;) writer.grab_frame() plt.clf() .",
            "url": "https://yairmau.github.io/jupyter/2020/01/01/gol.html",
            "relUrl": "/jupyter/2020/01/01/gol.html",
            "date": " ‚Ä¢ Jan 1, 2020"
        }
        
    
  
    
        ,"post45": {
            "title": "Fitzhugh-Nagumo Equation",
            "content": "https://youtu.be/5au-G5FuI_A . Introduction . We simulate the Fitzhugh-Nagumo equations $$ u_t = u - u^3 - v + nabla^2 u v_t = epsilon(u - a_1 v - a_0) + delta nabla^2 v, $$ using the semi-spectral time integration method. . This simultation was heavily inspired by Aric Hagberg&#39;s simulation in &quot;From Labyrinthine Patterns to Spiral Turbulence&quot;, PRL 1994. . The code below provides 3 initial conditions, &quot;squiggle, blocks, and random&quot;. For time integration, besides the spectral method, we also provide the Euler method. Details about the semi-spectral method can be found after the code. . Parameters: $ epsilon=0.3$, $ delta=2.0$, $a_1=1.4$, and $a_0=0$. . Other simulations and Python examples can be found on my website: yairmau.com. . The code . import packages . import numpy as np import matplotlib.pyplot as plt import os from matplotlib import rcParams rcParams[&#39;font.family&#39;] = &#39;monospace&#39; . define class with all the methods . class FitzHughNagumo(object): def __init__(self, epsilon=0.3, delta=2.0, a1=1.4, a0=0.0, n=(256, 256), l=(400, 400), start=0.0, step=1.0, finish=2000.0, dt=0.1, integration_type=&quot;spectral&quot;): self.epsilon = epsilon self.delta = delta self.a1 = a1 self.a0 = a0 self.n = n self.l = l self.start = start self.step = step self.finish = finish self.dt = dt self.integration_type = integration_type self.rhs_a = np.zeros((2, self.n[0], self.n[1])) def spectral_multiplier(self): dx = float(self.l[0]) / self.n[0] dy = float(self.l[1]) / self.n[1] # wave numbers fx = 2.0 * np.pi * np.fft.fftfreq(self.n[0], dx) fy = 2.0 * np.pi * np.fft.fftfreq(self.n[1], dy) kx = np.outer(fx, np.ones(self.n[0])) ky = np.outer(np.ones(self.n[1]), fy) # multiplier mult_a = np.zeros((2, self.n[0], self.n[1])) mult_a[0] = np.exp(-(kx ** 2 + ky ** 2) * self.dt) # u mult_a[1] = np.exp(-self.delta * (kx ** 2 + ky ** 2) * self.dt) # v return mult_a def rhs_reaction(self, a): u = a[0] # alias v = a[1] # alias # FHN right hand side self.rhs_a[0] = u - u ** 3 - v self.rhs_a[1] = self.epsilon * (u - self.a1 * v - self.a0) return self.rhs_a def rhs_euler(self, a): # boundary conditions in laplacian laplacian = self.periodic_laplacian u = a[0] # alias v = a[1] # alias dx = float(self.l[0]) / self.n[0] # FHN right hand side self.rhs_a[0] = u - u ** 3 - v + laplacian(u, dx=dx) self.rhs_a[1] = self.epsilon * (u - self.a1 * v - self.a0) + self.delta * laplacian(v, dx=dx) return self.rhs_a def draw(self, a, t): u = a[0] self.im = plt.imshow(u.real, cmap=&quot;Greys_r&quot;, origin=&#39;lower&#39;, vmin=-0.534522, vmax=0.534522, interpolation=&quot;gaussian&quot;) self.title = plt.title(&#39;time = {:&gt;4.0f}&#39;.format(0)) plt.xticks([]) plt.yticks([]) self.im.figure.canvas.draw() def draw_update(self, a, t): u = a[0] self.title.set_text(&#39;time = {:&gt;4.0f}&#39;.format(t)) self.im.set_data(u.real) self.im.figure.canvas.draw() def save_frame(self, i): fname = &quot;_tmp{:05d}.png&quot;.format(i) self.frame_names.append(fname) self.fig.savefig(fname, bbox_inches=&#39;tight&#39;, dpi=300) def periodic_laplacian(self, u, dx=1): &quot;&quot;&quot;Return finite difference Laplacian approximation of 2d array. Uses periodic boundary conditions and a 2nd order approximation.&quot;&quot;&quot; laplacian = (np.roll(u, -1, axis=0) + np.roll(u, +1, axis=0) + np.roll(u, -1, axis=1) + np.roll(u, +1, axis=1) - 4.0 * u) / (dx ** 2) return laplacian def random_ic(self): return 0.5 * (np.random.random((2, self.n[0], self.n[1])) - 0.5) def blocks_ic(self): a = np.ones((2, self.n[0], self.n[1])) a[0] = 0.534522 a[1] = 0.381802 n = self.n p = n[0] / 8 a[0][3 * p - 4:3 * p + 4, 5 * p - 4:5 * p + 4] = -0.534522 a[0][6 * p - 4:6 * p + 4, 3 * p - 4:3 * p + 4] = -0.534522 return a def squiggle_ic(self): a = np.ones((2, self.n[0], self.n[1])) l = self.l uplus = 0.534522 vplus = 0.381802 uminus = -uplus X, Y = np.meshgrid(np.linspace(0, self.l[0], self.n[0]), np.linspace(0, self.l[0], self.n[0])) cos_term = 0.05 * l[0] * np.sin(10 * (2 * np.pi) * Y / l[1] + np.pi * 0.3) exp_term = np.exp(-((Y - l[1] / 2) / (0.1 * l[1])) ** 2) width = 0.05 * l[0] Z = np.exp(-((X - l[0] / 2 + cos_term * exp_term) / width) ** 2) a[0] = uplus a[1] = vplus a[0][Z &gt; 0.8] = uminus return a . run simulation, save snapshots . plt.ion() plt.clf() foo = FitzHughNagumo() foo.fig = plt.figure(1) ax = foo.fig.add_subplot(111) a = foo.squiggle_ic() mult_a = foo.spectral_multiplier() fft_a = np.fft.fftn(a, axes=(1, 2)) t = foo.start foo.draw(a, t) foo.frame_names = [] foo.save_frame(0) for i, tout in enumerate(np.arange(foo.start + foo.step, foo.finish + foo.step, foo.step)): while t &lt; tout: if foo.integration_type == &quot;spectral&quot;: rhs_a = foo.rhs_reaction(a) fft_a = mult_a * (fft_a + foo.dt * np.fft.fftn(rhs_a, axes=(1, 2))) a = np.fft.ifftn(fft_a, axes=(1, 2)) if foo.integration_type == &quot;euler&quot;: a = a + foo.dt * foo.rhs_euler(a) t += foo.dt foo.draw_update(a, t) foo.save_frame(i + 1) . make movie, delete snapshots . fps = 24 frames = &quot;_tmp%5d.png&quot; movie_command = &quot;ffmpeg -y -r {:} -i {:} fhn.mp4&quot;.format(fps, frames) os.system(movie_command) for fname in foo.frame_names: os.remove(fname) . The semi-spectral method . The explanation below was taken from my thesis: &quot;Pattern Formation in Spatially Forced Systems: Application to Vegetation Restoration&quot;. . The semi-spectral method is extremely useful when working with reaction-diffusion systems, and with parabolic PDEs in general. This was the method used to run all the simulations of the Swift-Hohenberg model in this thesis, and it proved to be reliable and fast. The explanation below is a summary of &quot;Spectral algorithms for reaction-diffusion equations&quot;, by Richard V. Craster and Roberto Sassi, with a step by step recipe, so the reader can easily apply the method to any suitable problem. . the method . The semi-spectral transform method is very useful when we have to integrate a system that evolves really slowly. Let us say we have a (parabolic) system of the form: $$ begin{equation*} u_t= epsilon u + f(u)+D nabla^2u, label{eq:1} tag{1} end{equation*} $$ . where $f(u)$ is a nonlinear function. First, we compute the Fourier transform of eqref{eq:1}: $$ begin{equation*} hat{u}_t= epsilon hat{u} + hat{f}(u)-k^2D hat{u}, label{eq:2} tag{2} end{equation*} $$ where the hat denotes the Fourier transform. . We rearrange eqref{eq:2} in the following way: $$ begin{equation*} hat{u}_t+a hat{u}= hat{f}(u), label{eq:3} tag{3} end{equation*} $$ where $a=- epsilon +k^2D$, and now we make a variable substitution $$ begin{align*} hat{v}(k,t)&amp;= ; hat{u}(k,t) ,e^{at} label{eq:4a} tag{4a} hat{v}_t&amp;= ; hat{u}_te^{at}+a hat{u} ,e^{at}. label{eq:4b} tag{4b} end{align*} $$ . We multiply eqref{eq:3} by $e^{at}$ and we finally get $$ begin{equation*} hat{v}_t=e^{at} hat{f}(u). label{eq:5} tag{5} end{equation*} $$ . We can now advance $ hat{v}$ in time using a simple Euler step $$ begin{equation*} hat{v}^{t_{n+1}}= hat{v}^{t_n}+ Delta t left( e^{at_n} hat{f}(u) right). label{eq:6} tag{6} end{equation*} $$ . What we really want is $ hat{u}$, which, according to eqref{eq:4a}, is given by . $$ begin{align*} displaystyle hat{u}^{t_{n+1}}=&amp; ; hat{v}^{t_{n+1}}e^{-at_{n+1}} label{eq:7a} tag{7a} =&amp; ; hat{v}^{t_{n+1}}e^{-at_{n}}e^{-a Delta t} label{eq:7b} tag{7b} =&amp; ; left( hat{v}^{t_n}+ Delta t ; e^{a t_n} hat{f}(u) right)e^{-at_{n}}e^{-a Delta t} label{eq:7c} tag{7c} =&amp; ; left( hat{v}^{t_n}e^{-at_{n}}+ Delta t ;{e^{a t_n}} hat{f}(u) {e^{-at_{n}}} right)e^{-a Delta t} label{eq:7d} tag{7d} =&amp; ; left( hat{u}^{t_n}+ Delta t hat{f}(u) right)e^{-a Delta t} label{eq:7e} tag{7e}. end{align*} $$There is actually no need to use the variable substitution in eqref{eq:4a}. We now have an expression for $ hat{u}^{t_{n+1}}$: $$ begin{equation*} hat{u}^{t_{n+1}}= left( hat{u}^{t_n}+ Delta t hat{f}(u) right)e^{-a Delta t}. label{eq:8} tag{8} end{equation*} $$ . Now it is time to go back from the Fourier space to the real space, and for that we use an inverse Fourier transform $$ u^{t_{n+1}}= mathcal{F}^{-1}[ hat{u}^{t_{n+1}}]. label{eq:9} tag{9} $$ . step by step . To implement this technique, one just has to follow the steps below: . Calculate the Fourier transform of $u$: $ hat{u}= mathcal{F}[u]$. | Have $f(u)$ calculated and then take its Fourier transform: $ hat{f}(u)= mathcal{F}[f(u)]$. | For a given lattice with $N$ points, and $ delta x$ being the distance between them, make the frequency bin vector (matrix) $k$ for your one (two) dimensional system. In python the command would benumpy.fft.fftfreq(N, dx). . The frequency bin vector $k$ looks like: | . $$ begin{align} k&amp;=2 pi cdot left[ 0,1, cdots, tfrac{N}{2}-1,- tfrac{N}{2}, cdots,-1 right]/(N , delta x), qquad mbox{if N is even;} label{eq:10a} tag{10a} k&amp;=2 pi cdot left[ 0,1, cdots, tfrac{N-1}{2},- tfrac{N-1}{2}, cdots,-1 right]/(N , delta x), qquad mbox{if N is odd.} label{eq:10b} tag{10b} end{align} $$Remember that the domain size is given by $L=N , delta x$, which means that the denominator in the expressions above can be written simply as $L$. It is clear from that fact that $ delta k$, the tiniest slice of the Fourier space is $ delta k=2 pi/L$. Corollary: if you want to divide the Fourier space into very many parts, simply have a huge domain. If the system is two-dimensional, then have $k_x$ and $k_y$ calculated separately. The domain might not be square ($L_x neq L_y$), and you might want to divide the domain into a different number of points ($N_x neq N_y$). Anyway, prepare one-dimensional arrays of $k_x$ and $k_y$ as explained above, and then make an outer product of these arrays with a ones array of length $N$, as following: . $$ k_{x,2d} = begin{pmatrix} 1 1 vdots 1 end{pmatrix} begin{pmatrix} k_{x1} &amp; k_{x2} &amp; ... &amp; k_{xN} end{pmatrix} = begin{pmatrix} k_{x1} &amp; k_{x2} &amp; ... &amp; k_{xN} k_{x1} &amp; k_{x2} &amp; ... &amp; k_{xN} &amp; vdots &amp; &amp; k_{x1} &amp; k_{x2} &amp; ... &amp; k_{xN} end{pmatrix} label{eq:11} tag{11} $$and . $$ k_{y,2d} = begin{pmatrix} k_{x1} k_{x2} vdots k_{xN} end{pmatrix} begin{pmatrix} 1 &amp; 1 &amp; ... &amp; 1 end{pmatrix} = begin{pmatrix} k_{y1} &amp; k_{y1} &amp; &amp; k_{y1} k_{y2} &amp; k_{y2} &amp; ... &amp; k_{y2} vdots &amp; vdots &amp; &amp; vdots k_{yN} &amp; k_{yN} &amp; &amp; k_{yN} end{pmatrix}. label{eq:12} tag{12} $$Then factor $e^{-a Delta t}$ equals . $$ e^{-a Delta t}= e^{ left[ epsilon-D(k_x^2+k_y^2) right] Delta t}, label{eq:13} tag{13} $$where $k_x^2$ is the element-wise exponentiation of the 2d array $k_{x,2d}$. . Now that we have all the factors we need, we simply calculate $$ hat{u}^{t_{n+1}}= left( hat{u}^{t_n}+ Delta t hat{f}(u) right)e^{ left[ epsilon-D(k_x^2+k_y^2) right] Delta t}. label{eq:14} tag{14} $$ | We finally go back to the real space by applying the inverse Fourier transform: $u^{t_{n+1}}= mathcal{F}^{-1}[ hat{u}^{t_{n+1}}]$. | . example . For the parametrically forced Swift-Hohenberg equation $$ frac{ partial u}{ partial t} = [ epsilon + gamma cos(k_f x)]u - u^3 -( nabla^2+k_0^2)^2 u, label{eq:15} tag{15} $$ we have $$ f(u)= -u^3 + gamma u cos(k_f x), qquad a = epsilon - left(k_0- k_x^2 - k_y^2 right)^2. label{eq:16} tag{16} $$ .",
            "url": "https://yairmau.github.io/jupyter/2020/01/01/fhn.html",
            "relUrl": "/jupyter/2020/01/01/fhn.html",
            "date": " ‚Ä¢ Jan 1, 2020"
        }
        
    
  
    
        ,"post46": {
            "title": "Double Pendulum",
            "content": ". Introduction . The double pendulum is one of the most famous examples of chaos. Enjoy making your own animations! . The code . comment the lines below if you want interactive mode, i.e., if you want to see the animation in real time. . import matplotlib matplotlib.use(&#39;Agg&#39;) . import matplotlib.pyplot as plt import numpy as np import os from scipy.integrate import ode . define equations of motion and other functions . def equations(t, y, args): &quot;&quot;&quot; the equations for the double pendulum &quot;&quot;&quot; x1 = y[0] # x1 = theta1, angle x2 = y[1] # x2 = theta2, angle p1 = y[2] # p1 = omega1, angular velocity p2 = y[3] # p2 = omega2, angular velocity l1,l2,m1,m2,g = args x1_eq = p1 x2_eq = p2 p1_eq = -((g*(2*m1+m2)*np.sin(x1)+m2*(g*np.sin(x1-2*x2)+2*(l2*p2**2+l1*p1**2*np.cos(x1-x2))*np.sin(x1-x2)))/(2*l1*(m1+m2-m2*(np.cos(x1-x2))**2))) p2_eq = ((l1*(m1+m2)*p1**2+g*(m1+m2)*np.cos(x1)+l2*m2*p2**2*np.cos(x1-x2))*np.sin(x1-x2))/(l2*(m1+m2-m2*(np.cos(x1-x2))**2)) return [x1_eq, x2_eq, p1_eq, p2_eq] def calculate_trajectory(args,time,y0): &quot;&quot;&quot; uses scipy&#39;s ode itegrator to simulate the equations &quot;&quot;&quot; t0,t1,dt = time r = ode(equations).set_integrator(&#39;dopri5&#39;) r.set_initial_value(y0, t0).set_f_params(args) data=[[t0, y0[0], y0[1], y0[2], y0[3] ]] while r.successful() and r.t &lt; t1: r.integrate(r.t+dt) data.append([r.t, r.y[0], r.y[1], r.y[2], r.y[3] ]) return np.array(data) def from_angle_to_xy(args,angles): &quot;&quot;&quot; converts angles into xy positions &quot;&quot;&quot; l1,l2,m1,m2,g = args time,theta1,theta2 = angles.T x1 = l1*np.sin(theta1) y1 = -l1*np.cos(theta1) x2 = l2*np.sin(theta2) + x1 y2 = -l2*np.cos(theta2) + y1 return np.array([time,x1,y1,x2,y2]).T . parameters . l1 = 0.5 # length of arms l2 = 0.5 m1 = 1.0 # mass of the pendulum m2 = 1.0 g = 10.0 # acceleration of gravity args = [l1,l2,m1,m2,g] fps = 80 total_time = 5 # seconds time = [0.0,total_time,1.0/fps] # start, finish, dt ic = [np.pi*0.65, np.pi*1.1, 0.0, 0.0] . here the magic happens . d = calculate_trajectory(args,time,ic) data_TXY = from_angle_to_xy(args,d[:,:3]) . Let&#39;s plot stuff, and make a nice movie. Requrement: ffmpeg . make_movie=True params = {&#39;backend&#39;: &#39;ps&#39;, &#39;font.size&#39;: 20, &#39;font.family&#39;:&#39;serif&#39;, &#39;font.serif&#39;:[&#39;Computer Modern Roman&#39;], # Times, Palatino, New Century Schoolbook, Bookman, Computer Modern Roman &#39;ps.usedistiller&#39;: &#39;xpdf&#39;, &#39;text.usetex&#39;: True, } plt.rcParams.update(params) plt.ion() fig = plt.figure(figsize=(9.6,5.4),dpi=100) # 1920x1080 fig.subplots_adjust(left=0, right=1, top=1, bottom=0,hspace=0.02,wspace=0.02) ax = fig.add_subplot(111) ax.axis(&#39;off&#39;) # no frame def plot_last_seconds(data,index): &quot;&quot;&quot; Plots a line with the trajectory of the tip of pendulum 2 (x2,y2) &quot;&quot;&quot; how_long = 1.0 # seconds n = int(how_long/time[2]) to_plot = data[:index,:] if index &lt; n: prepend = np.tile(data[0],(n-index,1)) to_plot = np.vstack([prepend,to_plot]) index = n colormap = plt.cm.Greys_r colors = [colormap(i) for i in np.linspace(0.0, 1.0, n-1)] plots = [] for j in np.arange(n-1): p, = ax.plot(to_plot[index-j-1:index-j+1,3],to_plot[index-j-1:index-j+1,4], color=colors[j], zorder=-1) plots.append(p) return plots # &quot;plot&quot; returns a tuple of line objects, thus the comma t,x1,y1,x2,y2 = data_TXY[0] line1, = ax.plot([0.0,x1], [0.0,y1], &#39;r-&#39;) line2, = ax.plot([x1,x2], [y1,y2], &#39;r-&#39;) circ1, = ax.plot([x1], [y1], &#39;ro&#39;,markersize=10) circ2, = ax.plot([x2], [y2], &#39;ro&#39;,markersize=10) sizeY = 1.2 ax.axis([-sizeY*16/9,sizeY*16/9,-sizeY,sizeY]) frame_names = [] tex=ax.text(0.0,0.85,&#39;&#39;,ha=&quot;center&quot;) for i,v in enumerate(data_TXY): t,x1,y1,x2,y2 = v # print(&quot;t={:.2f}&quot;.format(t)) # you might want to know how things are going... line1.set_data([0.0,x1],[0.0,y1]) line2.set_data([x1,x2],[y1,y2]) circ1.set_data([x1],[y1]) circ2.set_data([x2],[y2]) # plot_last_seconds considerably slows down the simulation, # but makes it much prettier... pls = plot_last_seconds(data_TXY,i+1) tex.set_text(r&quot;$t={:.3f}$ s&quot;.format(t)) fig.canvas.draw() if make_movie: fname = &quot;_tmp{:05d}.png&quot;.format(i) frame_names.append(fname) fig.savefig(fname,bbox_inches=&#39;tight&#39;) for k in pls: k.remove() if make_movie: frames = &quot;_tmp%5d.png&quot; frames = &quot;_tmp%5d.png&quot; movie_command = &quot;ffmpeg -y -r {:} -i {:} double.mp4&quot;.format(fps,frames) os.system(movie_command) for fname in frame_names: os.remove(fname) .",
            "url": "https://yairmau.github.io/jupyter/2020/01/01/doublependulum.html",
            "relUrl": "/jupyter/2020/01/01/doublependulum.html",
            "date": " ‚Ä¢ Jan 1, 2020"
        }
        
    
  
    
        ,"post47": {
            "title": "Contour plots",
            "content": ". Introduction . Contour plots are great to show how a variable depends on two parameters. . The code . import numpy as np import matplotlib.pyplot as plt import matplotlib from IPython.display import Math # %matplotlib inline . configure plotting preferences . # http://wiki.scipy.org/Cookbook/Matplotlib/LaTeX_Examples # this is a latex constant, don&#39;t change it. pts_per_inch = 72.27 # write &quot; the textwidth&quot; (or &quot; showthe columnwidth&quot; for a 2 collumn text) text_width_in_pts = 246.0 # inside a figure environment in latex, the result will be on the # dvi/pdf next to the figure. See url above. text_width_in_inches = text_width_in_pts / pts_per_inch # make rectangles with a nice proportion golden_ratio = 0.618 # figure.png or figure.eps will be intentionally larger, because it is prettier inverse_latex_scale = 2 # when compiling latex code, use # includegraphics[scale=(1/inverse_latex_scale)]{figure} # we want the figure to occupy 2/3 (for example) of the text width fig_proportion = (3.0 / 3.0) csize = inverse_latex_scale * fig_proportion * text_width_in_inches # always 1.0 on the first argument fig_size = (1.0 * csize,golden_ratio * csize) # find out the fontsize of your latex text, and put it here text_size = inverse_latex_scale * 12 tick_size = inverse_latex_scale * 8 # learn how to configure: # http://matplotlib.sourceforge.net/users/customizing.html params = {&#39;backend&#39;: &#39;ps&#39;, &#39;axes.labelsize&#39;: text_size, &#39;legend.fontsize&#39;: tick_size, &#39;legend.handlelength&#39;: 2.5, &#39;legend.borderaxespad&#39;: 0, &#39;xtick.labelsize&#39;: tick_size, &#39;ytick.labelsize&#39;: tick_size, &#39;font.family&#39;: &#39;serif&#39;, &#39;font.size&#39;: text_size, # Times, Palatino, New Century Schoolbook, # Bookman, Computer Modern Roman &#39;font.serif&#39;: [&#39;Times&#39;], &#39;ps.usedistiller&#39;: &#39;xpdf&#39;, &#39;text.usetex&#39;: True, &#39;figure.figsize&#39;: fig_size, # include here any neede package for latex &#39;text.latex.preamble&#39;: [r&#39; usepackage{amsmath}&#39;, ], } plt.rcParams.update(params) plt.ioff() plt.clf() # figsize accepts only inches. fig = plt.figure(1, figsize=fig_size) fig.subplots_adjust(left=0.12, right=0.96, top=0.96, bottom=0.18, hspace=0.02, wspace=0.02) ax = fig.add_subplot(111) . def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=256): new_cmap = matplotlib.colors.LinearSegmentedColormap.from_list( &#39;trunc({n},{a:.2f},{b:.2f})&#39;.format(n=cmap.name, a=minval, b=maxval), cmap(np.linspace(minval, maxval, n))) return new_cmap cmap = plt.get_cmap(&#39;YlOrBr&#39;) my_cmap = truncate_colormap(cmap, 0.2, 0.9) . minX = 0 maxX = 10 minY = 0 maxY = 3 N = 50j y, x = np.mgrid[minY:maxY:N, minX:maxX:N] z = 2 * np.exp(-(0.02 * (x + 1) ** 2 + 0.05 * (y - 3.1) **2 )) divisions = np.arange(0.3, 2.1, 0.3) divisions2 = np.append(divisions, 2.5) divisions2 = np.append(-0.5, divisions2) # contour filled with colors ax.contourf(x, y, z, divisions2, cmap=my_cmap, vmin=0.0,vmax=2.0) # contour lines cont = ax.contour(x, y, z, divisions, colors=2 * [&#39;black&#39;] + [&#39;green&#39;] + 3 * [&#39;black&#39;], linewidth=.5) zcontour = cont.collections[1] dash1=(15, 10, 15, 10) zcontour.set_dashes([(0, dash1)]) zcontour = cont.collections[4] dash2=(20, 30) zcontour.set(color=&#39;blue&#39;, linestyle=[(0, dash2)], linewidth=4) # labels manual_locations = [(1.0, 2.5), (2.5, 2.5), (3.8, 2.5), (5.0, 2.5), (6.2, 2.5), (8.3, 2.5)] ax.clabel(cont, inline=1, fontsize=tick_size, fmt=&#39;z=%.2f%%&#39;, manual=manual_locations, colors=5 * [&#39;black&#39;] + [&#39;white&#39;]) ax.set_xlabel(r&quot;$x$ axis&quot;) ax.set_ylabel(r&quot;$y$ axis&quot;) fig . /Users/yairmau/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: The following kwargs were not used by contour: &#39;linewidth&#39; . %matplotlib notebook fig.savefig(&quot;./python_figures/contours.png&quot;,dpi=300) # fig.savefig(&quot;cont.eps&quot;) .",
            "url": "https://yairmau.github.io/jupyter/2020/01/01/contours.html",
            "relUrl": "/jupyter/2020/01/01/contours.html",
            "date": " ‚Ä¢ Jan 1, 2020"
        }
        
    
  
    
        ,"post48": {
            "title": "What's the best path to save someone from drowning?",
            "content": ". Introduction . Snell&#39;s law of refraction can be understood in this example, where the lifeguard wants to minimize the time it takes to get to the drowning person. . Code . import matplotlib import matplotlib.pyplot as plt import numpy as np . # http://wiki.scipy.org/Cookbook/Matplotlib/LaTeX_Examples # this is a latex constant, don&#39;t change it. pts_per_inch = 72.27 # write &quot; the textwidth&quot; (or &quot; showthe columnwidth&quot; for a 2 collumn text) text_width_in_pts = 252.0 # inside a figure environment in latex, the result will be on the # dvi/pdf next to the figure. See url above. text_width_in_inches = text_width_in_pts / pts_per_inch # make rectangles with a nice proportion golden_ratio = 0.618 # figure.png or figure.eps will be intentionally larger, because it is prettier inverse_latex_scale = 2 # when compiling latex code, use # includegraphics[scale=(1/inverse_latex_scale)]{figure} # we want the figure to occupy 2/3 (for example) of the text width fig_proportion = (3.0 / 3.0) csize = inverse_latex_scale * fig_proportion * text_width_in_inches # always 1.0 on the first argument fig_size = (1.0 * csize,golden_ratio * csize) # find out the fontsize of your latex text, and put it here text_size = inverse_latex_scale * 12 tick_size = inverse_latex_scale * 8 # learn how to configure: # http://matplotlib.sourceforge.net/users/customizing.html params = {&#39;axes.labelsize&#39;: text_size, &#39;legend.fontsize&#39;: tick_size, &#39;legend.handlelength&#39;: 2.5, &#39;legend.borderaxespad&#39;: 0, &#39;xtick.labelsize&#39;: tick_size, &#39;ytick.labelsize&#39;: tick_size, &#39;font.family&#39;: &#39;serif&#39;, &#39;font.size&#39;: text_size, # Times, Palatino, New Century Schoolbook, # Bookman, Computer Modern Roman # &#39;font.serif&#39;: [&#39;Times&#39;], # &#39;ps.usedistiller&#39;: &#39;xpdf&#39;, &#39;text.usetex&#39;: True, &#39;figure.figsize&#39;: fig_size, # include here any neede package for latex &#39;text.latex.preamble&#39;: [r&#39; usepackage{amsmath}&#39;, ], } plt.rcParams.update(params) # plt.clf() # figsize accepts only inches. plt.ioff() fig = plt.figure(1, figsize=fig_size) fig.subplots_adjust(left=0.0, right=1.0, top=1.0, bottom=0.0, hspace=0.02, wspace=0.02) ax = fig.add_subplot(111) . origin = [0, 0] lifeguard = [-3, -2] drowning = [2, 3] ax.set_xticks([]) ax.set_yticks([]) xlim = [-4, 4] ylim = [-4, 4] ax.axis([xlim[0], xlim[1], ylim[0], ylim[1]]) ##### drowning ##### # line ax.plot([origin[0], drowning[0]], [origin[1], drowning[1]], color=&quot;black&quot;, lw=2) # diamond ax.plot(drowning[0], drowning[1], &quot;D&quot;, markerfacecolor=&quot;black&quot;, markersize=10, markeredgewidth=3, color=&quot;black&quot;) # explanation ax.text(drowning[0], drowning[1] - 0.3, r&quot;drowning&quot;, verticalalignment=&quot;top&quot;) ax.text(drowning[0], drowning[1] - 0.8, r&quot;person&quot;, verticalalignment=&quot;top&quot;) ##### lifeguard ##### # line ax.plot([origin[0], lifeguard[0]], [origin[1], lifeguard[1]], color=&quot;black&quot;, lw=2) # circle ax.plot(lifeguard[0], lifeguard[1], &quot;o&quot;, markerfacecolor=&quot;black&quot;, markersize=10, markeredgewidth=3, color=&quot;black&quot;) # explanation ax.text(lifeguard[0], lifeguard[1] - 0.3, r&quot;lifeguard&quot;, verticalalignment=&quot;top&quot;) . Text(-3, -2.3, &#39;lifeguard&#39;) . background colors . sand = matplotlib.patches.Rectangle([xlim[0], ylim[0]], (xlim[1] - xlim[0]), (ylim[1] - ylim[0]) / 2.0, color=&quot;yellow&quot;, alpha=0.6) ax.add_patch(sand) sea = matplotlib.patches.Rectangle([xlim[0], 0], (xlim[1] - xlim[0]), (ylim[1] - ylim[0]) / 2.0, color=&quot;blue&quot;, alpha=0.4) ax.add_patch(sea) ###### sand ##### ax.text(0.95, 0.05, r&quot;(1) sand&quot;, transform=ax.transAxes, horizontalalignment=&#39;right&#39;) ###### sea ##### ax.text(0.95, 0.50, r&quot;(2) sea&quot;, transform=ax.transAxes, horizontalalignment=&#39;right&#39;, verticalalignment=&quot;bottom&quot;) . Text(0.95, 0.5, &#39;(2) sea&#39;) . spines through origin . # http://matplotlib.org/examples/pylab_examples/spine_placement_demo.html ax.spines[&#39;left&#39;].set_position(&#39;zero&#39;) ax.spines[&#39;right&#39;].set_color(&#39;none&#39;) ax.spines[&#39;bottom&#39;].set_position(&#39;zero&#39;) ax.spines[&#39;top&#39;].set_color(&#39;none&#39;) ax.spines[&#39;left&#39;].set_smart_bounds(False) ax.spines[&#39;bottom&#39;].set_smart_bounds(False) # ax.xaxis.set_ticks_position(&#39;bottom&#39;) # ax.yaxis.set_ticks_position(&#39;left&#39;) ax.set_xticks([]) ax.set_yticks([]) . [] . annotations . # h_1 ax.annotate(&quot;&quot;, xy=(lifeguard[0] - 0.3, lifeguard[1]), xycoords=&#39;data&#39;, xytext=(lifeguard[0] - 0.3, 0), textcoords=&#39;data&#39;, size=tick_size, arrowprops=dict(arrowstyle=&quot;&lt;-&gt;&quot;, connectionstyle=&quot;arc3&quot;), ) ax.text(lifeguard[0] - 0.2, lifeguard[1] / 2.0, r&quot;$h_1$&quot;, verticalalignment=&quot;center&quot;, horizontalalignment=&quot;left&quot;) # h_2 ax.annotate(&quot;&quot;, xy=(lifeguard[0] - 0.3, drowning[1]), xycoords=&#39;data&#39;, xytext=(lifeguard[0] - 0.3, 0), textcoords=&#39;data&#39;, size=tick_size, arrowprops=dict(arrowstyle=&quot;&lt;-&gt;&quot;, connectionstyle=&quot;arc3&quot;), ) ax.text(lifeguard[0] - 0.2, drowning[1] / 2.0, r&quot;$h_2$&quot;, verticalalignment=&quot;center&quot;, horizontalalignment=&quot;left&quot;) # L ax.annotate(&quot;&quot;, xy=(lifeguard[0], drowning[1] + 0.3), xycoords=&#39;data&#39;, xytext=(drowning[0], drowning[1] + 0.3), textcoords=&#39;data&#39;, size=tick_size, arrowprops=dict(arrowstyle=&quot;&lt;-&gt;&quot;, shrinkA=0, shrinkB=0, connectionstyle=&quot;arc3&quot;), ) ax.text((lifeguard[1] - lifeguard[0]) / 2.0, drowning[1] + 0.3, r&quot;$L$&quot;, verticalalignment=&quot;bottom&quot;, horizontalalignment=&quot;left&quot;) # l1 ax.text(lifeguard[0] / 2.0, 1.10 * lifeguard[1] / 2.0, r&quot;$ ell_1$&quot;, verticalalignment=&quot;top&quot;, horizontalalignment=&quot;left&quot;) # l2 ax.text(drowning[0] / 2.0, 0.95 * drowning[1] / 2.0, r&quot;$ ell_2$&quot;, verticalalignment=&quot;top&quot;, horizontalalignment=&quot;left&quot;) # theta_1 ax.annotate(&quot;&quot;, xy=(0, 0.5 * lifeguard[1]), xycoords=&#39;data&#39;, xytext=(0.2 * lifeguard[0], 0.2 * lifeguard[1]), textcoords=&#39;data&#39;, size=tick_size, arrowprops=dict(arrowstyle=&quot;-&quot;, lw=2, connectionstyle=&quot;angle3,angleA=-60,angleB=0&quot;), ) ax.text(0.1 * lifeguard[0], 0.5 * lifeguard[1], r&quot;$ theta_1$&quot;, verticalalignment=&quot;top&quot;, horizontalalignment=&quot;right&quot;) # theta_2 ax.annotate(&quot;&quot;, xy=(0, 0.3 * drowning[1]), xycoords=&#39;data&#39;, xytext=(0.2 * drowning[0], 0.2 * drowning[1]), textcoords=&#39;data&#39;, size=tick_size, arrowprops=dict(arrowstyle=&quot;-&quot;, lw=2, connectionstyle=&quot;angle3,angleA=120,angleB=0&quot;), ) ax.text(0.1 * drowning[0], 0.5 * drowning[1], r&quot;$ theta_2$&quot;, verticalalignment=&quot;top&quot;, horizontalalignment=&quot;left&quot;) . Text(0.2, 1.5, &#39;$ theta_2$&#39;) . %matplotlib inline fig.savefig(&quot;python_figures/bestpath.png&quot;, dpi=300) fig .",
            "url": "https://yairmau.github.io/jupyter/2020/01/01/bestpath.html",
            "relUrl": "/jupyter/2020/01/01/bestpath.html",
            "date": " ‚Ä¢ Jan 1, 2020"
        }
        
    
  
    
        ,"post49": {
            "title": "Bar plot",
            "content": ". Introduction . This code produces the figure above. Here we showcase the use of unicode text. . The code . from __future__ import unicode_literals import numpy as np import matplotlib import matplotlib.pyplot as plt from bidi import algorithm as bidialg # needed for arabic, hebrew import arabic_reshaper # needed for arabic . ModuleNotFoundError Traceback (most recent call last) &lt;ipython-input-2-aaa74850711b&gt; in &lt;module&gt; 4 import matplotlib.pyplot as plt 5 from bidi import algorithm as bidialg # needed for arabic, hebrew -&gt; 6 import arabic_reshaper # needed for arabic ModuleNotFoundError: No module named &#39;arabic_reshaper&#39; . # http://wiki.scipy.org/Cookbook/Matplotlib/LaTeX_Examples pts_per_inch = 72.27 # this is a latex constant, don&#39;t change it. text_width_in_pts = 300.0 # write &quot; the textwidth&quot; (or &quot; showthe columnwidth&quot; for a 2 collumn text) # inside a figure environment in latex, the result will be on the dvi/pdf next to the figure. See url above. text_width_in_inches=text_width_in_pts/pts_per_inch golden_ratio = 0.618 # make rectangles with a nice proportion inverse_latex_scale = 2 # figure.png or figure.eps will be intentionally larger, because it is prettier # when compiling latex code, use includegraphics[scale=(1/inverse_latex_scale)]{figure} fig_proportion = (3.0 / 3.0) # we want the figure to occupy 2/3 (for example) of the text width csize = inverse_latex_scale * fig_proportion * text_width_in_inches fig_size = (1 * csize, 1.3 * csize) # always 1.0 on the first argument text_size = inverse_latex_scale * 10 # find out the fontsize of your latex text, and put it here label_size = inverse_latex_scale * 10 tick_size = inverse_latex_scale * 8 # learn how to configure: # http://matplotlib.sourceforge.net/users/customizing.html params = {&#39;backend&#39;: &#39;ps&#39;, &#39;axes.labelsize&#39;: 16, &#39;legend.fontsize&#39;: 14, &#39;legend.handlelength&#39;: 2.5, &#39;legend.borderaxespad&#39;: 0, &#39;axes.labelsize&#39;: label_size, &#39;xtick.labelsize&#39;: tick_size, &#39;ytick.labelsize&#39;: tick_size, # &#39;font.family&#39;: &#39;serif&#39;, &#39;font.size&#39;: text_size, # &#39;font.serif&#39;: [&#39;Computer Modern Roman&#39;], &#39;ps.usedistiller&#39;: &#39;xpdf&#39;, # &#39;text.usetex&#39;: True, &#39;figure.figsize&#39;: fig_size, #&#39;text.latex.unicode&#39;: True, } plt.rcParams.update(params) plt.ioff() fig = plt.figure(1, figsize=fig_size) # figsize accepts only inches. fig.clf() dpi = 100 pixel_size = (700,700) fig_size = (pixel_size[0]/dpi,pixel_size[1]/dpi) ax1=fig.add_subplot(211) ax2=fig.add_subplot(212) fig.subplots_adjust(left=0.30, right=0.97, top=0.95, bottom=0.06, hspace=0.2, wspace=0.1) . words = [u&#39;Fu√üball √Öngstr√∂m ns√∏ster –†–æ—Å—Å–∏—è&#39;, u&#39;fran√ßais ma√Ætre nvoil√† √©go√Øste&#39;, u&#39;Espa√±a&#39;, u&#39;ƒ∞stanbul aƒüzƒ±&#39;, u&#39;Anything Unicode&#39; ] values1 = [2575, 5851, 3191, 2303, 3029] values2 = [4813, 5219, 5505, 6229, 6961] values1 = np.array(values1) values2 = np.array(values2) width = 0.35 # the width of the bars r = np.arange(len(values1)) . ax1 , horizontal bars . v1 = ax1.barh(r, values1, width, color=&#39;pink&#39;) v2 = ax1.barh(r + width, values2, width, color=&#39;brown&#39;) ax1.axis([0, 8600, r.min() - 0.3, r.max() + 1]) ax1.set_yticks(r) ax1.set_yticks(r + 1 * width) ax1.set_yticklabels(words) xt = np.arange(0, 8100, 1000) ax1.set_xticks(xt) ax1.set_xticklabels(xt) ax1.set_xlabel(u&#39;the values&#39;, fontsize=16) ax1.set_title(u&#39;Title here&#39;, fontsize=18) ax1.xaxis.grid(True) ax1.tick_params( axis=&#39;y&#39;, # changes apply to the y-axis which=&#39;both&#39;, # both major and minor ticks are affected left=&#39;off&#39;, # ticks along the left edge are off right=&#39;off&#39;, # ticks along the right edge are off labelleft=&#39;on&#39;) # labels along the bottom edge are on ax1.legend((v1, v2), (u&#39;2016&#39;, u&#39;2015&#39;), loc=(0.74,0.05)) def autolabel_hor(rects,ax, offset_x, offset_y): # attach some text labels at the tip of the bars for i,rect in enumerate(rects): width = rect.get_width() height = rect.get_height() ax.text(width + offset_x, rect.get_y() + offset_y * height, &#39;%d&#39; % int(width), ha=&#39;left&#39;, va=&#39;bottom&#39;, fontsize=14) autolabel_hor(v1, ax1, 100.0, -0.20) autolabel_hor(v2, ax1, 100.0, -0.10) . ax2, vertical bars . from bidi import algorithm as bidialg w1 = ax2.bar(r, values1, width, color=&#39;pink&#39;) w2 = ax2.bar(r + width, values2, width, color=&#39;brown&#39;) ax2.axis([r.min() - 0.3, r.max() + 1, 0, 8600]) ax2.set_xticks(r) ax2.set_xticks(r + 1 * width) shalom = bidialg.get_display(u&#39;◊©◊ú◊ï◊ù&#39;) salam = bidialg.get_display(arabic_reshaper.reshape(u&#39;ÿ≥ŸÑÿßŸÖ&#39;)) ax2.set_xticklabels([shalom, salam, &#39;ccc&#39;, &#39;ddd&#39;, &#39;eee&#39;]) xt = np.arange(0, 8200, 1000) ax2.set_yticks(xt) ax2.set_yticklabels(xt) ax2.yaxis.grid(True) ax2.tick_params( axis=&#39;x&#39;, # changes apply to the x-axis which=&#39;both&#39;, # both major and minor ticks are affected top=&#39;off&#39;, # ticks along the top edge are off bottom=&#39;off&#39;, # ticks along the bottom edge are off labelbottom=&#39;on&#39;) # labels along the bottom edge are on ax2.legend((w1, w2), (u&#39;2016&#39;, u&#39;2015&#39;), loc=&quot;upper center&quot;) def autolabel_ver(rects,ax, offset_x, offset_y): # attach some text labels at the tip of the bars for i,rect in enumerate(rects): width = rect.get_width() height = rect.get_height() ax.text(rect.get_x() + offset_x * width, height + offset_y, &#39;%d&#39; % int(height), ha=&#39;left&#39;, va=&#39;bottom&#39;, fontsize=14) autolabel_ver(w1, ax2, -0.3, 100.0) autolabel_ver(w2, ax2, 0., 100.0) . %matplotlib inline fig.savefig(&quot;./python_figures/bars.png&quot;) plt.show() fig .",
            "url": "https://yairmau.github.io/jupyter/2020/01/01/bars.html",
            "relUrl": "/jupyter/2020/01/01/bars.html",
            "date": " ‚Ä¢ Jan 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me, Yair",
          "content": "This is where you put the contents of your About page. Like all your pages, it‚Äôs in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://yairmau.github.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Cartoons",
          "content": "https://www.gocomics.com/calvinandhobbes/2015/01/12 . . https://xkcd.com/2207/ . . https://www.smbc-comics.com/comic/2013-06-16 . . Science: Abridged Beyond the Point of Usefulness . . http://www.sciencecartoonsplus.com/gallery/physics/galphys2j.php# . . .",
          "url": "https://yairmau.github.io/teaching/physics71031/gifs/cartoons.html",
          "relUrl": "/teaching/physics71031/gifs/cartoons.html",
          "date": ""
      }
      
  

  
      ,"page3": {
          "title": "Energy",
          "content": "Elastic energy . The First Hold &amp; Release Bungee Jump | Damien Walters . https://youtu.be/iN1beukMJJc . Potential and Kinetic Energy . Visualization of conservation of energy . High road low road track race, potential-kinetic energy tracks . https://youtu.be/_GJujClGYJQ . Mondo Duplantis 2018, play at 0.25 speed . 150 Ton Hydraulic Guillotine Vs Deck of Cards .",
          "url": "https://yairmau.github.io/teaching/physics71031/gifs/energy.html",
          "relUrl": "/teaching/physics71031/gifs/energy.html",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "Extra materials - 1d Kinematics",
          "content": "The Physics Classroom . This is an excellent interactive website, with lots of useful exercises: Distance vs. Displacement, Acceleration, Name That Motion, Motion Diagrams, Graph That Motion, Match That Graph, Position-Time Graphs - Conceptual Analysis, Position-Time Graphs - Numerical Analysis, Dots and Graphs, Which One Doesn‚Äôt Belong?, Free Fall, Up and Down. . Video Lectures . Khan Academy - One-dimensional motion Motion in a Straight Line: Crash Course Physics #1 Michel van Biezen - Lectures in MOTION IN ONE DIMENSION Michel van Biezen - Lectures in Motion in 1 Dimension: GRAPHIC SOLUTIONS . $x$, $v$, $a$ graphs . Draw the missing curves, with black dots in the same instants in time as in the given curve. All curved lines are parabolas. . Exercise 1 . Exercise 2 answer . Exercise 3 answer . Exercise 4 answer . Exercise 5 answer . Exercise 6 answer . Match the graphs, $x$ and $v$ . Match the curve on the left with one of the curves on the right. . Exercise 1 . Exercise 2 . Exercise 3 . Exercise 4 . Exercise 5 . Exercise 6 . Exercise 7 . Exercise 8 . Exercise 9 . Exercise 10 . Exercise 11 . Exercise 12 . Oil drop patterns . Oil drips from a car at fixed time intervals. Match the oil drop pattern the car leaves on the road with the curves on the right. Attention: there might be more than one solution! . Exercise 1 . Exercise 2 . Exercise 3 . Exercise 4 . Exercise 5 . Exercise 6 . Exercise 7 . Exercise 8 . Exercise 9 . Exercise 10 . Exercise 11 . Exercise 12 . Exercise 13 . Exercise 14 . Exercise 15 . Exercise 16 . Exercise 17 . Exercise 18 .",
          "url": "https://yairmau.github.io/teaching/physics71031/extra/extra-1d-kinematics.html",
          "relUrl": "/teaching/physics71031/extra/extra-1d-kinematics.html",
          "date": ""
      }
      
  

  
      ,"page5": {
          "title": "Extra materials - Basic Math",
          "content": "I will assume that student in this course have a minimal proficiency in math. Find below some links for basic math that we will need during this course. I will not teach any of these topics, if you feel that you don‚Äôt fully know this stuff, please go ahead and study these topics asap. . Trigonometry . Khan Academy Michel van Biezen . Pre-algebra . Arithmetic properties; factors and multiples; fractions; decimals; negative numbers and coordinate plane; rations, rates, proportions; equations, expressions, and inequalities; exponents, radicals, and scientific notation. . Khan Academy . Algebra . Michel van Biezen . ◊¢◊ë◊®◊ô◊™ . ◊ß◊ô◊ô◊ù ◊ê◊™◊® ◊©◊ú ◊ê◊ß◊ì◊û◊ô◊ô◊™ ◊ß◊î◊ê◊ü ◊ë◊¢◊ë◊®◊ô◊™, ◊õ◊ì◊ê◊ô ◊ú◊î◊õ◊ô◊® ◊ê◊ï◊™◊ï .",
          "url": "https://yairmau.github.io/teaching/physics71031/extra/extra-basic-math.html",
          "relUrl": "/teaching/physics71031/extra/extra-basic-math.html",
          "date": ""
      }
      
  

  
      ,"page6": {
          "title": "Extra materials - Momentum",
          "content": "Momentum, Lecture 1 . Momentum, Lecture 2 . Videos of people flying backwards after being shot: . Bruce Willis (watch the few first seconds) | Uma Thurman | Morgan Freeman (watch from 1:10) | . Momentum, Lecture 3 . Videos of a Newton‚Äôs Cradle . Newton‚Äôs Cradle with a High-Speed Video Camera | Amazing Demonstration Of A Giant Newton‚Äôs Cradle! | .",
          "url": "https://yairmau.github.io/teaching/physics71031/extra/extra-momentum.html",
          "relUrl": "/teaching/physics71031/extra/extra-momentum.html",
          "date": ""
      }
      
  

  
      ,"page7": {
          "title": "Extra materials - Units",
          "content": "basic units and prefixes . Units for three SI base quantities . Quantity Unit Name Unit Symbol . Length [L] | meter | m | . Time [T] | second | s | . Mass [M] | kilogram | kg | . Some prefixes for SI Units that you must remember! . Factor Prefix Symbol . $10^9$ | giga- | G | . $10^6$ | mega- | M | . $10^3$ | kilo- | k | . $10^{-2}$ | centi- | c | . $10^{-3}$ | milli- | m | . $10^{-6}$ | micro- | $ mu$ | . $10^{-9}$ | nano- | n | . exponent rules . . volume and surface area . . 1 horse-sized duck or 100 duck-sized horses? . .",
          "url": "https://yairmau.github.io/teaching/physics71031/extra/extra-units.html",
          "relUrl": "/teaching/physics71031/extra/extra-units.html",
          "date": ""
      }
      
  

  
      ,"page8": {
          "title": "Fluids",
          "content": "f ## Fluids . How a 50 kg iron working anvil floats on liquid mercury From https://twitter.com/Rainmaker1973/status/1194215829199605765?s=09 . Hydrostatic pressure . Pressure change during diving . Pressure change during diving pic.twitter.com/PZjiR7w004 . &mdash; Physics-astronomy (@Physicsastronmy) November 25, 2021 Fish tower . The Hydrostatic Paradox - Explained! . Pascal‚Äôs Blaising Barrel - Exploding Glass Barrel with Water Pressure . The Pressure Paradox #VeritasiumContest #GrandPrizeWinner . Surface tension . from https://twitter.com/Rainmaker1973/status/1191329332926570497?s=09 .",
          "url": "https://yairmau.github.io/teaching/physics71031/gifs/fluids.html",
          "relUrl": "/teaching/physics71031/gifs/fluids.html",
          "date": ""
      }
      
  

  
      ,"page9": {
          "title": "",
          "content": "Senior Lecturer [Assistant Professor] The Hebrew University of Jerusalem The Department of Soil and Water Sciences Robert H. Smith Faculty of Agriculture, Food and Environment . About . I‚Äôm interested in the interactions between soil, water and vegetation in drylands, in both natural and agricultural ecosystems. Our goal is to understand how the basic processes and feedbacks influence the ecosystem dynamics, and how this knowledge can be used to control those systems. . The common theme between my different research projects is land/ecosystem degradation caused by human activity. . Our research is based on ‚Äúsimple‚Äù mathematical models, that strive to capture the essential physical processes, while providing deep insight into the dynamics of the system. Some of the tools we use in the modeling of environmental questions come from dynamical systems, statistical physics and optimal control theory. . [Read more on current research projects‚Ä¶] . I love to write code, and I made available on the Tutorials page some working examples in Python and LaTeX. Check it out! . How to find me . ‚ÄÇ yair.mau@mail.huji.ac.il ‚ÄÇ +972 8 948 9386 ‚ÄÇ Rehovot Campus, Lubell building, office 19. Map here . Google Scholar Check out my CV . . Piet Hein Problems worthy ‚ÄÉ‚ÄÉof attack prove their worth ‚ÄÉ‚ÄÉby hitting back. .",
          "url": "https://yairmau.github.io/inde_old.html",
          "relUrl": "/inde_old.html",
          "date": ""
      }
      
  

  
      ,"page10": {
          "title": "Surface Hydrology",
          "content": "A Jupyter Notebook will open whenever you click on a button like this. You can then see its source code and download it by clicking on ‚ÄúView on GitHub‚Äù. . Filessss . Topic Lecture Exercises Code Data . 01 - Introduction | | | ¬† | ¬† | . 2020-2021 | ‚ÄÇ | ‚ÄÇ | ‚ÄÇ | ¬† | . 2019-2020 | ‚ÄÇ | ‚ÄÇ | ‚ÄÇ | ‚ÄÇ | . 2018-2019 | ‚ÄÇ | ‚ÄÇ | ‚ÄÇ | ¬† | . 2017-2018 | ‚ÄÇ | ‚ÄÇ | ‚ÄÇ | ¬† | . 01 - Introduction to the course . A very wide overview Lecture What is Hydrology? What are the main processes in the hydrologic cycle? An overview, based on the excellent USGS Water Science School. . Python intro Exercises Let‚Äôs have fun plotting some data. If you need, download the necessary data for this exercise here and here. . . . 02 - Precipitation . Data Data If necessary, download the following data to be used in the Jupyter Notebooks below: Bilbao (daily, and monthly), Eilat (daily, and monthly), Tel Aviv (daily, and monthly), Beer Sheva (daily, and monthly), Ben-Gurion airport (daily, and monthly), London (daily, and monthly). . Interannual variability of precipitation Lecture Quantifying interannual variability of precipitation. . Intra-annual variability of precipitation Lecture A comparison between the seasonality of Tel Aviv and London . Practice calculating inter- and intra-annual variability Exercises Now it‚Äôs your turn to calculate and plot graphs. . Code of the lectures above Code Interannual variability of precipitation; Code Intra-annual variability of precipitation . Return period Lecture Was Bilbao‚Äôs 1983 ‚Äúfreak‚Äù flood such an unexpected event? How often should we expect very large downpours? . Practice calculating return periods Exercises Now it‚Äôs your turn to calculate and plot graphs. . Code of return period Code Extremely dirty code, it‚Äôs really for my own reference :) . Assignment 1 Assignment First assignment . . . 03 - Evapotranspiration . Data Data If necessary, download the following data to be used in the Jupyter Notebooks below: Bet dagan (3h-data, evaporation pan, radiation), Headers for sub hourly data, . . Evapotranspiration - definitions and models Lecture . Evapotranspiration - Thornthwaite and Penman equations Exercises . Assignment 2 Assignment Second assignment . . . 04 - Infiltration . Data Data Nassif and Wilson (1975), ‚ÄúThe influence of slope and rain intensity on runoff and infiltration‚Äù, download here, figure 8 (2nd panel) from this paper download here, 4 csv files of infiltration rate against time, download here. . Infiltration Lecture Definitions, factors that influence infiltration, Horton equation, Green &amp; Ampt equation, the least squares method. . Infiltration Exercises Let‚Äôs practice! . . . 05 - Streamflow . Streamflow Lecture . Streamflow exercises Exercises . Unit Hydrograph Lecture . Assignment 3 Assignment Third assignment . . . 06 - Budyko‚Äôs framework . Budyko‚Äôs framework Lecture . . . 07 - Spatial variability . Spatial distribution Lecture . . . 08 - Final Assignment . Assignment .",
          "url": "https://yairmau.github.io/teaching/hydrology/",
          "relUrl": "/teaching/hydrology/",
          "date": ""
      }
      
  

  
      ,"page11": {
          "title": "Tutorials",
          "content": "Tutorials . . Python tutorial . $ LaTeX$ tutorial .",
          "url": "https://yairmau.github.io/tutorials/",
          "relUrl": "/tutorials/",
          "date": ""
      }
      
  

  
      ,"page12": {
          "title": "Five-Star Content I Recommend",
          "content": "Books . [Check out my Goodreads profile]. . Superintelligence: Paths, Dangers, Strategies, by Nick Bostrom. | The Unfolding of Language: An Evolutionary Tour of Mankind‚Äôs Greatest Invention, by Guy Deutscher. | . Blogs . Wait but why, by Tim Urban. | Fuck Yeah Fluid Dynamics, by Nicole Sharp. | . Short stories . They‚Äôre made out of meat, by Terry Bisson. | The Sentinel, by Arthur C. Clarke. | On Exactitude in Science, by Jorge Luis Borges. | The Feeling Of Power , by Isaac Asimov. | . Technical stuff . Linux Shell Scripting Tutorial | Pythonic Perambulations, by Jake VanderPlas. | . YouTube channels . 3Blue1Brown, minute physics, Veritasium, Smarter Every Day, CGP Grey, Kurzgesagt, Primitive Technology, Vsauce, Nerdwriter, Mathologer, Sciencium, Steve Mould . Podcasts . Hardcore History with Dan Carlin, especially the series ‚ÄúBlueprint for Armageddon‚Äù and ‚ÄúWrath of the Khans‚Äù. | 99% Invisible | Revisionist History | Lexicon Valley | Making Sense, with Sam Harris | .",
          "url": "https://yairmau.github.io/more/five-stars/",
          "relUrl": "/more/five-stars/",
          "date": ""
      }
      
  

  
      ,"page13": {
          "title": "Tools",
          "content": "Programming . Anaconda . ‚ÄúThe easiest way to perform Python/R data science and machine learning on Linux, Windows, and Mac OS X.‚Äù . Sublime Text . ‚ÄúSublime Text is a sophisticated text editor for code, markup and prose.‚Äù . Jupyter Notebook . ‚ÄúThe Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text.‚Äù . CodeCademy . ‚ÄúLearn to code interactively, for free.‚Äù . Documents . PDFtk . ‚ÄúUse PDFtk Pro to quickly split, merge, rotate, watermark, stamp and secure PDF pages and documents‚Äù Command line version (powerful, see examples); friendly graphical tool for Windows. . Overleaf . ‚ÄúAn online LaTeX editor that‚Äôs easy to use.‚Äù By the way, Overleaf has the best LaTeX tutorials out there. Check it out. Also, see this excellent WikiBook on LaTeX. . BRISS . ‚ÄúA simple cross-platform (Linux, Windows, Mac OSX) application for cropping PDF files‚Äù . Data . Figshare . ‚ÄúFigshare is an online digital repository where researchers can preserve and share their research outputs, including figures, datasets, images, and videos.‚Äù . BitBucket . ‚ÄúBitbucket is a web-based hosting service for projects that use Git revision control systems.‚Äù . WebPlotDigitizer . ‚ÄúExtract data from plots, images, and maps.‚Äù . $ LaTeX$ . doi2bib . ‚ÄúGive us a DOI and we‚Äôll do our best to get you the BibTeX entry.‚Äù . Tables Generator . ‚ÄúQuickly create even complex LaTeX tables online with our generator ‚Äì cells merging is supported together with borders editing.‚Äù .",
          "url": "https://yairmau.github.io/more/tools/",
          "relUrl": "/more/tools/",
          "date": ""
      }
      
  

  
      ,"page14": {
          "title": "Physics A (extended) - 71031",
          "content": "Goals . This basic Physics course has three main goals: . By the end of this course you will be able to look at the world around you and start describing it in a formal (mathematical) language. | By the end of this course you will be able to read a graph and tell the story behind it, i.e., you will translate a formal (mathematical) language into regular speech any human can understand. | We will get acquainted with the concept of Energy. This, in my humblest opinion, is one of the most important ideas in Science. | . Patron saint . Galileo is undoubtedly our hero in this course. Honorable runners-up: Archimedes and Newton. . . Lecture Notes (in hebrew) and more . Book: I loosely use Halliday &amp; Resnick‚Äôs Principles of Physics (11th edtion). Software: I use Stylus Labs Write to write my classnotes, it is available for Windows, Mac, Linux, Android, and iOS. Hardware: I use both a Wacom Cintiq 16 and an iPad air. Legend: lecture notes pdf lecture notes source (write) svgz powerpoint widget in jupyter notebook (might take a while to load‚Ä¶) other materials animations and gifs . subject lectures other . basic math | ¬† | | . units | ‚ÄÇ | ‚ÄÇ | . 1d kinematics | ‚ÄÇ | ‚ÄÇ ‚ÄÇ ‚ÄÇ | . vectors | ‚ÄÇ | ¬† | . 2d kinematics | ‚ÄÇ | ‚ÄÇ | . circular motion | ‚ÄÇ | ‚ÄÇ | . Newton‚Äôs laws | ‚ÄÇ | ‚ÄÇ ‚ÄÇ | . work-energy theorem | ‚ÄÇ | ‚ÄÇ | . potential energy | ‚ÄÇ | | . potential energy diagrams | ‚ÄÇ | ¬† | . linear momentum | ‚ÄÇ | ‚ÄÇ | . gravitation | ‚ÄÇ | | . hydrostatics | ‚ÄÇ | | . hydrodynamics | ‚ÄÇ | ‚ÄÇ | . Click here for details on all lectures of the 2021-22 academic year. Here are other very nice videos not directly related to any specific topic. . Some past exams (in hebrew) . Year Midterm Moed A Moed B Moed C . 2021-2022 | ‚ÄÇ | ‚ÄÇ | ¬† | ¬† | . 2020-2021 | ‚ÄÇ | ‚ÄÇ | ‚ÄÇ | ¬† | . 2019-2020 | ‚ÄÇ | ‚ÄÇ | ‚ÄÇ | ‚ÄÇ | . 2018-2019 | ‚ÄÇ | ‚ÄÇ | ‚ÄÇ | ¬† | . 2017-2018 | ‚ÄÇ | ‚ÄÇ | ‚ÄÇ | ¬† | . Extra stuff . Physics cartoons . Great online Physics resources: . Khan Academy - English | Khan Academy - ◊¢◊ë◊®◊ô◊™ | Michel van Biezen | Walter Lewin‚Äôs 8.01x - MIT Physics I: Classical Mechanics | PhET: fun, free, interactive, research-based science and mathematics simulations | . link for zoom meetings .",
          "url": "https://yairmau.github.io/teaching/physics71031/",
          "relUrl": "/teaching/physics71031/",
          "date": ""
      }
      
  

  
      ,"page15": {
          "title": "Agrotech Lab",
          "content": "Course goals . The goal of this course is to give a first hands-on experience with experimental design data collection, analysis and reporting. We want students to creatively engage with their projects, finding by themselves solutions to the challenges they encounter. The role of the teaching staff will be to consult and advise, not spoon-feed solutions. . The main values we would like to promote: curiosity, independence, creativity, and a can-do attitude. . Guiding questions and Description . This course deals with the design of a simple experiment focusing on data collection and analysis. The guiding questions throughout this course are: . What is my research question? | What data is needed? | How will data be collected? | How will data be analyzed? | How will data be reported? | . Throughout the course, students will critically engage with each of these questions, and as a case study, will build an experiment to monitor the flow of water in the soil-plant-atmosphere continuum, first a joint experiment for the whole class and then a private project which will be selected from a list of topics. The list of projects covers several disciplines, including biological and microbiological science, as well as soil and water sciences. The culmination of this course will be a final project, in which students will devise an experiment and apply the acquired tools to a subject of their choosing, and submit a report on it. See below some of the past projects developed in our course. . The monitoring and control systems to be build are based on the ESP32, a versatile (and cheap!) microcontroller. The ESP32 is similar to the Arduino, with the advantage that it has inbuilt Wi-Fi and Bluetooth features. . 2020-2021 Projects . Yedidya and Yehuda: GitHub page , Instructables . | Adar and Gabriel: GitHub page . | Shani and Nir: GitHub page . | Liron and Yuval: GitHub page . | .",
          "url": "https://yairmau.github.io/teaching/agrotech/",
          "relUrl": "/teaching/agrotech/",
          "date": ""
      }
      
  

  
      ,"page16": {
          "title": "",
          "content": "Research Specialist Wanted! . Important: We‚Äôre looking for a part-time research specialist, more details here. . Senior Lecturer Institute of Environmental Sciences ¬† The Hebrew University of Jerusalem . About . I‚Äôm interested in the interactions between soil, water and vegetation in drylands, in both natural and agricultural ecosystems. Our goal is to understand how the basic processes and feedbacks influence the ecosystem dynamics, and how this knowledge can be used to control those systems. . The common theme between my different research projects is land/ecosystem degradation caused by human activity. . Our research is based on ‚Äúsimple‚Äù mathematical models, that strive to capture the essential physical processes, while providing deep insight into the dynamics of the system. Some of the tools we use in the modeling of environmental questions come from dynamical systems, statistical physics and optimal control theory. . [Read more on current research projects‚Ä¶] . I love to write code, and I made available on the Tutorials page some working examples in Python and LaTeX. Check it out! . How to find me . ‚ÄÇ yair.mau@mail.huji.ac.il ‚ÄÇ +972 8 948 9386 ‚ÄÇ Rehovot Campus, Lubell building, office 19. Map here . Google Scholar Check out my CV . . Piet Hein Problems worthy ‚ÄÉ‚ÄÉof attack prove their worth ‚ÄÉ‚ÄÉby hitting back. .",
          "url": "https://yairmau.github.io/",
          "relUrl": "/",
          "date": ""
      }
      
  

  
      ,"page17": {
          "title": "Teaching",
          "content": "I currently teach the following courses at the Hebrew University of Jerusalem, campus Rehovot. . Surface Hydrology . Agrotech Lab . Physics A (extended) .",
          "url": "https://yairmau.github.io/teaching/",
          "relUrl": "/teaching/",
          "date": ""
      }
      
  

  
      ,"page18": {
          "title": "Publications",
          "content": "Publications . . Taiwo Adeyemo, Isaac Kramer, Guy J. Levy, and Yair Mau. Salinity and sodicity can cause hysteresis in soil hydraulic conductivity. arXiv, 2021. doi: doi Abstract ‚ÄÉ BibTeX ‚ÄÉ PDF ‚ÄÉ arXiv ‚ÄÉ . Soil salinization and sodification can cause detrimental effects to soil structure, with important implications to irrigated agriculture. Knowledge of the extent to which degradation in soil structure due to salinization and sodification is reversible is still lacking, however. The objective of our study is to examine the effects of the history of solute composition on the degree of reversibility in saturated hydraulic conductivity (Ksat). We systematically investigate the effects of salinity (electrolyte concentration) and sodicity (sodium adsorption ratio) on Ksat, for three soils of varying clay content. The soil column experiments yielded hysteresis graphs, in which Ksat does not go back to its original values after initial decay. We developed indices to quantify the degree of Ksat degradation and reversibility, and found that contrary to our expectations, high susceptibility to degradation does not always correlate with low capability of rehabilitation. Measurements of soil swelling helped us discern when degradation was mainly caused by swelling or clay dispersion. Our findings underscore the need of understanding hysteresis caused by salinity and sodicity, aiming at a better management of agricultural soils for sustainable long-term use. @misc{adeyemo2021salinity,&nbsp;&nbsp; title={Salinity and sodicity can cause hysteresis in soil hydraulic conductivity},&nbsp;&nbsp; author={Taiwo Adeyemo and Isaac Kramer and Guy J. Levy and Yair Mau},&nbsp;&nbsp; year={2021},&nbsp;&nbsp; eprint={2109.09111},&nbsp;&nbsp; archivePrefix={arXiv},&nbsp;&nbsp; primaryClass={physics.geo-ph}} . Yuval R. Zelnik, Yair Mau, Moshe Shachak, and Ehud Meron. High-integrity human intervention in ecosystems: Tracking self-organization modes. PLOS Computational Biology, 2021. doi: doi Abstract ‚ÄÉ BibTeX ‚ÄÉ PDF ‚ÄÉ . Humans play major roles in shaping and transforming the ecology of Earth. Unlike natural drivers of ecosystem change, which are erratic and unpredictable, human intervention in ecosystems generally involves planning and management, but often results in detrimental outcomes. Using model studies and aerial-image analysis, we argue that the design of a successful human intervention form calls for the identification of the self-organization modes that drive ecosystem change, and for studying their dynamics. We demonstrate this approach with two examples: grazing management in drought-prone ecosystems, and rehabilitation of degraded vegetation by water harvesting. We show that grazing can increase the resilience to droughts, rather than imposing an additional stress, if managed in a spatially non-uniform manner, and that fragmental restoration along contour bunds is more resilient than the common practice of continuous restoration in vegetation stripes. We conclude by discussing the need for additional studies of self-organization modes and their dynamics. @article{zelnik2021high,&nbsp;&nbsp; doi = {doi},&nbsp;&nbsp; url = {url},&nbsp;&nbsp; year = {2021},&nbsp;&nbsp; author = {Yuval R. Zelnik and Yair Mau and Moshe Shachak and Ehud Meron},&nbsp;&nbsp; title = {High-integrity human intervention in ecosystems: Tracking self-organization modes},&nbsp;&nbsp; journal = {PLOS Computational Biology},&nbsp;&nbsp; volume={vol},&nbsp;&nbsp; number={num},&nbsp;&nbsp; pages={pages}} . Isaac Kramer, Yuval Bayer, Taiwo Adeyemo, and Yair Mau. Hysteresis in soil hydraulic conductivity as driven by salinity and sodicity: a modeling framework . HESS, 2021. doi: 10.5194/hess-25-1993-2021 Abstract ‚ÄÉ BibTeX ‚ÄÉ PDF ‚ÄÉ GitHub . Declining soil-saturated hydraulic conductivity (Ks) as a result of saline and sodic irrigation water is a major cause of soil degradation. While it is understood that the mechanisms that lead to degradation can cause irreversible changes in Ks, existing models do not account for hysteresis between the degradation and rehabilitation processes. We develop the first model for the effect of saline and sodic water on Ks that explicitly includes hysteresis. As such, the idea that a soil&#39;s history of degradation and rehabilitation determines its future Ks lies at the center of this model. By means of a ‚Äúweight‚Äù function, the model accounts for soil-specific differences, such as clay content. The weight function also determines the form of the hysteresis curves, which are not restricted to a single shape, as in some existing models for irreversible soil processes. The concept of the weight function is used to develop a reversibility index, which allows for the quantitative comparison of different soils and their susceptibility to irreversible degradation. We discuss the experimental setup required to find a soil&#39;s weight function and show how the weight function determines the degree to which Ks is reversible for a given soil. We demonstrate the feasibility of this procedure by presenting experimental results showcasing the presence of hysteresis in soil Ks and using these results to calculate a weight function. Past experiments and models on the decline of Ks due to salinity and sodicity focus on degradation alone, ignoring any characterization of the degree to which declines in Ks are reversible. Our model and experimental results emphasize the need to measure ‚Äúreversal curves‚Äù, which are obtained from rehabilitation measurements following mild declines in Ks. The developed model has the potential to significantly improve our ability to assess the risk of soil degradation by allowing for the consideration of how the accumulation of small degradation events can cause significant land degradation. @article{kramer2020hysteresis,&nbsp;&nbsp; doi = {10.5194/hess-25-1993-2021},&nbsp;&nbsp; url = {https://doi.org/10.5194/hess-25-1993-2021},&nbsp;&nbsp; year = {2021},&nbsp;&nbsp; author = {Isaac Kramer and Yuval Bayer and Taiwo Adeyemo and Yair Mau},&nbsp;&nbsp; title = {Hysteresis in soil hydraulic conductivity as driven by salinity and sodicity: a modeling framework},&nbsp;&nbsp; journal = {Hydrology and Earth System Sciences},&nbsp;&nbsp; volume={25},&nbsp;&nbsp; number={4},&nbsp;&nbsp; pages={1993--2008}} . Isaac Kramer and Yair Mau. Soil degradation risks assessed by the SOTE model for salinity and sodicity. Water Resources Research, 2020. doi: 10.1029/2020WR027456 Abstract ‚ÄÉ BibTeX ‚ÄÉ PDF ‚ÄÉ GitHub . Soil salinity and sodicity are serious environmental hazards, with the potential to limit agricultural production and cause destructive soil degradation. These concerns are especially high in dry areas, which often rely on saline and sodic irrigation water to support agriculture. To assess long-term soil degradation risk, we introduce the SOTE model, which describes the dynamics of soil water content, salinity, and sodicity, as driven by irrigation and rainfall. The SOTE model incorporates how changes in salinity and sodicity affect saturated soil hydraulic conductivity, Ks, on a soil-specific basis. The model was successfully validated against results from a multi-year lysimeter experiment involving different irrigation water qualities and precipitation. We evaluated the impact of shorter rainy seasons on the dynamics of soil degradation in a Mediterranean climate. Critical degradation risk, indicated by reductions in Ks greater than 20%, increased from 0 to 3% when the rainy season was shortened from 130 to 80 days. Alarmingly, when irreversible degradation is allowed for, overall risk increases to 68%. Assessing the effect of irrigation water on different soils textures, we found that while greater clay fractions are usually more susceptible to dispersion, accurate risk assessment hinges on soil-water dynamics. SOTE is amenable to large-ensemble simulations of stochastic climatic conditions, for which trends in the statistics of salinization and soil degradation can be identified. As such, SOTE can be a useful land management tool, allowing planners to understand the risk of long-term soil degradation given irrigation practices, soil qualities, and climate conditions. @article{kramer2020soil,&nbsp;&nbsp; doi = {10.1029/2020WR027456},&nbsp;&nbsp; url = {https://doi.org/10.1029/2020WR027456},&nbsp;&nbsp; year = {2020},&nbsp;&nbsp; publisher = {Wiley},&nbsp;&nbsp; volume = {56},&nbsp;&nbsp; number = {10},&nbsp;&nbsp; author = {Isaac Kramer and Yair Mau},&nbsp;&nbsp; title = {Soil degradation risks assessed by the SOTE model for salinity and sodicity},&nbsp;&nbsp; journal = {Water Resources Research}} . Xing Chen, Mukesh Kumar, Daniel de B. Richter, and Yair Mau. Impact of gully incision on hillslope hydrology. Hydrological Processes, 2020. doi: 10.1002/hyp.13845 Abstract ‚ÄÉ BibTeX ‚ÄÉ PDF . The Southern U.S. Piedmont ranging from Virginia to Georgia underwent severe gully erosion over a century of farming mainly for cotton (1800s to 1930s). Although tree succession blanketed much of this region by the middle 20th century, gully erosion still occurs, especially during wet seasons. While many studies on gully erosion have focused on soil loss, soil carbon exchange, and stormwater response, the impacts on soil moisture, groundwater, and transpiration remain understudied. Using a newly developed 2D hydrologic model, this study analyzes the impacts of gully erosion on hillslope hydrologic states and fluxes. Results indicate that increases in gully incision lead to reduction in groundwater table, root zone soil moisture, and transpiration. These reductions show seasonal variations, but the season when the reduction is maximum differs among the hydrologic variables. Spatially, the impacts are generally the greatest near the toe of the hillslope and reduce further away from it, although the reductions are sometimes non‚Äêmonotonic. Overall, the impacts are larger for shallow gully depths and diminish as the incision goes deeper. Lastly, the extent of impacts on a heterogeneous hillslope is found to be very different with respect to a homogeneous surrogate made of dominant soil properties. These results show that through gully erosion, the landscape not only loses soil but also a large amount of water from the subsurface. The magnitude of water loss is, however, dependent on hydrogeologic and topographic configuration of the hillslope. The results will facilitate (a) mapping of relative susceptibility of landscapes to gullying, (b) understanding of the impacts of stream manipulations such as due to dredging on hillslope eco‚Äêhydrology, (c) prioritization of mitigation measures to prevent gullying, and (d) design of observation campaigns to assess the impacts of gullying on hydrologic response. @article{Chen2020,&nbsp;&nbsp; doi = {10.1002/hyp.13845},&nbsp;&nbsp; url = {https://doi.org/10.1002/hyp.13845},&nbsp;&nbsp; year = {2020},&nbsp;&nbsp; publisher = {Wiley},&nbsp;&nbsp; volume = {34},&nbsp;&nbsp; number = {19},&nbsp;&nbsp; pages = {3848--3866},&nbsp;&nbsp; author = {Xing Chen and Mukesh Kumar and Daniel B. Richter and Yair Mau},&nbsp;&nbsp; title = {Impact of gully incision on hillslope hydrology},&nbsp;&nbsp; journal = {Hydrological Processes}} . Avigail Kaner, Yakir Preisler, Jos√© M. Gr√ºnzweig, and Yair Mau. Internal water storage buffering maintains plant function under drought as described by a general hydraulic model. biorxiv, 2020. doi: 10.1101/2020.02.11.943563 Abstract ‚ÄÉ BibTeX ‚ÄÉ PDF . Internal water storage is of crucial importance for plants under drought stress, allowing them to temporarily maintain transpiration higher than root-uptake flow, thus potentially keeping a positive carbon balance. A deep understanding of this adaptation is key for predicting the fate of ecosystems subjected to climate change-induced droughts of increasing intensity and duration. Using a minimalistic model, we derive predictions for how environmental drivers (atmospheric demand and soil water availability) interplay with the water storage, creating time lags between the flows in the plant, and granting the plant increased hydraulic safety margin protecting its xylem from embolism. We parametrize our model against transpiration and sap flow measurements in a semi-arid pine forest during seasonal drought. From the parametrized whole-stand traits, we derive a 3.7-hour time lag between transpiration and sap flow, and that 31% of daily transpiration comes directly from the plant‚Äôs internal water storage, both corroborated by the measurements. Due to the model simplicity, our results are useful for interpreting, analyzing, and predicting the effects of the internal storage buffering from the individual plant to the ecosystem scale. Because internal storage produces survival-enhancing behavior in sub-daily time scales, it is an indispensable component for modeling ecosystems under drought stress. @article{kaner2020internal,&nbsp;&nbsp; doi = {10.1101/2020.02.11.943563},&nbsp;&nbsp; url = {https://doi.org/10.1101/2020.02.11.943563},&nbsp;&nbsp; year = {2020},&nbsp;&nbsp; publisher = {Cold Spring Harbor Laboratory},&nbsp;&nbsp; volume = {},&nbsp;&nbsp; number = {},&nbsp;&nbsp; pages = {},&nbsp;&nbsp; author = {Avigail Kaner and Yakir Preisler and Jos{ &#39;{e}} M Gr &quot;{u}nzweig and Yair Mau},&nbsp;&nbsp; title = {Internal water storage buffering maintains plant function under drought as described by a general hydraulic model},&nbsp;&nbsp; journal = {}} . Yair Mau and Amilcare Porporato. Optimal control solutions to sodic soil reclamation. Advances in Water Resources, 91:37‚Äì45, 2016. doi: 10.1016/j.advwatres.2016.02.014 Abstract ‚ÄÉ BibTeX ‚ÄÉ PDF . We study the reclamation process of a sodic soil by irrigation with water amended with calcium cations. In order to explore the entire range of time-dependent strategies, this task is framed as an optimal control problem, where the amendment rate is the control and the total rehabilitation time is the quantity to be minimized. We use a minimalist model of vertically averaged soil salinity and sodicity, in which the main feedback controlling the dynamics is the nonlinear coupling of soil water and exchange complex, given by the Gapon equation. We show that the optimal solution is a bang‚Äìbang control strategy, where the amendment rate is discontinuously switched along the process from a maximum value to zero. The solution enables a reduction in remediation time of about 50%, compared with the continuous use of good-quality irrigation water. Because of its general structure, the bang‚Äìbang solution is also shown to work for the reclamation of other soil conditions, such as saline‚Äìsodic soils. The novelty in our modeling approach is the capability of searching the entire ‚Äústrategy space‚Äù for optimal time-dependent protocols. The optimal solutions found for the minimalist model can be then fine-tuned by experiments and numerical simulations, applicable to realistic conditions that include spatial variability and heterogeneities. @article{mau2016optimal,&nbsp;&nbsp; doi = {10.1016/j.advwatres.2016.02.014},&nbsp;&nbsp; year = {2016},&nbsp;&nbsp; publisher = {Elsevier {BV}},&nbsp;&nbsp; volume = {91},&nbsp;&nbsp; pages = {37--45},&nbsp;&nbsp; author = {Yair Mau and Amilcare Porporato},&nbsp;&nbsp; title = {Optimal control solutions to sodic soil reclamation},&nbsp;&nbsp; journal = {Advances in Water Resources}} . Amilcare Porporato, Xue Feng, Stefano Manzoni, Yair Mau, Anthony J. Parolari, and Giulia Vico. Ecohydrological modeling in agroecosystems: Examples and challenges. Water Resources Research, 51(7):5081‚Äì5099, 2015. doi: 10.1002/2015WR017289 Abstract ‚ÄÉ BibTeX ‚ÄÉ PDF . Human societies are increasingly altering the water and biogeochemical cycles to both improve ecosystem productivity and reduce risks associated with the unpredictable variability of climatic drivers. These alterations, however, often cause large negative environmental consequences, raising the question as to how societies can ensure a sustainable use of natural resources for the future. Here we discuss how ecohydrological modeling may address these broad questions with special attention to agroecosystems. The challenges related to modeling the two-way interaction between society and environment are illustrated by means of a dynamical model in which soil and water quality supports the growth of human society but is also degraded by excessive pressure, leading to critical transitions and sustained societal growth-collapse cycles. We then focus on the coupled dynamics of soil water and solutes (nutrients or contaminants), emphasizing the modeling challenges, presented by the strong nonlinearities in the soil and plant system and the unpredictable hydroclimatic forcing, that need to be overcome to quantitatively analyze problems of soil water sustainability in both natural and agricultural ecosystems. We discuss applications of this framework to problems of irrigation, soil salinization, and fertilization and emphasize how optimal solutions for large-scale, long-term planning of soil and water resources in agroecosystems under uncertainty could be provided by methods from stochastic control, informed by physically and mathematically sound descriptions of ecohydrological and biogeochemical interactions. @article{porporato2015ecohydrological,&nbsp;&nbsp; doi = {10.1002/2015wr017289},&nbsp;&nbsp; year = {2015},&nbsp;&nbsp; publisher = {Wiley-Blackwell},&nbsp;&nbsp; author = {Amilcare Porporato and Xue Feng and Stefano Manzoni and Yair Mau and Anthony J. Parolari and Giulia Vico},&nbsp;&nbsp; title = {Ecohydrological modeling in agroecosystems: Examples and challenges},&nbsp;&nbsp; journal = {Water Resources Research},&nbsp;&nbsp; volume = {51},&nbsp;&nbsp; number = {7},&nbsp;&nbsp; pages = {5081--5099}, } . Yair Mau and Amilcare Porporato. A dynamical system approach to soil salinity and sodicity. Advances in Water Resources, 83:68‚Äì76, 2015. doi: 10.1016/j.advwatres.2015.05.010 Abstract ‚ÄÉ BibTeX ‚ÄÉ PDF . Soil salinity and sodicity impose severe constrains to agriculture, especially in arid and semi-arid regions, where good-quality water for irrigation is scarce. While detailed models have been proposed in the past to describe the dynamics of salt and sodium in the soil, they typically require cumbersome calculations and are not amenable to theoretical analysis. Here we present an analytical model for the dynamics of salinity and sodicity in the root zone. We determine the dependence of steady-state salinity and sodicity levels on irrigation water quality and derive the trajectories in the phase space. The only stationary solution the equations admit is a stable node. Through numerical integration and analysis of the eigenvalues of the derived two-dimensional system of equations, the slower time scale associated with sodification is quantified with respect to the faster time scale associated to salinization. The role of different cation exchange equations (Gapon and Vanselow conventions) are shown to be practically the same with regard to the phase-space dynamics and the time scales. The results can be applied in controlling for low levels of salinity and sodicity, and in planning remediation strategies that are timely and economical. @article{mau2015dynamical,&nbsp;&nbsp; year = {2015},&nbsp;&nbsp; publisher = {Elsevier {BV}},&nbsp;&nbsp; volume = {83},&nbsp;&nbsp; pages = {68--76},&nbsp;&nbsp; author = {Yair Mau and Amilcare Porporato},&nbsp;&nbsp; title = {A dynamical system approach to soil salinity and sodicity},&nbsp;&nbsp; journal = {Advances in Water Resources},&nbsp;&nbsp; doi = {10.1016/j.advwatres.2015.05.010}, } . Yair Mau, Lev Haim, and Ehud Meron. Reversing desertification as a spatial resonance problem. Physical Review E, 91(1):012903, 2015. doi: 10.1103/PhysRevE.91.012903 Abstract ‚ÄÉ BibTeX ‚ÄÉ PDF . An important environmental application of pattern control by periodic spatial forcing is the restoration of vegetation patterns in water-limited ecosystems that went through desertification. Vegetation restoration is often based on periodic landscape modulations that intercept overland water flow and form favorable conditions for vegetation growth. Viewing this method as a spatial resonance problem, we show that plain realizations of this method, assuming a complete vegetation response to the imposed modulation pattern, suffer from poor resilience to rainfall variability. By contrast, less intuitive realizations, based on the inherent spatial modes of vegetation growth and involving partial vegetation implantation, can be highly resilient and equally productive. We derive these results using two complementary models, a realistic vegetation model, and a simple pattern formation model that lends itself to mathematical analysis and highlights the universal aspects of the behaviors found with the vegetation model. We focus on reversing desertification as an outstanding environmental problem, but the main conclusions hold for any spatially forced system near the onset of a finite-wave-number instability that is subjected to noisy conditions. @article{mau2015reversing,&nbsp;&nbsp; title={Reversing desertification as a spatial resonance problem},&nbsp;&nbsp; author={Yair Mau and Lev Haim and Ehud Meron},&nbsp;&nbsp; journal={Physical Review E},&nbsp;&nbsp; volume={91},&nbsp;&nbsp; number={1},&nbsp;&nbsp; pages={012903},&nbsp;&nbsp; year={2015},&nbsp;&nbsp; publisher={APS},&nbsp;&nbsp; doi = {10.1103/physreve.91.012903}, } . Yair Mau, Xue Feng, and Amilcare Porporato. Multiplicative jump processes and applications to leaching of salt and contaminants in the soil. Physical Review E, 90(5):052128, 2014. doi: 10.1103/PhysRevE.90.052128 Abstract ‚ÄÉ BibTeX ‚ÄÉ PDF . We consider simple systems driven multiplicatively by white shot noise, which appear in the modeling of the dynamics of soil nutrients and contaminants. The dynamics of these systems is analyzed in two ways: solving a hierarchy of linear ordinary differential equations for the moments, which gives a time scale of convergence of the stationary probability density function; and characterizing the crossing properties, such as the mean first-passage time and the mean frequency of level crossing. These results are readily applicable to the study of geophysical systems, such as the problem of accumulation of salt in the root zone, i.e., soil salinization. @article{mau2014multiplicative,&nbsp;&nbsp; title={Multiplicative jump processes and applications to leaching of salt and contaminants in the soil},&nbsp;&nbsp; author={Yair Mau and Xue Feng and Amilcare Porporato},&nbsp;&nbsp; journal={Physical Review E},&nbsp;&nbsp; volume={90},&nbsp;&nbsp; number={5},&nbsp;&nbsp; pages={052128},&nbsp;&nbsp; year={2014},&nbsp;&nbsp; publisher={APS},&nbsp;&nbsp; doi = {10.1103/physreve.90.052128}, } . Lev Haim, Yair Mau, and Ehud Meron. Spatial forcing of pattern-forming systems that lack inversion symmetry. Physical Review E, 90(2):022904, 2014. doi: 10.1103/PhysRevE.90.022904 Abstract ‚ÄÉ BibTeX ‚ÄÉ PDF . The entrainment of periodic patterns to spatially periodic parametric forcing is studied. Using a weak nonlinear analysis of a simple pattern formation model we study the resonant responses of one-dimensional systems that lack inversion symmetry. Focusing on the first three n:1 resonances, in which the system adjusts its wavenumber to one nth of the forcing wavenumber, we delineate commonalities and differences among the resonances. Surprisingly, we find that all resonances show multiplicity of stable phase states, including the 1:1 resonance. The phase states in the 2:1 and 3:1 resonances, however, differ from those in the 1:1 resonance in remaining symmetric even when the inversion symmetry is broken. This is because of the existence of a discrete translation symmetry in the forced system. As a consequence, the 2:1 and 3:1 resonances show stationary phase fronts and patterns, whereas phase fronts within the 1:1 resonance are propagating and phase patterns are transients. In addition, we find substantial differences between the 2:1 resonance and the other two resonances. While the pattern forming instability in the 2:1 resonance is supercritical, in the 1:1 and 3:1 resonances it is subcritical, and while the inversion asymmetry extends the ranges of resonant solutions in the 1:1 and 3:1 resonances, it has no effect on the 2:1 resonance range. We conclude by discussing a few open questions. @article{haim2014spatial,&nbsp;&nbsp; title={Spatial forcing of pattern-forming systems that lack inversion symmetry},&nbsp;&nbsp; author={Lev Haim and Yair Mau and Ehud Meron},&nbsp;&nbsp; journal={Physical Review E},&nbsp;&nbsp; volume={90},&nbsp;&nbsp; number={2},&nbsp;&nbsp; pages={022904},&nbsp;&nbsp; year={2014},&nbsp;&nbsp; publisher={APS},&nbsp;&nbsp; doi = {10.1103/physreve.90.022904}, } . Yair Mau, Lev Haim, Aric Hagberg, and Ehud Meron. Competing resonances in spatially forced pattern-forming systems. Physical Review E, 88(3): 032917, 2013. doi: 10.1103/PhysRevE.88.032917 Abstract ‚ÄÉ BibTeX ‚ÄÉ PDF . Spatial periodic forcing can entrain a pattern-forming system in the same way as temporal periodic forcing can entrain an oscillator. The forcing can lock the pattern&#39;s wave number to a fraction of the forcing wave number within tonguelike domains in the forcing parameter plane, it can increase the pattern&#39;s amplitude, and it can also create patterns below their onset. We derive these results using a multiple-scale analysis of a spatially forced Swift-Hohenberg equation in one spatial dimension. In two spatial dimensions the one-dimensional forcing can induce a symmetry-breaking instability that leads to two-dimensional (2D) patterns, rectangular or oblique. These patterns resonate with the forcing by locking their wave-vector component in the forcing direction to half the forcing wave number. The range of this type of 2:1 resonance overlaps with the 1:1 resonance tongue of stripe patterns. Using a multiple-scale analysis in the overlap region we show that the 2D patterns can destabilize the 1:1 resonant stripes even at exact resonance. This result sheds new light on the use of spatial periodic forcing for controlling patterns. @article{mau2013competing,&nbsp;&nbsp; title={Competing resonances in spatially forced pattern-forming systems},&nbsp;&nbsp; author={Yair Mau and Lev Haim and Aric Hagberg and Ehud Meron},&nbsp;&nbsp; journal={Physical Review E},&nbsp;&nbsp; volume={88},&nbsp;&nbsp; number={3},&nbsp;&nbsp; pages={032917},&nbsp;&nbsp; year={2013},&nbsp;&nbsp; publisher={APS},&nbsp;&nbsp; doi = {10.1103/physreve.88.032917}, } . Yair Mau, Aric Hagberg, and Ehud Meron. Spatial periodic forcing can displace patterns it is intended to control. Physical Review Letters, 109 (3):034102, 2012. doi: 10.1103/PhysRevLett.109.034102 Abstract ‚ÄÉ BibTeX ‚ÄÉ PDF . Spatial periodic forcing of pattern-forming systems is an important, but lightly studied, method of controlling patterns. It can be used to control the amplitude and wave number of one-dimensional periodic patterns, to stabilize unstable patterns, and to induce them below instability onset. We show that, although in one spatial dimension the forcing acts to reinforce the patterns, in two dimensions it acts to destabilize or displace them by inducing two-dimensional rectangular and oblique patterns. @article{mau2012spatial,&nbsp;&nbsp; title={Spatial periodic forcing can displace patterns it is intended to control},&nbsp;&nbsp; author={Yair Mau and Aric Hagberg and Ehud Meron},&nbsp;&nbsp; journal={Physical Review Letters},&nbsp;&nbsp; volume={109},&nbsp;&nbsp; number={3},&nbsp;&nbsp; pages={034102},&nbsp;&nbsp; year={2012},&nbsp;&nbsp; publisher={APS},&nbsp;&nbsp; doi = {10.1103/physrevlett.109.034102}, } . Yair Mau, Aric Hagberg, and Ehud Meron. Dual-mode spiral vortices. Physical Review E, 80(6):065203, 2009. doi: 10.1103/physreve.80.065203 Abstract ‚ÄÉ BibTeX ‚ÄÉ PDF . We show that spiral vortices in oscillatory systems can lose stability to secondary modes to form dual-mode spiral vortices. The secondary modes grow at the vortex core where the oscillation amplitude vanishes but are nonlinearly damped by the oscillatory mode away from the core. Gradients of the oscillation phase, induced by the hosted secondary mode, can lead to additional hosting events that culminate in periodic core oscillations or in a novel form of spatiotemporal chaos. The results of this study apply to physical, chemical, and biological systems that go through cusp-Hopf, fold-Hopf, and Hopf-Turing bifurcations. @article{mau2009dual,&nbsp;&nbsp; title={Dual-mode spiral vortices},&nbsp;&nbsp; author={Yair Mau and Aric Hagberg and Ehud Meron},&nbsp;&nbsp; journal={Physical Review E},&nbsp;&nbsp; volume={80},&nbsp;&nbsp; number={6},&nbsp;&nbsp; pages={065203},&nbsp;&nbsp; year={2009},&nbsp;&nbsp; publisher={APS},&nbsp;&nbsp; doi = {10.1103/physreve.80.065203}, } Tip: find the BibTeX of any publication by copying its doi here: doi2bib.org. . Theses . PhD Thesis . Title: ‚ÄúPattern Formation in Spatially Forced Systems: Application to Vegetation Restoration‚Äù Advisor: Prof. Ehud Meron. The Physics Department, The Ben-Gurion University of the Negev, Israel. PDF . Master‚Äôs Thesis . Title: ‚ÄúLocalized Spatial Strutures in Non-Equilibrium Systems‚Äù Advisor: Prof. Ehud Meron. The Physics Department, The Ben-Gurion University of the Negev, Israel. PDF Watch pretty cool videos on this research!! .",
          "url": "https://yairmau.github.io/publications/",
          "relUrl": "/publications/",
          "date": ""
      }
      
  

  
      ,"page19": {
          "title": "Group",
          "content": "Yair Mau, Principal Investigator . . . Current group members . Isaac Kramer, PhD student . . . Nurit Goldberg, PhD student . . . Laura Rez, MSc student . . . Yuval Bayer, Undergraduate Student . . . Group Pictures . 2020-09-03, SkyTown, Tel Aviv . Laura Rez, Yuval Bayer, Nurit Goldberg, Avigail Kaner, Yair Mau, Taiwo Adeyemo, Isaac Kramer . 2020-03-18, First Corona zoom meeting . Nurit, Isaac, Yair, Taiwo, Avigail, Avi (clockwise from top left) . 2019-01-10, Yatir Forest . Avi Gross, Avigail Kaner, Isaac Kramer, Yair Mau . Alumni . Taiwo Adeyemo, MSc student (2021) . . . Michael Avi Gross, MSc student (2021) . . . Avigail Kaner, Research Specialist (2021) . . .",
          "url": "https://yairmau.github.io/group/",
          "relUrl": "/group/",
          "date": ""
      }
      
  

  
      ,"page20": {
          "title": "Research",
          "content": "Research . . Interests . I‚Äôm interested in the interactions between soil, water and vegetation in drylands, in both natural and agricultural ecosystems. Our goal is to understand how the basic processes and feedbacks influence the ecosystem dynamics, and how this knowledge can be used to control those systems. . The common theme between my different research projects is land/ecosystem degradation caused by human activity. . Our research is based on ‚Äúsimple‚Äù mathematical models, that strive to capture the essential physical processes, while providing deep insight into the dynamics of the system. Some of the tools we use in the modeling of environmental questions come from dynamical systems, statistical physics and optimal control theory. . We are recruiting excellent students to our group! Here you can find some more detail on our current projects. . Projects . Soil salinization . Every week, the world loses an area greater than Manhattan to salt-related soil degradation. It is estimated that 20% of all irrigated lands are affected by salinity, with an even higher fraction of salt-degraded soils in arid and semiarid regions. Salt accumulation in the soil, usually induced by insufficient drainage and poor-quality irrigation water, imposes severe restrictions on food production. . Some of the questions we‚Äôd like to answer are: On what time scales the salinization process occurs, and what is the effect of a drier and more extreme climate on the salt buildup? What are the critical thresholds for (irreversible) soil degradation? How can we rehabilitate a degraded soil by making optimal use of the resources available? What role can treated wastewater have on dryland agriculture that is sustainable with respect to the ecosystem services? ‚Äã . Nutrient dynamics . Nitrogen management is of critical importance to food security and environmental sustainability. Because of artificial fertilizers, we have seen a sharp increase in agricultural production in the last century. However, the same nitrogen available to plant uptake, can polute the groundwater, streams and lakes, and be released as nitrous oxide, a potent greenhouse gas. . We would like to understand how the nonlinear dynamics of nitrogen and carbon cycling is influenced by the hydrological cycle. In particular, we would like to study the influence of random rainfall events and a changing climate in driving these cycles. . Tree water balance in a semiarid pine forest and rates of survival under global climate change . Longer droughts and temperature rise impose severe risks of widespread forest mortality. Drought-adapted trees in semiarid and arid ecosystems cope with water stress by regulating their transpiration rate, thus saving internal water content and avoiding runaway decline in water potential. . Our goal is to characterize how trees manage their water budget in order to increase survival probability under prolonged drought periods. . Our group will approach these objectives by modeling the soil-tree-atmosphere water transport to assess tree survival probability for current and future climate scenarios. A collaboration with an experimental group will provide us data from a research station in the Yatir forest, in the Negev desert (see map above). . Studying the fate of the Yatir forest gives us a window into the future, for it is expected that many ecosystems around the world will face increasing drought stress as a result of climate change. .",
          "url": "https://yairmau.github.io/research/",
          "relUrl": "/research/",
          "date": ""
      }
      
  

  
      ,"page21": {
          "title": "Quotes",
          "content": "The scientist is not a person who gives the right answers, he‚Äôs one who asks the right questions. Claude L√©vi-Strauss . | The most exciting phrase to hear in science, the one that heralds new discoveries, is not ‚ÄòEureka!‚Äô (I found it!) but ‚ÄòThat‚Äôs funny‚Ä¶‚Äô Isaac Asimov . | In theory, there is no difference between theory and practice. But, in practice, there is. Jan L. A. van de Snepscheut . | Contrary to what Asimov says, the most exciting phrase in science, the one that heralds new discoveries, is not ‚ÄòEureka!‚Äô or ‚ÄòThat‚Äôs funny‚Ä¶,‚Äô it‚Äôs ‚ÄòYour research grant has been approved.‚Äô John Alejandro King . | A theory is something nobody believes, except the person who made it. An experiment is something everybody believes, except the person who made it. Attributed to Albert Einstein . | The purpose of models is not to fit the data but to sharpen the questions. Samuel Karlin . | Research is what I‚Äôm doing when I don‚Äôt know what I‚Äôm doing. Wernher von Braun . | Nothing is more practical than a good theory. Kurt Lewin . | Just as there are odors that dogs can smell and we cannot, as well as sounds that dogs can hear and we cannot, so too there are wavelengths of light we cannot see and flavors we cannot taste. Why then, given our brains wired the way they are, does the remark, ‚ÄúPerhaps there are thoughts we cannot think,‚Äù surprise you? Richard Hamming . | An expert is a person who has made all the mistakes that can be made in a very narrow field. Niels Bohr . | An expert is one who knows more and more about less and less until he knows absolutely everything about nothing. Nicholas Butler . | Essentially, all models are wrong, but some are useful. George E. P. Box . | Theory is when you know everything but nothing works. Praxis is when everything works but no one knows why. In our lab, theory and praxis are combined: nothing works and no one know why. Unknown . | The great tragedy of Science ‚Äî the slaying of a beautiful hypothesis by an ugly fact. Thomas Henry Huxley . | So, in the face of overwhelming odds, I‚Äôm left with only one option, I‚Äôm gonna have to science the shit out of this. Mark Watney . | The fact that we live at the bottom of a deep gravity well, on the surface of a gas covered planet going around a nuclear fireball 90 million miles away and think this to be normal is obviously some indication of how skewed our perspective tends to be. Douglas Adams, The Salmon of Doubt: Hitchhiking the Galaxy One Last Time . | You‚Äôd be amazed how much research you can get done when you have no life whatsoever. Ernest Cline, Ready Player One . | When you have eliminated all which is impossible, then whatever remains, however improbable, must be the truth. Sherlock Holmes . | With four parameters I can fit an elephant, and with five I can make him wiggle his trunk. John von Neumann . | The purpose of computing is insight, not numbers. Richard Hamming . | The most important thing about an equation is all the quantities that don‚Äôt appear in it; once we know what the essentials are, figuring out how they depend on each other is often the easier part. Pedro Domingos, The Master Algorithm . | Thermodynamics is easy ‚Äì I‚Äôve learned it many times. Harvey S. Leff . | Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise. John Tukey, The future of data analysis . | An article about computational result is advertising, not scholarship. The actual scholarship is the full software environment, code and data, that produced the result. Buckheit and Donoho (1995) . | ‚ÄùForty-two!‚Äù yelled Loonquawl. ‚ÄùIs that all you‚Äôve got to show for seven and a half million years‚Äô work?‚Äù ‚ÄùI checked it very thoroughly,‚Äù said the computer, ‚Äùand that quite definitely is the answer. I think the problem, to be quite honest with you, is that you‚Äôve never actually known what the question is.‚Äù ‚ÄùBut it was the Great Question! The Ultimate Question of Life, the Universe and Everything!‚Äù howled Loonquawl. ‚ÄùYes,‚Äù said Deep Thought with the air of one who suffers fools gladly, ‚Äùbut what actually is it?‚Äù A slow stupefied silence crept over the men as they stared at the computer and then at each other. ‚ÄùWell, you know, it‚Äôs just Everything ‚Ä¶ Everything ‚Ä¶ ‚Äù offered Phouchg weakly. ‚ÄùExactly!‚Äù said Deep Thought. ‚ÄùSo once you do know what the question actually is, you‚Äôll know what the answer means.‚Äù Douglas Adams, The Hitchiker‚Äôs Guide to the Galaxy . | Computers are useless. They can only give you answers. Pablo Picasso . | The true and only goal of science is to reveal unity rather than mechanism. Henri Poincar√© . | One of the principal objects of theoretical research is to find the point of view from which the subject appears in the greatest simplicity. Josiah Willard Gibbs . | The future is already here ‚Äî it‚Äôs just not very evenly distributed. William Gibson . | True science teaches, above all, to doubt and to be ignorant. Miguel de Unamuno . | If you are the smartest person in the room, then you are in the wrong room. Unknown . | Dividing one number by another is mere computation; knowing what to divide by what is mathematics. Jordan Ellenberg, How Not to Be Wrong: The Power of Mathematical Thinking . | I‚Äôve been in this argument a thousand times. Won them all and convinced nobody. Amos Tversky, on the hot hand fallacy . | Mathematics is the extension of common sense by other means. Jordan Ellenberg, How Not to Be Wrong: The Power of Mathematical Thinking . | If I had more time, I would have written a shorter letter. Blaise Pascal . | I am going to give what I will call an elementary demonstration. But elementary does not mean easy to understand. Elementary means that very little is required to know ahead of time in order to understand it, except to have an infinite amount of intelligence. There may be a large number of steps that hard to follow, but to each does not require already knowing the calculus or Fourier transforms. Richard P. Feynman . | In this section a mathematical model of the growing embryo will be described. This model will be a simplification and an idealization, and consequently a falsification. It is to be hoped that the features retained for discussion are those of greatest importance in the present state of knowledge. Alan Turing, The Chemical Basis of Morphogenesis . | Problems worthy of attack prove their worth by hitting back. Piet Hein . | Art is solving problems that cannot be formulated before they have been solved. The shaping of the question is part of the answer. Piet Hein . | Physics is the simplest of all the sciences, and fundamental physics‚Å†‚Å†‚Äîthe study of the basic pieces of reality at the deepest level‚Å†‚Äîis the simplest of all. Not ‚Äúsimple‚Äù in the sense that the homework problems are easy, but simple in the sense that Galileo‚Äôs trick of ignoring friction and air resistance makes our lives easier. The reason why physics classes seem so hard is not because physics is so hard‚Äîit‚Äôs because we understand so much of it that there‚Äôs a lot to learn, and that‚Äôs because it‚Äôs fundamentally pretty simple. Sean Carroll, The Big Picture . | Simplicity is a great virtue but it requires hard work to achieve it and education to appreciate it. And to make matters worse: complexity sells better. Edsger W. Dijkstra . | The fundamental role of a teacher is not to deliver information. It is guide the social process of learning. The job of a teacher is to inspire, to challenge, to excite their students to want to learn. Yes, they also do explain and demonstrate and show things, but fundamentally that is beside the point. The most important thing a teacher does is make every student feel like they are important, to make them feel accountable for doing the work of learning. Derek Alexander Muller, youtube . | What is the first business of one who practices philosophy? To get rid of self-conceit. For it is impossible for anyone to begin to learn that which he thinks he already knows. Epictetus . | Confusion is a feeling that precedes learning something. It means you should pay closer attention, not disengage. The Stoic Emperor, twitter . | Let no one ignorant of geometry enter here. Said to be engraved on the door of Plato‚Äôs Academy, the oldest recorded entrance requirement of a college . | We don‚Äôt use heuristics just because they are fast and efficient. We use them because they are less precise and thus less vulnerable to be misled by noise. A good heuristic is to distrust whoever adds or requires unnecessary precision. Another good heuristic is to distrust whoever thinks that precision always correlates with correctness. Luca Dellanna, twitter . | The comments aren‚Äôt there to explain the code to the programmers, the code is there to explain the comments to the computer. Andy Harris, How to begin thinking like a programmer . | .",
          "url": "https://yairmau.github.io/more/quotes/",
          "relUrl": "/more/quotes/",
          "date": ""
      }
      
  

  
      ,"page22": {
          "title": "$\,\LaTeX$",
          "content": "Equations . With $ LaTeX$ you can write beautiful equations like . RŒºŒΩ‚àí12gŒºŒΩ‚ÄâR+gŒºŒΩŒõ=8œÄGc4TŒºŒΩR_{ mu nu} - {1 over 2}g_{ mu nu} ,R + g_{ mu nu} Lambda = displaystyle{8 pi G over c^4} T_{ mu nu}RŒºŒΩ‚Äã‚àí21‚ÄãgŒºŒΩ‚ÄãR+gŒºŒΩ‚ÄãŒõ=c48œÄG‚ÄãTŒºŒΩ‚Äã or œï=1+11+11+‚ãØ=1+52 phi = 1 + cfrac{1}{1+ cfrac{1}{1+ dotsb}}= frac{1+ sqrt{5}}{2}œï=1+1+1+‚ãØ1‚Äã1‚Äã=21+5‚Äã‚Äã . Learn more here. . Presentation . Make beautiful presentations using Beamer! Download a full working example of my favorite template [zip] [overleaf]. Browse the presentation below to get an idea. . Posters . Make beautiful posters with LaTeX. Portrait [zip] [overleaf] Landscape [zip] [overleaf] . . PhD Thesis . This template is based on my PhD thesis, it can make your life easier! . Download [zip] [overleaf]. | Don‚Äôt forget to read the file README.txt | Enjoy! | Hebrew . A LaTeX document with Hebrew text. Download .tex source here, and the pdf here. . See below a really practical use of the tools provided above. This is an exercise list in hebrew. Download .tex source here, and the pdf here. .",
          "url": "https://yairmau.github.io/tutorials/latex/",
          "relUrl": "/tutorials/latex/",
          "date": ""
      }
      
  

  
      ,"page23": {
          "title": "Science Communication",
          "content": "Talks . A scientific talk is not a scientific paper, it is a completely different medium! Here are a few tips that I find useful. . ![](/images/phd-comics-presentation.jpg) Cartoon by Jorge Cham How To Give a Talk . [How To Give a Talk, by David L Stern.] . Excerpt: . Principle 1 - Don‚Äôt Put Words On Slides It may not seem intuitive, but including words on slides has a bigger detrimental effect on the quality of a talk than any other issue I will address. . The slides in most science talks contain many words. This simple fact has many consequences, none positive. Most obviously, words on slides impel listeners to read the words on the slide. If you are talking while they are reading, then you generate cognitive dissonance that makes it difficult for the audience to understand either your spoken words or your written words. So, if you want your audience to read the words on a slide, then, presumably, you should shut up. But, I don‚Äôt recommend that. Instead, just cut all the words form your slides. . The David Attenborough Style of Scientific Presentation . [The David Attenborough Style of Scientific Presentation.] . Excerpt: . Get into this mindset: your main job is to be an entertainer, not a scientist. Most scientists don‚Äôt do this, which is why most scientific talks are bad. The fact of the matter is that if the audience doesn‚Äôt understand and enjoy your talk, they won‚Äôt care if your science is good. . A LaTeX template . Check out a nice LaTeX template for presentations [here]. . . Visualizations . Visualizations can take a message very, very far. I put a lot of thinking into how to best express an idea. I believe that great graphs greatly improve a scientific paper, and a thoughful graph/animation surely brings home the main point of a talk. Here are a few thoughts on visualizations. . Inspiration . Hans Rosling, Gapminder | Max Roser, Our World in Data | Stephen Malinowski, Music Animation Machine | The Economist, Daily Chart | . Colors . Don‚Äôt use the rainbow color map. Why? [Read this], by Robert Kosara. . So how should you choose the colors for a graph? . Visit colorbrewer. It gives sequential, diverging and qualitative colormaps that can be chosen to be ‚Äúcolorblind safe‚Äù, ‚Äúprint safe‚Äù, or ‚Äúphotocopy safe‚Äù. Usually, it is the first and only tool I use. It‚Äôs just great! | Visit Adobe Color CC. It gives you much more freedom to play with the colors than colorbrewer. Use it wisely! | . More on color: read Subtleties of Color, by Robert Simmon. . Browse for ideas . The Data Visualisation Catalogue | .",
          "url": "https://yairmau.github.io/more/communication/",
          "relUrl": "/more/communication/",
          "date": ""
      }
      
  

  
      ,"page24": {
          "title": "Science",
          "content": "",
          "url": "https://yairmau.github.io/blog/science/",
          "relUrl": "/blog/science/",
          "date": ""
      }
      
  

  
      ,"page25": {
          "title": "News",
          "content": "",
          "url": "https://yairmau.github.io/blog/news/",
          "relUrl": "/blog/news/",
          "date": ""
      }
      
  

  
      ,"page26": {
          "title": "Geek stuff",
          "content": "",
          "url": "https://yairmau.github.io/blog/geek/",
          "relUrl": "/blog/geek/",
          "date": ""
      }
      
  

  
      ,"page27": {
          "title": "Art",
          "content": "",
          "url": "https://yairmau.github.io/blog/art/",
          "relUrl": "/blog/art/",
          "date": ""
      }
      
  

  
      ,"page28": {
          "title": "Blog",
          "content": "News . Some of the latest news in the research group . Science . Science-related posts . Geek thoughts . This is where I write about random geek/math/puzzles stuff . Art . Art, origami, mathematical constructions, music, random projects . . . Selected blogposts . Here are a few blogposts that you might find interesting .",
          "url": "https://yairmau.github.io/blog/",
          "relUrl": "/blog/",
          "date": ""
      }
      
  

  
      ,"page29": {
          "title": "AGU 2021",
          "content": ". Important: Download the full poster here (~10Mb) Send me an email if you have any questions: yair.mau@mail.huji.ac.il . The poster mentions 4 papers we have worked on lately: . Isaac Kramer, Yuval Bayer, and Yair Mau. The Sustainability of Treated Wastewater Irrigation: The Impact of Hysteresis on Soil Hydraulic Conductivity . (under review, WRR), 2021. . Taiwo Adeyemo, Isaac Kramer, Guy J. Levy, and Yair Mau. Salinity and sodicity can cause hysteresis in soil hydraulic conductivity. arXiv, 2021. PDF ‚ÄÉ (under review, Geoderma) . Isaac Kramer, Yuval Bayer, Taiwo Adeyemo, and Yair Mau. Hysteresis in soil hydraulic conductivity as driven by salinity and sodicity: a modeling framework . HESS, 2021. doi: 10.5194/hess-25-1993-2021 PDF ‚ÄÉ GitHub . Isaac Kramer and Yair Mau. Soil degradation risks assessed by the SOTE model for salinity and sodicity. Water Resources Research, 2020. doi: 10.1029/2020WR027456 PDF ‚ÄÉ GitHub .",
          "url": "https://yairmau.github.io/AGU2021/",
          "relUrl": "/AGU2021/",
          "date": ""
      }
      
  

  
      ,"page30": {
          "title": "Python",
          "content": "Python is a great language for scientific computing, most of the programming done by our group is in python. We provide below some links for learning this language, and below we offer many python code examples. You are invited to download these codes, tweak with them, break them, hack them as you wish! . Most codes focus on plotting, but other algorithms such as numerical integration and Fourier transforms can also be found. . Some useful links for learning python: . Python Data Science Handbook | Learn Python the Hard Way | A Crash Course in Python for Scientists | Software Carpentry | Fun with Lists | Python | Codecademy | Scipy Lecture Notes | A gallery of interesting IPython Notebooks | Think Python: How to Think Like a Computer Scientist | The Python Graph Gallery | . This might be overwhelming, so I suggest you to follow this: . My advice on learning Python:Don&#39;t set out to &quot;learn Python&quot;. Choose a problem you&#39;re interested in and learn to solve it with Python. . &mdash; Jake VanderPlas (@jakevdp) September 10, 2017 Start by downloading Anaconda, a package manager application that will help you get started with python in all platforms. . Make sure you are acquainted with ipython (interactive python, try it here), and with Jupyter notebook. ‚Äã . I recommend Sublime Text for writing code, and you can install the Anaconda package to it to have a smooth IDE. [not the same anaconda as mentioned above! Yes, it‚Äôs confusing, I know.] . . Fitzhugh-Nagumo ‚Äî Labyrinthine Patterns . Jupyter notebook . Main features: how to make a movie, time-integration methods (semi-spectral and Euler) . . Conway‚Äôs Game of Life, acorn initial conditionnn . Jupyter notebook . This is a (slightly) modified version of Glowing Python‚Äôs code. I make it available here because it features a few nice things: . how to make a movie using matplotlib.animation | how to write a generator (function with yield) | how to plot a sparce array (spy) | . Main features: matplotlib.animation, yield, with, matplotlib.pyplot.spy . . Least squares fit of nonlinear function . Jupyter notebook . Main features: LaTeX text, scipy.optimize.curve_fit, matplotlib.patches . . Fun with histograms . Jupyter notebook . Main features: np.histogram, plt.hist, plt.bar, plt.barh, gridspec, least squares fit of nonlinear function, plt.hist2d . . Fancy subplot grid . Jupyter notebook . Highly customizable subplot structure. Also, figure contains several axis configurations and labeling options. Main features: gridspec subplots; numpy-compatible heaviside; label, ticks and axis manipulations; log scale . . Streamplot . Jupyter notebook . Streamplot of a two-dimensional linear system, with eigenvectors and nullclines. Python shows LaTeX equations beautifully. Main features: meshgrid, streamplot, contour, legend, LaTeX . . The time dependent Ginzburg-Landau equation . Jupyter notebook . Numerical integration of a parabolic partial differential equation, using finite differences: Euler step to advance time, and a 5-point stencil to approximate the Laplacian. Main features: imshow, colorbar, set_data . . The double pendulum . Jupyter notebook . Numerical integration of the equations of motion of the double pendulum. This time, scipy‚Äôs ode itegrator was used. Nice example of how to make a movie. Main features: scipy.integrate.ode, set_data, set_aspect(&#39;equal&#39;), remove plot, movie . . The Hilbert curve . Jupyter notebook . Construction of the Hilbert curve as a Lindenmayer system (L-system). Main features: string operations, movie . . A hysteresis mechanism . Jupyter notebook . Hysteresis mechanism created by the bistability of states. System goes to minimum points u in the energy functional $f=u^4-2u^2+hu$. The parameter $h$ is ramped down and up during this simulation. Main features: sympy analytical calculations, numpy dtypes, movie . . Contour plot . Jupyter notebook . Contour plot with many customizable options. Also, a nice way to truncate a colormap so it gives the color range that you want. Main features: truncate_colormap, contour, contourf (fill), clabel (contour label) . . . Least action principle on the beach . Jupyter notebook . How should ‚Äãlifeguard run in order to save a drowning person in minimal time? Answer: by using Snell‚Äôs law of refraction! This is a nice example how to use spines (x and y axis form a cross), instead of rectangular figures as usual. Also, ‚Äúannotations‚Äù are used, where things can be labeled with the help of arrows. Main features: spines, matplotlib.patches.Rectangle, annotate . . . This website‚Äôs logo . Jupyter notebook . Simple example of how to make a figure without any visible axes. Main features: set_axis_off, fill_between, matplotlib inline plot on Jupyter . . . Bars . Jupyter notebook . Horizontal and vertical bars, with numeric legends. Unicode support. Main features: unicode, bar, barh, grid . .",
          "url": "https://yairmau.github.io/tutorials/python/",
          "relUrl": "/tutorials/python/",
          "date": ""
      }
      
  

  
      ,"page31": {
          "title": "More",
          "content": "Tutorials . Tools I recommend . Quotes . Science Communication . Five-star Content .",
          "url": "https://yairmau.github.io/more/",
          "relUrl": "/more/",
          "date": ""
      }
      
  

  

  
      ,"page33": {
          "title": "",
          "content": "Senior Lecturer The Institute of Environmental Sciences The Hebrew University of Jerusalem . About . I‚Äôm interested in the interactions between soil, water and vegetation in drylands, in both natural and agricultural ecosystems. Our goal is to understand how the basic processes and feedbacks influence the ecosystem dynamics, and how this knowledge can be used to control those systems. . The common theme between my different research projects is land/ecosystem degradation caused by human activity. . Our research is based on ‚Äúsimple‚Äù mathematical models, that strive to capture the essential physical processes, while providing deep insight into the dynamics of the system. Some of the tools we use in the modeling of environmental questions come from dynamical systems, statistical physics and optimal control theory. . [Read more on current research projects‚Ä¶] . I love to write code, and I made available on the Tutorials page some working examples in Python and LaTeX. Check it out! . How to find me . ‚ÄÇ yair.mau@mail.huji.ac.il ‚ÄÇ +972 8 948 9386 ‚ÄÇ Rehovot Campus, Lubell building, office 19. Map here . Google Scholar Check out my CV . . Piet Hein Problems worthy ‚ÄÉ‚ÄÉof attack prove their worth ‚ÄÉ‚ÄÉby hitting back. .",
          "url": "https://yairmau.github.io/index_old.html",
          "relUrl": "/index_old.html",
          "date": ""
      }
      
  

  

  
      ,"page35": {
          "title": "Kinematics",
          "content": "Motivation for studying kinematics . firefighting airplanes in action, dogs jumping into a car, woman walking the wrong way . X and Y movements are independent . #PhysicsFactlet (179)The rate of change of position is velocity.The rate of change of velocity is acceleration.The rate of change of acceleration is jerk.The rate of change of jerk is jounce.The rate of change of jounce is crackle.The rate of change of crackle is pop. . &mdash; Jacopo Bertolotti (@j_bertolotti) October 8, 2019 2d kinematics . Harlem Globetrotter Makes Incredible Trick Shot From Plane Flying 70 MPH . jumping goats . Kevin failed Physics . Yes, Kevin failed physics and math, but he knew how to build a helluva ramp! üç∫ ü•¥ pic.twitter.com/8rPrtRmCYy . &mdash; üç∫ Hold My Beer üç∫ (@HldMyBeer) August 31, 2021 Galilean relativity . swimming against the current . Mythbusters - Soccer Ball Shot from Truck . https://youtu.be/BLuI118nhzc . Circular motion . Hamster, from https://twitter.com/SJSchauer/status/1186484325451227136?s=09 . Human Loop the Loop with Damien Walters . Ball in a pie pan: Testing Experiment . Beer flipping . 2001: A Space Odyssey . Centripetal force . Many forces can take the role of the centripetal force. . .",
          "url": "https://yairmau.github.io/teaching/physics71031/gifs/kinematics.html",
          "relUrl": "/teaching/physics71031/gifs/kinematics.html",
          "date": ""
      }
      
  

  
      ,"page36": {
          "title": "Linear Momentum & Center of Mass",
          "content": "Collisions . brain during collision . golf ball . Football to the Face 1000x Slower - The Slow Mo Guys . Elastic collisions . failed collision . bullets ricochet off water . periodic billiard collision . Althea Reinhardt‚Äôs face save . This is the face save btw pic.twitter.com/nSztPi6Hcq . &mdash; Nooruddean (@BeardedGenius) December 18, 2021 . Inelastic collisions . Apple collision at 90 km/h. . Center of mass . center of mass parabolic trajectory . Josh Imatorbhebhe vertical jump . Internal vs external forces . How to push your pickup truck .",
          "url": "https://yairmau.github.io/teaching/physics71031/gifs/linear-momentum.html",
          "relUrl": "/teaching/physics71031/gifs/linear-momentum.html",
          "date": ""
      }
      
  

  
      ,"page37": {
          "title": "Newton's Laws",
          "content": "Newton‚Äôs first law . The fall of the dinosaurs . Trampoline with leaves . At the Kibo ISS module . Rollerblades on a moving table . What is Inertia? . . Newton‚Äôs second law . Man with superhuman strength . Inside the ISS . . Whack-a-Stack . Apollo 15 hammer-feather drop . Newton‚Äôs third law . Newton cartoon . . Motorcycle kicks car . Friction . Static friction . Static vs. kinetic friction . No friction on inclined plane . Cat fails to jump, not enough friction . Spidergirl .",
          "url": "https://yairmau.github.io/teaching/physics71031/gifs/newtons-laws.html",
          "relUrl": "/teaching/physics71031/gifs/newtons-laws.html",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
      ,"page42": {
          "title": "Units",
          "content": "Any time you pick up a well shuffled deck, you are almost certainly holding an arrangement of cards that has never before existed and will likely never exist again. $52! approx 10^{68}$. Suppose a new permutation of 52 cards was drawn every second starting from The Big Bang (13.8 billion years ago). You wouldn‚Äôt even be close. To count out all 52! permutations you would need $10^{51}$ ages of the universe. . https://twitter.com/InertialObservr/status/1174403655711084544?s=09 . If you worked every single day, making $5000/day, from the time Columbus sailed to America, to the time you are reading this tweet, you would still not be a billionaire. . How much larger/heavier/longer was the Megalodon compared with a great white? . This is the mass damper of the Taipei 101 skyscraper: it has a mass of 728 tons and a diameter of 5.4 meters. It helps stabilize the building in high winds and this is the record movement realized during typhoon Soudelor with 160 km/h winds what is the mass density of this ball? . https://twitter.com/Rainmaker1973/status/1038498046571683840?s=09 . https://www.youtube.com/watch?v=xqELmBNyWfU . orders of magnitude, from . https://twitter.com/Rainmaker1973/status/1125710475378012161 . . .",
          "url": "https://yairmau.github.io/teaching/physics71031/gifs/units.html",
          "relUrl": "/teaching/physics71031/gifs/units.html",
          "date": ""
      }
      
  

  
      ,"page43": {
          "title": "Wow",
          "content": "rope swing record . does the period of a pendulum depend on its mass? . Time-Lapse: Lose Yourself in the Night Sky . fastest response time . The kangaroo rat‚Äôs escape response to a snake attack is less than 70 milliseconds and the quickest mammalian startle response. . Selected solar system objects to scale in size, rotation speed and axial tilt . Milky Way and Earth . Milky Way spinning around Earth . Milky Way fixed, earth spinning round . The Milky Way Fly Out . Stefan Payne-Wardenaar . Least action, path of bowling ball minimizes action . A free-falling frame of reference cancels gravity . {% twitter https://twitter.com/sfera314/status/1173284029900173315 %} . Mesmerising Mass Sheep Herding . Solar eclipse from space . 150 Ton Hydraulic Guillotine Vs Deck of Cards .",
          "url": "https://yairmau.github.io/teaching/physics71031/gifs/wow.html",
          "relUrl": "/teaching/physics71031/gifs/wow.html",
          "date": ""
      }
      
  

  
  

  
  

  
      ,"page46": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://yairmau.github.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}